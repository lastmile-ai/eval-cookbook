{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "3e67f200",
            "metadata": {},
            "source": [
                "# How to call functions with chat models\n",
                "\n",
                "This notebook covers how to use the Chat Completions API in combination with external functions to extend the capabilities of GPT models.\n",
                "\n",
                "`tools` is an optional parameter in the Chat Completion API which can be used to provide function specifications. The purpose of this is to enable models to generate function arguments which adhere to the provided specifications. Note that the API will not actually execute any function calls. It is up to developers to execute function calls using model outputs.\n",
                "\n",
                "Within the `tools` parameter, if the `functions` parameter is provided then by default the model will decide when it is appropriate to use one of the functions. The API can be forced to use a specific function by setting the `tool_choice` parameter to `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}`. The API can also be forced to not use any function by setting the `tool_choice` parameter to `\"none\"`. If a function is used, the output will contain `\"finish_reason\": \"tool_calls\"` in the response, as well as a `tool_calls` object that has the name of the function and the generated function arguments.\n",
                "\n",
                "### Overview\n",
                "\n",
                "This notebook contains the following 2 sections:\n",
                "\n",
                "- **How to generate function arguments:** Specify a set of functions and use the API to generate function arguments.\n",
                "- **How to call functions with model generated arguments:** Close the loop by actually executing functions with model generated arguments."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "64c85e26",
            "metadata": {},
            "source": [
                "## How to generate function arguments"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "80e71f33",
            "metadata": {
                "pycharm": {
                    "is_executing": true
                }
            },
            "outputs": [],
            "source": [
                "# !pip install scipy --quiet\n",
                "# !pip install tenacity --quiet\n",
                "# !pip install tiktoken --quiet\n",
                "# !pip install termcolor --quiet\n",
                "# !pip install openai --quiet\n",
                "# !pip install lastmile-eval"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dab872c5",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:45:23.563149Z",
                    "start_time": "2024-05-15T17:45:22.925978Z"
                }
            },
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "import openai\n",
                "from lastmile_eval.rag.debugger.api.tracing import LastMileTracer\n",
                "from lastmile_eval.rag.debugger.tracing import openai as openai_tracing\n",
                "from lastmile_eval.rag.debugger.tracing.sdk import get_lastmile_tracer\n",
                "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
                "from termcolor import colored\n",
                "\n",
                "GPT_MODEL = \"gpt-4o\"\n",
                "tracer: LastMileTracer = get_lastmile_tracer(\n",
                "    tracer_name=\"my-tracer\",\n",
                "    initial_params={\"motivation_quote\": \"I love staring into the sun\"},\n",
                "    # output_filepath=\"/Users/jonathan/Projects/eval/tracing_v1.out\",\n",
                "    # output_filepath=OUTPUT_FILE_PATH,\n",
                ")\n",
                "client = openai_tracing.wrap(openai.OpenAI(), tracer)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "id": "aefb33e0",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2024-05-17 17:48:32,574 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"}, {'role': 'user', 'content': \"What's the weather like today\"}], 'model': 'gpt-4-turbo'}}\n",
                        "2024-05-17 17:48:32,576 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
                        "2024-05-17 17:48:32,577 - close.started\n",
                        "2024-05-17 17:48:32,577 - close.complete\n",
                        "2024-05-17 17:48:32,577 - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
                        "2024-05-17 17:48:32,586 - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x2b45ef9d0>\n",
                        "2024-05-17 17:48:32,586 - start_tls.started ssl_context=<ssl.SSLContext object at 0x172415eb0> server_hostname='api.openai.com' timeout=5.0\n",
                        "2024-05-17 17:48:32,600 - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1723df790>\n",
                        "2024-05-17 17:48:32,600 - send_request_headers.started request=<Request [b'POST']>\n",
                        "2024-05-17 17:48:32,601 - send_request_headers.complete\n",
                        "2024-05-17 17:48:32,601 - send_request_body.started request=<Request [b'POST']>\n",
                        "2024-05-17 17:48:32,602 - send_request_body.complete\n",
                        "2024-05-17 17:48:32,602 - receive_response_headers.started request=<Request [b'POST']>\n",
                        "2024-05-17 17:48:34,500 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 17 May 2024 21:48:34 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'lastmile-ai'), (b'openai-processing-ms', b'1765'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'2000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'1999945'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'1ms'), (b'x-request-id', b'req_e63c3d9a594b2f39603e483bd9ea6482'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8856cfefddd4438b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
                        "2024-05-17 17:48:34,501 - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "2024-05-17 17:48:34,502 - receive_response_body.started request=<Request [b'POST']>\n",
                        "2024-05-17 17:48:34,503 - receive_response_body.complete\n",
                        "2024-05-17 17:48:34,503 - response_closed.started\n",
                        "2024-05-17 17:48:34,504 - response_closed.complete\n",
                        "2024-05-17 17:48:34,504 - HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Fri, 17 May 2024 21:48:34 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'lastmile-ai', 'openai-processing-ms': '1765', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '2000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '1999945', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '1ms', 'x-request-id': 'req_e63c3d9a594b2f39603e483bd9ea6482', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '8856cfefddd4438b-EWR', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
                        "2024-05-17 17:48:34,505 - request_id: req_e63c3d9a594b2f39603e483bd9ea6482\n",
                        "2024-05-17 17:48:34,542 - https://lastmileai.dev:443 \"POST /api/trace/create HTTP/1.1\" 200 10\n",
                        "2024-05-17 17:48:34,544 - Starting new HTTPS connection (1): lastmileai.dev:443\n",
                        "2024-05-17 17:48:34,634 - https://lastmileai.dev:443 \"POST /api/rag_query_traces/create HTTP/1.1\" 200 None\n"
                    ]
                },
                {
                    "ename": "TypeError",
                    "evalue": "Object of type ChatCompletionMessage is not JSON serializable",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[21], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m messages\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: assistant_message})\n\u001b[1;32m     11\u001b[0m messages\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm in Glasgow, Scotland.\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m---> 12\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# tool_choice=tool_choice,\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/anaconda3/envs/empty2/lib/python3.11/site-packages/lastmile_eval/rag/debugger/tracing/openai.py:345\u001b[0m, in \u001b[0;36mCompletionsV1Wrapper.create\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mChatCompletionWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__completions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtracer\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/anaconda3/envs/empty2/lib/python3.11/site-packages/lastmile_eval/rag/debugger/tracing/openai.py:85\u001b[0m, in \u001b[0;36mChatCompletionWrapper.create\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     84\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_params(kwargs)\n\u001b[0;32m---> 85\u001b[0m     params_flat \u001b[38;5;241m=\u001b[39m \u001b[43mflatten_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracer\u001b[38;5;241m.\u001b[39mstart_as_current_span(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat-completion-span\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m span:\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;66;03m# latest_resolved_prompt = _extract_latest_resolved_prompt(params)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracer\u001b[38;5;241m.\u001b[39mmark_rag_query_trace_event(\n\u001b[1;32m     90\u001b[0m             PromptResolved(fully_resolved_prompt\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mdumps(params))\n\u001b[1;32m     91\u001b[0m         )\n",
                        "File \u001b[0;32m~/anaconda3/envs/empty2/lib/python3.11/site-packages/lastmile_eval/rag/debugger/tracing/openai.py:20\u001b[0m, in \u001b[0;36mflatten_json\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflatten_json\u001b[39m(obj):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n",
                        "File \u001b[0;32m~/anaconda3/envs/empty2/lib/python3.11/site-packages/lastmile_eval/rag/debugger/tracing/openai.py:20\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflatten_json\u001b[39m(obj):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mitems()}\n",
                        "File \u001b[0;32m~/anaconda3/envs/empty2/lib/python3.11/json/__init__.py:231\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# cached encoder\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m skipkeys \u001b[38;5;129;01mand\u001b[39;00m ensure_ascii \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     check_circular \u001b[38;5;129;01mand\u001b[39;00m allow_nan \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m separators \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sort_keys \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n",
                        "File \u001b[0;32m~/anaconda3/envs/empty2/lib/python3.11/json/encoder.py:200\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m encode_basestring(o)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    202\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(chunks)\n",
                        "File \u001b[0;32m~/anaconda3/envs/empty2/lib/python3.11/json/encoder.py:258\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[1;32m    255\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[0;32m--> 258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/anaconda3/envs/empty2/lib/python3.11/json/encoder.py:180\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    181\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
                        "\u001b[0;31mTypeError\u001b[0m: Object of type ChatCompletionMessage is not JSON serializable"
                    ]
                }
            ],
            "source": [
                "messages = []\n",
                "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
                "messages.append({\"role\": \"user\", \"content\": \"What's the weather like today\"})\n",
                "response = client.chat.completions.create(\n",
                "        model=\"gpt-4-turbo\",\n",
                "        messages=messages,\n",
                "        # tool_choice=tool_choice,\n",
                "    )\n",
                "assistant_message = response.choices[0].message\n",
                "messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
                "messages.append({\"role\": \"user\", \"content\": \"I'm in Glasgow, Scotland.\"})\n",
                "response = client.chat.completions.create(\n",
                "        model=\"gpt-4-turbo\",\n",
                "        messages=messages,\n",
                "        # tool_choice=tool_choice,\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "9bae0ccd",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "\"I don't have real-time data access, including current weather conditions. To find out the weather, I recommend checking a reliable weather website like Weather.com, using a weather app, or looking up the weather on a search engine for your location.\""
                        ]
                    },
                    "execution_count": 18,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "response.choices[0].message.content"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "69ee6a93",
            "metadata": {},
            "source": [
                "### Utilities\n",
                "\n",
                "First let's define a few utilities for making calls to the Chat Completions API and for maintaining and keeping track of the conversation state."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "745ceec5",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:45:28.816345Z",
                    "start_time": "2024-05-15T17:45:28.814155Z"
                }
            },
            "outputs": [],
            "source": [
                "@retry(wait=wait_random_exponential(multiplier=1, max=40), stop=stop_after_attempt(3))\n",
                "def chat_completion_request(messages, tools=None, tool_choice=None, model=GPT_MODEL):\n",
                "    response = client.chat.completions.create(\n",
                "        model=model,\n",
                "        messages=messages,\n",
                "        tools=tools,\n",
                "        tool_choice=tool_choice,\n",
                "    )\n",
                "    return response\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "c4d1c99f",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:45:30.003910Z",
                    "start_time": "2024-05-15T17:45:30.001259Z"
                }
            },
            "outputs": [],
            "source": [
                "def pretty_print_conversation(messages):\n",
                "    role_to_color = {\n",
                "        \"system\": \"red\",\n",
                "        \"user\": \"green\",\n",
                "        \"assistant\": \"blue\",\n",
                "        \"function\": \"magenta\",\n",
                "    }\n",
                "    \n",
                "    for message in messages:\n",
                "        if message[\"role\"] == \"system\":\n",
                "            print(colored(f\"system: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
                "        elif message[\"role\"] == \"user\":\n",
                "            print(colored(f\"user: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
                "        elif message[\"role\"] == \"assistant\" and message.get(\"function_call\"):\n",
                "            print(colored(f\"assistant: {message['function_call']}\\n\", role_to_color[message[\"role\"]]))\n",
                "        elif message[\"role\"] == \"assistant\" and not message.get(\"function_call\"):\n",
                "            print(colored(f\"assistant: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
                "        elif message[\"role\"] == \"function\":\n",
                "            print(colored(f\"function ({message['name']}): {message['content']}\\n\", role_to_color[message[\"role\"]]))\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "29d4e02b",
            "metadata": {},
            "source": [
                "### Basic concepts\n",
                "\n",
                "Let's create some function specifications to interface with a hypothetical weather API. We'll pass these function specification to the Chat Completions API in order to generate function arguments that adhere to the specification."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "d2e25069",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:45:31.794879Z",
                    "start_time": "2024-05-15T17:45:31.792617Z"
                }
            },
            "outputs": [],
            "source": [
                "tools = [\n",
                "    {\n",
                "        \"type\": \"function\",\n",
                "        \"function\": {\n",
                "            \"name\": \"get_current_weather\",\n",
                "            \"description\": \"Get the current weather\",\n",
                "            \"parameters\": {\n",
                "                \"type\": \"object\",\n",
                "                \"properties\": {\n",
                "                    \"location\": {\n",
                "                        \"type\": \"string\",\n",
                "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
                "                    },\n",
                "                    \"format\": {\n",
                "                        \"type\": \"string\",\n",
                "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
                "                        \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
                "                    },\n",
                "                },\n",
                "                \"required\": [\"location\", \"format\"],\n",
                "            },\n",
                "        }\n",
                "    },\n",
                "    {\n",
                "        \"type\": \"function\",\n",
                "        \"function\": {\n",
                "            \"name\": \"get_n_day_weather_forecast\",\n",
                "            \"description\": \"Get an N-day weather forecast\",\n",
                "            \"parameters\": {\n",
                "                \"type\": \"object\",\n",
                "                \"properties\": {\n",
                "                    \"location\": {\n",
                "                        \"type\": \"string\",\n",
                "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
                "                    },\n",
                "                    \"format\": {\n",
                "                        \"type\": \"string\",\n",
                "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
                "                        \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
                "                    },\n",
                "                    \"num_days\": {\n",
                "                        \"type\": \"integer\",\n",
                "                        \"description\": \"The number of days to forecast\",\n",
                "                    }\n",
                "                },\n",
                "                \"required\": [\"location\", \"format\", \"num_days\"]\n",
                "            },\n",
                "        }\n",
                "    },\n",
                "]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "bfc39899",
            "metadata": {},
            "source": [
                "If we prompt the model about the current weather, it will respond with some clarifying questions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "518d6827",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:45:35.282310Z",
                    "start_time": "2024-05-15T17:45:33.861496Z"
                }
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2024-05-17 17:36:21,514 - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"}, {'role': 'user', 'content': \"What's the weather like today\"}], 'model': 'gpt-4o', 'tool_choice': None, 'tools': [{'type': 'function', 'function': {'name': 'get_current_weather', 'description': 'Get the current weather', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'format': {'type': 'string', 'enum': ['celsius', 'fahrenheit'], 'description': 'The temperature unit to use. Infer this from the users location.'}}, 'required': ['location', 'format']}}}, {'type': 'function', 'function': {'name': 'get_n_day_weather_forecast', 'description': 'Get an N-day weather forecast', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'format': {'type': 'string', 'enum': ['celsius', 'fahrenheit'], 'description': 'The temperature unit to use. Infer this from the users location.'}, 'num_days': {'type': 'integer', 'description': 'The number of days to forecast'}}, 'required': ['location', 'format', 'num_days']}}}]}}\n",
                        "2024-05-17 17:36:21,524 - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
                        "2024-05-17 17:36:21,525 - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
                        "2024-05-17 17:36:21,533 - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x177a69cd0>\n",
                        "2024-05-17 17:36:21,533 - start_tls.started ssl_context=<ssl.SSLContext object at 0x177e1a570> server_hostname='api.openai.com' timeout=5.0\n",
                        "2024-05-17 17:36:21,545 - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x176d7bf10>\n",
                        "2024-05-17 17:36:21,545 - send_request_headers.started request=<Request [b'POST']>\n",
                        "2024-05-17 17:36:21,546 - send_request_headers.complete\n",
                        "2024-05-17 17:36:21,546 - send_request_body.started request=<Request [b'POST']>\n",
                        "2024-05-17 17:36:21,547 - send_request_body.complete\n",
                        "2024-05-17 17:36:21,547 - receive_response_headers.started request=<Request [b'POST']>\n",
                        "2024-05-17 17:36:22,244 - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 17 May 2024 21:36:22 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'lastmile-ai'), (b'openai-processing-ms', b'466'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'10000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'9999944'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_33a1b71b0ff2cc74be5a9f7b79872d15'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=cvRavB1DeJb_.kkmjAvf1BT2AsHyK7h1WZs_qBWpt0A-1715981782-1.0.1.1-Xa7Qe_2SxbHn6BqmOqtnnAZJytteuo8GnA2ZR0TV4D4EZMU3R5D4Tg94kP.puasyHY0EoSjl.F3jxAzASg4ChQ; path=/; expires=Fri, 17-May-24 22:06:22 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=X7Cqq2xa16a4ez08azwonoo1GkDsLb2Wgme6f6lAn6k-1715981782254-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8856be16de04c47c-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
                        "2024-05-17 17:36:22,247 - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "2024-05-17 17:36:22,248 - receive_response_body.started request=<Request [b'POST']>\n",
                        "2024-05-17 17:36:22,249 - receive_response_body.complete\n",
                        "2024-05-17 17:36:22,250 - response_closed.started\n",
                        "2024-05-17 17:36:22,250 - response_closed.complete\n",
                        "2024-05-17 17:36:22,250 - HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Fri, 17 May 2024 21:36:22 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('openai-organization', 'lastmile-ai'), ('openai-processing-ms', '466'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=15724800; includeSubDomains'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '10000000'), ('x-ratelimit-remaining-requests', '9999'), ('x-ratelimit-remaining-tokens', '9999944'), ('x-ratelimit-reset-requests', '6ms'), ('x-ratelimit-reset-tokens', '0s'), ('x-request-id', 'req_33a1b71b0ff2cc74be5a9f7b79872d15'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=cvRavB1DeJb_.kkmjAvf1BT2AsHyK7h1WZs_qBWpt0A-1715981782-1.0.1.1-Xa7Qe_2SxbHn6BqmOqtnnAZJytteuo8GnA2ZR0TV4D4EZMU3R5D4Tg94kP.puasyHY0EoSjl.F3jxAzASg4ChQ; path=/; expires=Fri, 17-May-24 22:06:22 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('set-cookie', '_cfuvid=X7Cqq2xa16a4ez08azwonoo1GkDsLb2Wgme6f6lAn6k-1715981782254-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '8856be16de04c47c-EWR'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
                        "2024-05-17 17:36:22,251 - request_id: req_33a1b71b0ff2cc74be5a9f7b79872d15\n",
                        "2024-05-17 17:36:22,267 - Starting new HTTPS connection (1): lastmileai.dev:443\n",
                        "2024-05-17 17:36:22,456 - https://lastmileai.dev:443 \"POST /api/trace/create HTTP/1.1\" 200 10\n",
                        "2024-05-17 17:36:22,459 - Starting new HTTPS connection (1): lastmileai.dev:443\n",
                        "2024-05-17 17:36:22,551 - https://lastmileai.dev:443 \"POST /api/rag_query_traces/create HTTP/1.1\" 200 None\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "ChatCompletionMessage(content=\"Sure, could you please provide the city and state (or city and country) for which you'd like to know the weather?\", role='assistant', function_call=None, tool_calls=None)"
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "messages = []\n",
                "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
                "messages.append({\"role\": \"user\", \"content\": \"What's the weather like today\"})\n",
                "chat_response = chat_completion_request(\n",
                "    messages, tools=tools\n",
                ")\n",
                "assistant_message = chat_response.choices[0].message\n",
                "messages.append(assistant_message)\n",
                "assistant_message\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "4c999375",
            "metadata": {},
            "source": [
                "Once we provide the missing information, it will generate the appropriate function arguments for us."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "23c42a6e",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:45:43.553403Z",
                    "start_time": "2024-05-15T17:45:42.205590Z"
                }
            },
            "outputs": [
                {
                    "ename": "RetryError",
                    "evalue": "RetryError[<Future at 0x177a68c90 state=finished raised TypeError>]",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "File \u001b[0;32m~/anaconda3/envs/empty2/lib/python3.11/site-packages/tenacity/__init__.py:470\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 470\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n",
                        "Cell \u001b[0;32mIn[3], line 3\u001b[0m, in \u001b[0;36mchat_completion_request\u001b[0;34m(messages, tools, tool_choice, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;129m@retry\u001b[39m(wait\u001b[38;5;241m=\u001b[39mwait_random_exponential(multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m), stop\u001b[38;5;241m=\u001b[39mstop_after_attempt(\u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat_completion_request\u001b[39m(messages, tools\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, tool_choice\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, model\u001b[38;5;241m=\u001b[39mGPT_MODEL):\n\u001b[0;32m----> 3\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
                        "File \u001b[0;32m~/anaconda3/envs/empty2/lib/python3.11/site-packages/lastmile_eval/rag/debugger/tracing/openai.py:345\u001b[0m, in \u001b[0;36mCompletionsV1Wrapper.create\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mChatCompletionWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__completions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtracer\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/anaconda3/envs/empty2/lib/python3.11/site-packages/lastmile_eval/rag/debugger/tracing/openai.py:85\u001b[0m, in \u001b[0;36mChatCompletionWrapper.create\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_params(kwargs)\n\u001b[0;32m---> 85\u001b[0m params_flat \u001b[38;5;241m=\u001b[39m \u001b[43mflatten_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracer\u001b[38;5;241m.\u001b[39mstart_as_current_span(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat-completion-span\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m span:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# latest_resolved_prompt = _extract_latest_resolved_prompt(params)\u001b[39;00m\n",
                        "File \u001b[0;32m~/anaconda3/envs/empty2/lib/python3.11/site-packages/lastmile_eval/rag/debugger/tracing/openai.py:20\u001b[0m, in \u001b[0;36mflatten_json\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflatten_json\u001b[39m(obj):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n",
                        "File \u001b[0;32m~/anaconda3/envs/empty2/lib/python3.11/site-packages/lastmile_eval/rag/debugger/tracing/openai.py:20\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflatten_json\u001b[39m(obj):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mitems()}\n",
                        "File \u001b[0;32m~/anaconda3/envs/empty2/lib/python3.11/json/__init__.py:231\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m skipkeys \u001b[38;5;129;01mand\u001b[39;00m ensure_ascii \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     check_circular \u001b[38;5;129;01mand\u001b[39;00m allow_nan \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m separators \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sort_keys \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
                        "File \u001b[0;32m~/anaconda3/envs/empty2/lib/python3.11/json/encoder.py:200\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_one_shot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
                        "File \u001b[0;32m~/anaconda3/envs/empty2/lib/python3.11/json/encoder.py:258\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    254\u001b[0m     _iterencode \u001b[38;5;241m=\u001b[39m _make_iterencode(\n\u001b[1;32m    255\u001b[0m         markers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault, _encoder, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindent, floatstr,\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_separator, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_keys,\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipkeys, _one_shot)\n\u001b[0;32m--> 258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_iterencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/anaconda3/envs/empty2/lib/python3.11/json/encoder.py:180\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03ma serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m(to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    181\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
                        "\u001b[0;31mTypeError\u001b[0m: Object of type ChatCompletionMessage is not JSON serializable",
                        "\nThe above exception was the direct cause of the following exception:\n",
                        "\u001b[0;31mRetryError\u001b[0m                                Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m messages\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm in Glasgow, Scotland.\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m----> 2\u001b[0m chat_response \u001b[38;5;241m=\u001b[39m \u001b[43mchat_completion_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m assistant_message \u001b[38;5;241m=\u001b[39m chat_response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\n\u001b[1;32m      6\u001b[0m messages\u001b[38;5;241m.\u001b[39mappend(assistant_message)\n",
                        "File \u001b[0;32m~/anaconda3/envs/empty2/lib/python3.11/site-packages/tenacity/__init__.py:330\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(\n\u001b[1;32m    327\u001b[0m     f, functools\u001b[38;5;241m.\u001b[39mWRAPPER_ASSIGNMENTS \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__defaults__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__kwdefaults__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    328\u001b[0m )\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_f\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[0;32m--> 330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m~/anaconda3/envs/empty2/lib/python3.11/site-packages/tenacity/__init__.py:467\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 467\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
                        "File \u001b[0;32m~/anaconda3/envs/empty2/lib/python3.11/site-packages/tenacity/__init__.py:368\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    366\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[0;32m--> 368\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
                        "File \u001b[0;32m~/anaconda3/envs/empty2/lib/python3.11/site-packages/tenacity/__init__.py:411\u001b[0m, in \u001b[0;36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreraise:\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m retry_exc\u001b[38;5;241m.\u001b[39mreraise()\n\u001b[0;32m--> 411\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfut\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexception\u001b[39;00m()\n",
                        "\u001b[0;31mRetryError\u001b[0m: RetryError[<Future at 0x177a68c90 state=finished raised TypeError>]"
                    ]
                }
            ],
            "source": [
                "messages.append({\"role\": \"user\", \"content\": \"I'm in Glasgow, Scotland.\"})\n",
                "chat_response = chat_completion_request(\n",
                "    messages, tools=tools\n",
                ")\n",
                "assistant_message = chat_response.choices[0].message\n",
                "messages.append(assistant_message)\n",
                "assistant_message\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "c14d4762",
            "metadata": {},
            "source": [
                "By prompting it differently, we can get it to target the other function we've told it about."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fa232e54",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:45:47.090638Z",
                    "start_time": "2024-05-15T17:45:46.302475Z"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "ChatCompletionMessage(content='Please specify the number of days (x) for which you want the weather forecast for Glasgow, Scotland.', role='assistant', function_call=None, tool_calls=None)"
                        ]
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "messages = []\n",
                "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
                "messages.append({\"role\": \"user\", \"content\": \"what is the weather going to be like in Glasgow, Scotland over the next x days\"})\n",
                "chat_response = chat_completion_request(\n",
                "    messages, tools=tools\n",
                ")\n",
                "assistant_message = chat_response.choices[0].message\n",
                "messages.append(assistant_message)\n",
                "assistant_message\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "6172ddac",
            "metadata": {},
            "source": [
                "Once again, the model is asking us for clarification because it doesn't have enough information yet. In this case it already knows the location for the forecast, but it needs to know how many days are required in the forecast."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c7d8a543",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:45:49.790820Z",
                    "start_time": "2024-05-15T17:45:48.847752Z"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_Yg5ydH9lHhLjjYQyXbNvh004', function=Function(arguments='{\"location\":\"Glasgow, Scotland\",\"format\":\"celsius\",\"num_days\":5}', name='get_n_day_weather_forecast'), type='function')]))"
                        ]
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "messages.append({\"role\": \"user\", \"content\": \"5 days\"})\n",
                "chat_response = chat_completion_request(\n",
                "    messages, tools=tools\n",
                ")\n",
                "chat_response.choices[0]\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "4b758a0a",
            "metadata": {},
            "source": [
                "#### Forcing the use of specific functions or no function"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "412f79ba",
            "metadata": {},
            "source": [
                "We can force the model to use a specific function, for example get_n_day_weather_forecast by using the function_call argument. By doing so, we force the model to make assumptions about how to use it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "559371b7",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:45:54.194255Z",
                    "start_time": "2024-05-15T17:45:52.975746Z"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_aP8ZEtGcyseL0btTMYxTCKbk', function=Function(arguments='{\"location\":\"Toronto, Canada\",\"format\":\"celsius\",\"num_days\":1}', name='get_n_day_weather_forecast'), type='function')])"
                        ]
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# in this cell we force the model to use get_n_day_weather_forecast\n",
                "messages = []\n",
                "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
                "messages.append({\"role\": \"user\", \"content\": \"Give me a weather report for Toronto, Canada.\"})\n",
                "chat_response = chat_completion_request(\n",
                "    messages, tools=tools, tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_n_day_weather_forecast\"}}\n",
                ")\n",
                "chat_response.choices[0].message"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a7ab0f58",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:45:56.841233Z",
                    "start_time": "2024-05-15T17:45:55.433397Z"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_5HqCVRaAoBuU0uTlO3MUwaWX', function=Function(arguments='{\"location\": \"Toronto, Canada\", \"format\": \"celsius\"}', name='get_current_weather'), type='function'), ChatCompletionMessageToolCall(id='call_C9kCha28xHEsxYl4PxZ1l5LI', function=Function(arguments='{\"location\": \"Toronto, Canada\", \"format\": \"celsius\", \"num_days\": 3}', name='get_n_day_weather_forecast'), type='function')])"
                        ]
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# if we don't force the model to use get_n_day_weather_forecast it may not\n",
                "messages = []\n",
                "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
                "messages.append({\"role\": \"user\", \"content\": \"Give me a weather report for Toronto, Canada.\"})\n",
                "chat_response = chat_completion_request(\n",
                "    messages, tools=tools\n",
                ")\n",
                "chat_response.choices[0].message"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "3bd70e48",
            "metadata": {},
            "source": [
                "We can also force the model to not use a function at all. By doing so we prevent it from producing a proper function call."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "acfe54e6",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:45:59.800346Z",
                    "start_time": "2024-05-15T17:45:59.289603Z"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "ChatCompletionMessage(content=\"I'll get the current weather for Toronto, Canada in Celsius.\", role='assistant', function_call=None, tool_calls=None)"
                        ]
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "messages = []\n",
                "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
                "messages.append({\"role\": \"user\", \"content\": \"Give me the current weather (use Celcius) for Toronto, Canada.\"})\n",
                "chat_response = chat_completion_request(\n",
                "    messages, tools=tools, tool_choice=\"none\"\n",
                ")\n",
                "chat_response.choices[0].message\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b616353b",
            "metadata": {},
            "source": [
                "### Parallel Function Calling\n",
                "\n",
                "Newer models such as gpt-4o or gpt-3.5-turbo can call multiple functions in one turn."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "380eeb68",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:46:04.048553Z",
                    "start_time": "2024-05-15T17:46:01.273501Z"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[ChatCompletionMessageToolCall(id='call_pFdKcCu5taDTtOOfX14vEDRp', function=Function(arguments='{\"location\": \"San Francisco, CA\", \"format\": \"fahrenheit\", \"num_days\": 4}', name='get_n_day_weather_forecast'), type='function'),\n",
                            " ChatCompletionMessageToolCall(id='call_Veeyp2hYJOKp0wT7ODxmTjaS', function=Function(arguments='{\"location\": \"Glasgow, UK\", \"format\": \"celsius\", \"num_days\": 4}', name='get_n_day_weather_forecast'), type='function')]"
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "messages = []\n",
                "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
                "messages.append({\"role\": \"user\", \"content\": \"what is the weather going to be like in San Francisco and Glasgow over the next 4 days\"})\n",
                "chat_response = chat_completion_request(\n",
                "    messages, tools=tools, model=GPT_MODEL\n",
                ")\n",
                "\n",
                "assistant_message = chat_response.choices[0].message.tool_calls\n",
                "assistant_message"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "b4482aee",
            "metadata": {},
            "source": [
                "## How to call functions with model generated arguments\n",
                "\n",
                "In our next example, we'll demonstrate how to execute functions whose inputs are model-generated, and use this to implement an agent that can answer questions for us about a database. For simplicity we'll use the [Chinook sample database](https://www.sqlitetutorial.net/sqlite-sample-database/).\n",
                "\n",
                "*Note:* SQL generation can be high-risk in a production environment since models are not perfectly reliable at generating correct SQL."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "f7654fef",
            "metadata": {},
            "source": [
                "### Specifying a function to execute SQL queries\n",
                "\n",
                "First let's define some helpful utility functions to extract data from a SQLite database."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "30f6b60e",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:46:07.270851Z",
                    "start_time": "2024-05-15T17:46:07.265545Z"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Opened database successfully\n"
                    ]
                }
            ],
            "source": [
                "import sqlite3\n",
                "\n",
                "conn = sqlite3.connect(\"data/Chinook.db\")\n",
                "print(\"Opened database successfully\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "abec0214",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:46:09.345308Z",
                    "start_time": "2024-05-15T17:46:09.342998Z"
                }
            },
            "outputs": [],
            "source": [
                "def get_table_names(conn):\n",
                "    \"\"\"Return a list of table names.\"\"\"\n",
                "    table_names = []\n",
                "    tables = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
                "    for table in tables.fetchall():\n",
                "        table_names.append(table[0])\n",
                "    return table_names\n",
                "\n",
                "\n",
                "def get_column_names(conn, table_name):\n",
                "    \"\"\"Return a list of column names.\"\"\"\n",
                "    column_names = []\n",
                "    columns = conn.execute(f\"PRAGMA table_info('{table_name}');\").fetchall()\n",
                "    for col in columns:\n",
                "        column_names.append(col[1])\n",
                "    return column_names\n",
                "\n",
                "\n",
                "def get_database_info(conn):\n",
                "    \"\"\"Return a list of dicts containing the table name and columns for each table in the database.\"\"\"\n",
                "    table_dicts = []\n",
                "    for table_name in get_table_names(conn):\n",
                "        columns_names = get_column_names(conn, table_name)\n",
                "        table_dicts.append({\"table_name\": table_name, \"column_names\": columns_names})\n",
                "    return table_dicts\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "77e6e5ea",
            "metadata": {},
            "source": [
                "Now can use these utility functions to extract a representation of the database schema."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0c0104cd",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:46:11.303746Z",
                    "start_time": "2024-05-15T17:46:11.301210Z"
                }
            },
            "outputs": [],
            "source": [
                "database_schema_dict = get_database_info(conn)\n",
                "database_schema_string = \"\\n\".join(\n",
                "    [\n",
                "        f\"Table: {table['table_name']}\\nColumns: {', '.join(table['column_names'])}\"\n",
                "        for table in database_schema_dict\n",
                "    ]\n",
                ")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "ae73c9ee",
            "metadata": {},
            "source": [
                "As before, we'll define a function specification for the function we'd like the API to generate arguments for. Notice that we are inserting the database schema into the function specification. This will be important for the model to know about."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0258813a",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:46:16.569530Z",
                    "start_time": "2024-05-15T17:46:16.567801Z"
                }
            },
            "outputs": [],
            "source": [
                "tools = [\n",
                "    {\n",
                "        \"type\": \"function\",\n",
                "        \"function\": {\n",
                "            \"name\": \"ask_database\",\n",
                "            \"description\": \"Use this function to answer user questions about music. Input should be a fully formed SQL query.\",\n",
                "            \"parameters\": {\n",
                "                \"type\": \"object\",\n",
                "                \"properties\": {\n",
                "                    \"query\": {\n",
                "                        \"type\": \"string\",\n",
                "                        \"description\": f\"\"\"\n",
                "                                SQL query extracting info to answer the user's question.\n",
                "                                SQL should be written using this database schema:\n",
                "                                {database_schema_string}\n",
                "                                The query should be returned in plain text, not in JSON.\n",
                "                                \"\"\",\n",
                "                    }\n",
                "                },\n",
                "                \"required\": [\"query\"],\n",
                "            },\n",
                "        }\n",
                "    }\n",
                "]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "da08c121",
            "metadata": {},
            "source": [
                "### Executing SQL queries\n",
                "\n",
                "Now let's implement the function that will actually excute queries against the database."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "65585e74",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:46:19.198723Z",
                    "start_time": "2024-05-15T17:46:19.197043Z"
                }
            },
            "outputs": [],
            "source": [
                "def ask_database(conn, query):\n",
                "    \"\"\"Function to query SQLite database with a provided SQL query.\"\"\"\n",
                "    try:\n",
                "        results = str(conn.execute(query).fetchall())\n",
                "    except Exception as e:\n",
                "        results = f\"query failed with error: {e}\"\n",
                "    return results"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8f6885e9f0af5c40",
            "metadata": {},
            "source": [
                "##### Steps to invoke a function call using Chat Completions API: \n",
                "\n",
                "**Step 1**: Prompt the model with content that may result in model selecting a tool to use. The description of the tools such as a function names and signature is defined in the 'Tools' list and passed to the model in API call. If selected, the function name and parameters are included in the response.<br>\n",
                "  \n",
                "**Step 2**: Check programmatically if model wanted to call a function. If true, proceed to step 3. <br>  \n",
                "**Step 3**: Extract the function name and parameters from response, call the function with parameters. Append the result to messages. <br>    \n",
                "**Step 4**: Invoke the chat completions API with the message list to get the response. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e8b7cb9cdc7a7616",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:46:25.725379Z",
                    "start_time": "2024-05-15T17:46:24.255505Z"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_bXMf903yO78sdsMZble4yu90', function=Function(arguments='{\"query\":\"SELECT A.Title, COUNT(T.TrackId) AS TrackCount FROM Album A JOIN Track T ON A.AlbumId = T.AlbumId GROUP BY A.Title ORDER BY TrackCount DESC LIMIT 1;\"}', name='ask_database'), type='function')])\n"
                    ]
                }
            ],
            "source": [
                "# Step #1: Prompt with content that may result in function call. In this case the model can identify the information requested by the user is potentially available in the database schema passed to the model in Tools description. \n",
                "messages = [{\n",
                "    \"role\":\"user\", \n",
                "    \"content\": \"What is the name of the album with the most tracks?\"\n",
                "}]\n",
                "\n",
                "response = client.chat.completions.create(\n",
                "    model='gpt-4o', \n",
                "    messages=messages, \n",
                "    tools= tools, \n",
                "    tool_choice=\"auto\"\n",
                ")\n",
                "\n",
                "# Append the message to messages list\n",
                "response_message = response.choices[0].message \n",
                "messages.append(response_message)\n",
                "\n",
                "print(response_message)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "351c39def3417776",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:46:30.346444Z",
                    "start_time": "2024-05-15T17:46:29.699046Z"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "The album with the most tracks is titled \"Greatest Hits,\" and it contains 57 tracks.\n"
                    ]
                }
            ],
            "source": [
                "# Step 2: determine if the response from the model includes a tool call.   \n",
                "tool_calls = response_message.tool_calls\n",
                "if tool_calls:\n",
                "    # If true the model will return the name of the tool / function to call and the argument(s)  \n",
                "    tool_call_id = tool_calls[0].id\n",
                "    tool_function_name = tool_calls[0].function.name\n",
                "    tool_query_string = eval(tool_calls[0].function.arguments)['query']\n",
                "    \n",
                "    # Step 3: Call the function and retrieve results. Append the results to the messages list.      \n",
                "    if tool_function_name == 'ask_database':\n",
                "        results = ask_database(conn, tool_query_string)\n",
                "        \n",
                "        messages.append({\n",
                "            \"role\":\"tool\", \n",
                "            \"tool_call_id\":tool_call_id, \n",
                "            \"name\": tool_function_name, \n",
                "            \"content\":results\n",
                "        })\n",
                "        \n",
                "        # Step 4: Invoke the chat completions API with the function response appended to the messages list\n",
                "        # Note that messages with role 'tool' must be a response to a preceding message with 'tool_calls'\n",
                "        model_response_with_function_call = client.chat.completions.create(\n",
                "            model=\"gpt-4o\",\n",
                "            messages=messages,\n",
                "        )  # get a new response from the model where it can see the function response\n",
                "        print(model_response_with_function_call.choices[0].message.content)\n",
                "    else: \n",
                "        print(f\"Error: function {tool_function_name} does not exist\")\n",
                "else: \n",
                "    # Model did not identify a function to call, result can be returned to the user \n",
                "    print(response_message.content) "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "2d89073c",
            "metadata": {},
            "source": [
                "## Next Steps\n",
                "\n",
                "See our other [notebook](How_to_call_functions_for_knowledge_retrieval.ipynb) that demonstrates how to use the Chat Completions API and functions for knowledge retrieval to interact conversationally with a knowledge base."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
