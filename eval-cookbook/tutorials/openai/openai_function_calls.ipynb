{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e67f200",
   "metadata": {},
   "source": [
    "# How to call functions with chat models\n",
    "\n",
    "This tutorial is built upon the [function call cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_call_functions_with_chat_models.ipynb) from OpenAI."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64c85e26",
   "metadata": {},
   "source": [
    "## How to generate function arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80e71f33",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# !pip install scipy --quiet\n",
    "# !pip install tenacity --quiet\n",
    "# !pip install tiktoken --quiet\n",
    "# !pip install termcolor --quiet\n",
    "# !pip install openai --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43a2b240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ~/.env file with this line: OPENAI_API_KEY=<your key here>\n",
    "# You can get your key from https://platform.openai.com/api-keys \n",
    "import openai\n",
    "import dotenv\n",
    "import os\n",
    "dotenv.load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GPT_MODEL = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87cf57f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG] 2024-07-03 12:19:50,307 _config.py:80: load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "[DEBUG] 2024-07-03 12:19:50,308 _config.py:146: load_verify_locations cafile='/opt/homebrew/Caskroom/miniconda/base/envs/eval/lib/python3.12/site-packages/certifi/cacert.pem'\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "from lastmile_eval.rag.debugger.api.tracing import LastMileTracer\n",
    "\n",
    "from tracing_auto_instrumentation.openai import wrap_openai\n",
    "from lastmile_eval.rag.debugger.tracing.sdk import get_lastmile_tracer\n",
    "\n",
    "tracer: LastMileTracer = get_lastmile_tracer(\n",
    "    tracer_name=\"OpenAI Text Calling w. Streaming\",\n",
    ")\n",
    "client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "client = wrap_openai(client, tracer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dab872c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T17:45:23.563149Z",
     "start_time": "2024-05-15T17:45:22.925978Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "from termcolor import colored"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69ee6a93",
   "metadata": {},
   "source": [
    "### Utilities\n",
    "\n",
    "First let's define a few utilities for making calls to the Chat Completions API and for maintaining and keeping track of the conversation state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "745ceec5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T17:45:28.816345Z",
     "start_time": "2024-05-15T17:45:28.814155Z"
    }
   },
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(multiplier=1, max=40), stop=stop_after_attempt(3))\n",
    "def chat_completion_request(messages, tools=None, tool_choice=None, model=GPT_MODEL, stream=True):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            tool_choice=tool_choice,\n",
    "            stream=stream,\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(\"Unable to generate ChatCompletion response\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4d1c99f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T17:45:30.003910Z",
     "start_time": "2024-05-15T17:45:30.001259Z"
    }
   },
   "outputs": [],
   "source": [
    "def pretty_print_conversation(messages):\n",
    "    role_to_color = {\n",
    "        \"system\": \"red\",\n",
    "        \"user\": \"green\",\n",
    "        \"assistant\": \"blue\",\n",
    "        \"function\": \"magenta\",\n",
    "    }\n",
    "    \n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"system\":\n",
    "            print(colored(f\"system: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
    "        elif message[\"role\"] == \"user\":\n",
    "            print(colored(f\"user: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
    "        elif message[\"role\"] == \"assistant\" and message.get(\"function_call\"):\n",
    "            print(colored(f\"assistant: {message['function_call']}\\n\", role_to_color[message[\"role\"]]))\n",
    "        elif message[\"role\"] == \"assistant\" and not message.get(\"function_call\"):\n",
    "            print(colored(f\"assistant: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
    "        elif message[\"role\"] == \"function\":\n",
    "            print(colored(f\"function ({message['name']}): {message['content']}\\n\", role_to_color[message[\"role\"]]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29d4e02b",
   "metadata": {},
   "source": [
    "### Basic concepts\n",
    "\n",
    "Let's create some function specifications to interface with a hypothetical weather API. We'll pass these function specification to the Chat Completions API in order to generate function arguments that adhere to the specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2e25069",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T17:45:31.794879Z",
     "start_time": "2024-05-15T17:45:31.792617Z"
    }
   },
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"format\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                        \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\", \"format\"],\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_n_day_weather_forecast\",\n",
    "            \"description\": \"Get an N-day weather forecast\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"format\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                        \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
    "                    },\n",
    "                    \"num_days\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The number of days to forecast\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\", \"format\", \"num_days\"]\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfc39899",
   "metadata": {},
   "source": [
    "## Non-Async Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "518d6827",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T17:45:35.282310Z",
     "start_time": "2024-05-15T17:45:33.861496Z"
    }
   },
   "outputs": [],
   "source": [
    "def execute_tool_call_and_display_response(stream: bool):\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
    "    messages.append({\"role\": \"user\", \"content\": \"What's the weather like today in Glasgow, Scotland?\"})\n",
    "    chat_response = chat_completion_request(\n",
    "        messages, tools=tools, stream=stream\n",
    "    )\n",
    "    if stream:\n",
    "        for chunk in chat_response:\n",
    "            print(f\"{chunk=}\")\n",
    "    else:\n",
    "        print(f\"{chat_response=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e4ff021",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG] 2024-07-03 12:19:50,342 _base_client.py:446: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"}, {'role': 'user', 'content': \"What's the weather like today in Glasgow, Scotland?\"}], 'model': 'gpt-3.5-turbo', 'stream': False, 'tool_choice': None, 'tools': [{'type': 'function', 'function': {'name': 'get_current_weather', 'description': 'Get the current weather', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'format': {'type': 'string', 'enum': ['celsius', 'fahrenheit'], 'description': 'The temperature unit to use. Infer this from the users location.'}}, 'required': ['location', 'format']}}}, {'type': 'function', 'function': {'name': 'get_n_day_weather_forecast', 'description': 'Get an N-day weather forecast', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'format': {'type': 'string', 'enum': ['celsius', 'fahrenheit'], 'description': 'The temperature unit to use. Infer this from the users location.'}, 'num_days': {'type': 'integer', 'description': 'The number of days to forecast'}}, 'required': ['location', 'format', 'num_days']}}}]}}\n",
      "[DEBUG] 2024-07-03 12:19:50,349 _base_client.py:949: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "[DEBUG] 2024-07-03 12:19:50,350 _trace.py:45: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "[DEBUG] 2024-07-03 12:19:50,401 _trace.py:45: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x12fd27830>\n",
      "[DEBUG] 2024-07-03 12:19:50,401 _trace.py:45: start_tls.started ssl_context=<ssl.SSLContext object at 0x12fca6fd0> server_hostname='api.openai.com' timeout=5.0\n",
      "[DEBUG] 2024-07-03 12:19:50,436 _trace.py:45: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x12fc7efc0>\n",
      "[DEBUG] 2024-07-03 12:19:50,436 _trace.py:45: send_request_headers.started request=<Request [b'POST']>\n",
      "[DEBUG] 2024-07-03 12:19:50,437 _trace.py:45: send_request_headers.complete\n",
      "[DEBUG] 2024-07-03 12:19:50,437 _trace.py:45: send_request_body.started request=<Request [b'POST']>\n",
      "[DEBUG] 2024-07-03 12:19:50,438 _trace.py:45: send_request_body.complete\n",
      "[DEBUG] 2024-07-03 12:19:50,438 _trace.py:45: receive_response_headers.started request=<Request [b'POST']>\n",
      "[DEBUG] 2024-07-03 12:19:51,297 _trace.py:45: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 03 Jul 2024 19:19:51 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'lastmile-ai'), (b'openai-processing-ms', b'596'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'4999939'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_79aa394bacf95530e010f807563eb7ab'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=1tZfbTd8w3fpMS9xEBcFEnO4Rc8H2qZ7E5WpriX8UCo-1720034391-1.0.1.1-7oBwhNyJeKo4e08SvuXWHotVzzc68Csr_2zYNNpaSpkjIJoFzVCYE_HiYBkgDCOBcATWOU2zjPzCGeH34G7vKQ; path=/; expires=Wed, 03-Jul-24 19:49:51 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=1KxyEVm9zLgAsu4Bya4gWRtx_jdkGM8VvCE_VL_nAWs-1720034391292-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89d93abc5cb4cf82-SJC'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "[INFO] 2024-07-03 12:19:51,301 _client.py:1026: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[DEBUG] 2024-07-03 12:19:51,302 _trace.py:45: receive_response_body.started request=<Request [b'POST']>\n",
      "[DEBUG] 2024-07-03 12:19:51,304 _trace.py:45: receive_response_body.complete\n",
      "[DEBUG] 2024-07-03 12:19:51,305 _trace.py:45: response_closed.started\n",
      "[DEBUG] 2024-07-03 12:19:51,306 _trace.py:45: response_closed.complete\n",
      "[DEBUG] 2024-07-03 12:19:51,306 _base_client.py:988: HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Wed, 03 Jul 2024 19:19:51 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('openai-organization', 'lastmile-ai'), ('openai-processing-ms', '596'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '5000000'), ('x-ratelimit-remaining-requests', '9999'), ('x-ratelimit-remaining-tokens', '4999939'), ('x-ratelimit-reset-requests', '6ms'), ('x-ratelimit-reset-tokens', '0s'), ('x-request-id', 'req_79aa394bacf95530e010f807563eb7ab'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=1tZfbTd8w3fpMS9xEBcFEnO4Rc8H2qZ7E5WpriX8UCo-1720034391-1.0.1.1-7oBwhNyJeKo4e08SvuXWHotVzzc68Csr_2zYNNpaSpkjIJoFzVCYE_HiYBkgDCOBcATWOU2zjPzCGeH34G7vKQ; path=/; expires=Wed, 03-Jul-24 19:49:51 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('set-cookie', '_cfuvid=1KxyEVm9zLgAsu4Bya4gWRtx_jdkGM8VvCE_VL_nAWs-1720034391292-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '89d93abc5cb4cf82-SJC'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "[DEBUG] 2024-07-03 12:19:51,307 _base_client.py:996: request_id: req_79aa394bacf95530e010f807563eb7ab\n",
      "[DEBUG] 2024-07-03 12:19:51,324 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
      "[DEBUG] 2024-07-03 12:19:51,711 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/trace/create HTTP/11\" 200 10\n",
      "[DEBUG] 2024-07-03 12:19:51,713 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
      "[DEBUG] 2024-07-03 12:19:52,099 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/rag_query_traces/create HTTP/11\" 200 None\n",
      "[DEBUG] 2024-07-03 12:19:52,101 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
      "[DEBUG] 2024-07-03 12:19:52,483 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/rag_events/create HTTP/11\" 200 None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat_response=ChatCompletion(id='chatcmpl-9gzo6zigtb417lthNFUclxw8UOnyy', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_MPpMg2sDWn0YhIU1ivH6q7do', function=Function(arguments='{\"location\":\"Glasgow, Scotland\",\"format\":\"celsius\"}', name='get_current_weather'), type='function')]))], created=1720034390, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=24, prompt_tokens=196, total_tokens=220))\n"
     ]
    }
   ],
   "source": [
    "# Without Streaming\n",
    "execute_tool_call_and_display_response(stream=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1833cb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG] 2024-07-03 12:19:52,491 _base_client.py:446: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"}, {'role': 'user', 'content': \"What's the weather like today in Glasgow, Scotland?\"}], 'model': 'gpt-3.5-turbo', 'stream': True, 'tool_choice': None, 'tools': [{'type': 'function', 'function': {'name': 'get_current_weather', 'description': 'Get the current weather', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'format': {'type': 'string', 'enum': ['celsius', 'fahrenheit'], 'description': 'The temperature unit to use. Infer this from the users location.'}}, 'required': ['location', 'format']}}}, {'type': 'function', 'function': {'name': 'get_n_day_weather_forecast', 'description': 'Get an N-day weather forecast', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'format': {'type': 'string', 'enum': ['celsius', 'fahrenheit'], 'description': 'The temperature unit to use. Infer this from the users location.'}, 'num_days': {'type': 'integer', 'description': 'The number of days to forecast'}}, 'required': ['location', 'format', 'num_days']}}}]}}\n",
      "[DEBUG] 2024-07-03 12:19:52,492 _base_client.py:949: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
      "[DEBUG] 2024-07-03 12:19:52,493 _trace.py:45: send_request_headers.started request=<Request [b'POST']>\n",
      "[DEBUG] 2024-07-03 12:19:52,493 _trace.py:45: send_request_headers.complete\n",
      "[DEBUG] 2024-07-03 12:19:52,494 _trace.py:45: send_request_body.started request=<Request [b'POST']>\n",
      "[DEBUG] 2024-07-03 12:19:52,494 _trace.py:45: send_request_body.complete\n",
      "[DEBUG] 2024-07-03 12:19:52,494 _trace.py:45: receive_response_headers.started request=<Request [b'POST']>\n",
      "[DEBUG] 2024-07-03 12:19:53,085 _trace.py:45: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 03 Jul 2024 19:19:53 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'lastmile-ai'), (b'openai-processing-ms', b'389'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'4999939'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_efcd0a2bf8ccde6c502305d7c2e31c93'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89d93ac93d93cf82-SJC'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "[INFO] 2024-07-03 12:19:53,085 _client.py:1026: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[DEBUG] 2024-07-03 12:19:53,086 _base_client.py:988: HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Wed, 03 Jul 2024 19:19:53 GMT', 'content-type': 'text/event-stream; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'lastmile-ai', 'openai-processing-ms': '389', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '5000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '4999939', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '0s', 'x-request-id': 'req_efcd0a2bf8ccde6c502305d7c2e31c93', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89d93ac93d93cf82-SJC', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "[DEBUG] 2024-07-03 12:19:53,086 _base_client.py:996: request_id: req_efcd0a2bf8ccde6c502305d7c2e31c93\n",
      "[DEBUG] 2024-07-03 12:19:53,086 _trace.py:45: receive_response_body.started request=<Request [b'POST']>\n",
      "[DEBUG] 2024-07-03 12:19:53,255 _trace.py:45: receive_response_body.complete\n",
      "[DEBUG] 2024-07-03 12:19:53,256 _trace.py:45: response_closed.started\n",
      "[DEBUG] 2024-07-03 12:19:53,257 _trace.py:45: response_closed.complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk=ChatCompletionChunk(id='chatcmpl-9gzo899c0NsVrtVO0vPOKRMzW7Crm', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=0, id='call_ocxcueR45covydAzV4LhpkGo', function=ChoiceDeltaToolCallFunction(arguments='', name='get_current_weather'), type='function')]), finish_reason=None, index=0, logprobs=None)], created=1720034392, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "chunk=ChatCompletionChunk(id='chatcmpl-9gzo899c0NsVrtVO0vPOKRMzW7Crm', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='{\"', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1720034392, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "chunk=ChatCompletionChunk(id='chatcmpl-9gzo899c0NsVrtVO0vPOKRMzW7Crm', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='location', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1720034392, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "chunk=ChatCompletionChunk(id='chatcmpl-9gzo899c0NsVrtVO0vPOKRMzW7Crm', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\":\"', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1720034392, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "chunk=ChatCompletionChunk(id='chatcmpl-9gzo899c0NsVrtVO0vPOKRMzW7Crm', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='G', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1720034392, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "chunk=ChatCompletionChunk(id='chatcmpl-9gzo899c0NsVrtVO0vPOKRMzW7Crm', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='las', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1720034392, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "chunk=ChatCompletionChunk(id='chatcmpl-9gzo899c0NsVrtVO0vPOKRMzW7Crm', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='gow', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1720034392, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "chunk=ChatCompletionChunk(id='chatcmpl-9gzo899c0NsVrtVO0vPOKRMzW7Crm', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=',', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1720034392, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "chunk=ChatCompletionChunk(id='chatcmpl-9gzo899c0NsVrtVO0vPOKRMzW7Crm', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' Scotland', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1720034392, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "chunk=ChatCompletionChunk(id='chatcmpl-9gzo899c0NsVrtVO0vPOKRMzW7Crm', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\",\"', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1720034392, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "chunk=ChatCompletionChunk(id='chatcmpl-9gzo899c0NsVrtVO0vPOKRMzW7Crm', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='format', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1720034392, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "chunk=ChatCompletionChunk(id='chatcmpl-9gzo899c0NsVrtVO0vPOKRMzW7Crm', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\":\"', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1720034392, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "chunk=ChatCompletionChunk(id='chatcmpl-9gzo899c0NsVrtVO0vPOKRMzW7Crm', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='c', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1720034392, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "chunk=ChatCompletionChunk(id='chatcmpl-9gzo899c0NsVrtVO0vPOKRMzW7Crm', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='elsius', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1720034392, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "chunk=ChatCompletionChunk(id='chatcmpl-9gzo899c0NsVrtVO0vPOKRMzW7Crm', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\"}', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1720034392, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "chunk=ChatCompletionChunk(id='chatcmpl-9gzo899c0NsVrtVO0vPOKRMzW7Crm', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None), finish_reason='tool_calls', index=0, logprobs=None)], created=1720034392, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG] 2024-07-03 12:19:53,385 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/trace/create HTTP/11\" 200 10\n",
      "[DEBUG] 2024-07-03 12:19:53,387 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
      "[DEBUG] 2024-07-03 12:19:53,785 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/rag_query_traces/create HTTP/11\" 200 None\n",
      "[DEBUG] 2024-07-03 12:19:53,787 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
      "[DEBUG] 2024-07-03 12:19:54,188 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/rag_events/create HTTP/11\" 200 None\n"
     ]
    }
   ],
   "source": [
    "# With Streaming\n",
    "execute_tool_call_and_display_response(stream=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fafdc2",
   "metadata": {},
   "source": [
    "## Async Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "727944dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG] 2024-07-03 12:23:18,435 _config.py:80: load_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "[DEBUG] 2024-07-03 12:23:18,437 _config.py:146: load_verify_locations cafile='/opt/homebrew/Caskroom/miniconda/base/envs/eval/lib/python3.12/site-packages/certifi/cacert.pem'\n"
     ]
    }
   ],
   "source": [
    "client = openai.AsyncOpenAI(api_key=OPENAI_API_KEY)\n",
    "client = wrap_openai(client, tracer)\n",
    "\n",
    "@retry(wait=wait_random_exponential(multiplier=1, max=40), stop=stop_after_attempt(3))\n",
    "async def async_chat_completion_request(messages, tools=None, tool_choice=None, model=GPT_MODEL, stream=True):\n",
    "    try:\n",
    "        print(\"about to print response\")\n",
    "        response = await client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            tool_choice=tool_choice,\n",
    "            stream=stream,\n",
    "        )\n",
    "        print(f\"{response=}\")\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(\"Unable to generate ChatCompletion response\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63447cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def execute_async_tool_call_and_display_response(stream: bool):\n",
    "    messages = []\n",
    "    messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
    "    messages.append({\"role\": \"user\", \"content\": \"What's the weather like today in Glasgow, Scotland?\"})\n",
    "    chat_response = await async_chat_completion_request(\n",
    "        messages, tools=tools, stream=stream\n",
    "    )\n",
    "    if stream:\n",
    "        async for chunk in chat_response:\n",
    "            print(f\"{chunk=}\")\n",
    "    else:\n",
    "        print(f\"{chat_response=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec915fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG] 2024-07-03 12:23:22,703 _base_client.py:446: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"}, {'role': 'user', 'content': \"What's the weather like today in Glasgow, Scotland?\"}], 'model': 'gpt-3.5-turbo', 'stream': False, 'tool_choice': None, 'tools': [{'type': 'function', 'function': {'name': 'get_current_weather', 'description': 'Get the current weather', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'format': {'type': 'string', 'enum': ['celsius', 'fahrenheit'], 'description': 'The temperature unit to use. Infer this from the users location.'}}, 'required': ['location', 'format']}}}, {'type': 'function', 'function': {'name': 'get_n_day_weather_forecast', 'description': 'Get an N-day weather forecast', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'format': {'type': 'string', 'enum': ['celsius', 'fahrenheit'], 'description': 'The temperature unit to use. Infer this from the users location.'}, 'num_days': {'type': 'integer', 'description': 'The number of days to forecast'}}, 'required': ['location', 'format', 'num_days']}}}]}}\n",
      "[DEBUG] 2024-07-03 12:23:22,714 _trace.py:85: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "[DEBUG] 2024-07-03 12:23:22,777 _trace.py:85: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x12f4d2cf0>\n",
      "[DEBUG] 2024-07-03 12:23:22,777 _trace.py:85: start_tls.started ssl_context=<ssl.SSLContext object at 0x138560cd0> server_hostname='api.openai.com' timeout=5.0\n",
      "[DEBUG] 2024-07-03 12:23:22,814 _trace.py:85: start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x12fff02c0>\n",
      "[DEBUG] 2024-07-03 12:23:22,816 _trace.py:85: send_request_headers.started request=<Request [b'POST']>\n",
      "[DEBUG] 2024-07-03 12:23:22,817 _trace.py:85: send_request_headers.complete\n",
      "[DEBUG] 2024-07-03 12:23:22,818 _trace.py:85: send_request_body.started request=<Request [b'POST']>\n",
      "[DEBUG] 2024-07-03 12:23:22,819 _trace.py:85: send_request_body.complete\n",
      "[DEBUG] 2024-07-03 12:23:22,820 _trace.py:85: receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about to print response\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG] 2024-07-03 12:23:23,630 _trace.py:85: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 03 Jul 2024 19:23:23 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'lastmile-ai'), (b'openai-processing-ms', b'673'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'4999939'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_a001c2759259c61e9977e5f50f2c3273'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=4V12cAGwv66SHQAVefqF38wkkW6lnzRrdBo6LYvkF2c-1720034603-1.0.1.1-4TkYq9b2tREAIym9hb.v94bm0Wv_KhgWbm8yA5d7adUM.GOBd0Dti36nV7TuyDbI_TZvyCmb8s4EzUnapjBqdw; path=/; expires=Wed, 03-Jul-24 19:53:23 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=EWv3PlT.levuGjl7k6viEaClkwr0EAblv9i3L12qIUQ-1720034603622-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89d93febb8baf941-SJC'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "[INFO] 2024-07-03 12:23:23,633 _client.py:1773: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[DEBUG] 2024-07-03 12:23:23,635 _trace.py:85: receive_response_body.started request=<Request [b'POST']>\n",
      "[DEBUG] 2024-07-03 12:23:23,636 _trace.py:85: receive_response_body.complete\n",
      "[DEBUG] 2024-07-03 12:23:23,637 _trace.py:85: response_closed.started\n",
      "[DEBUG] 2024-07-03 12:23:23,638 _trace.py:85: response_closed.complete\n",
      "[DEBUG] 2024-07-03 12:23:23,639 _base_client.py:1558: HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response=ChatCompletion(id='chatcmpl-9gzrXaCATPGExrbeJXKhvSamlXj3O', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_W3XsPDgvT8QMIlCe6QuhT3Cj', function=Function(arguments='{\"location\":\"Glasgow, Scotland\",\"format\":\"celsius\"}', name='get_current_weather'), type='function')]))], created=1720034603, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=24, prompt_tokens=196, total_tokens=220))\n",
      "chat_response=ChatCompletion(id='chatcmpl-9gzrXaCATPGExrbeJXKhvSamlXj3O', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_W3XsPDgvT8QMIlCe6QuhT3Cj', function=Function(arguments='{\"location\":\"Glasgow, Scotland\",\"format\":\"celsius\"}', name='get_current_weather'), type='function')]))], created=1720034603, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=24, prompt_tokens=196, total_tokens=220))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ERROR] 2024-07-03 12:23:23,649 __init__.py:156: Failed to detach context\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/eval/lib/python3.12/site-packages/opentelemetry/trace/__init__.py\", line 570, in use_span\n",
      "    yield span\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/eval/lib/python3.12/site-packages/opentelemetry/sdk/trace/__init__.py\", line 1091, in start_as_current_span\n",
      "    yield span\n",
      "  File \"/Users/rossdancraig/Projects/lmai/repos/eval/src/lastmile_eval/rag/debugger/tracing/lastmile_tracer.py\", line 282, in start_as_current_span\n",
      "    yield span\n",
      "  File \"/Users/rossdancraig/Projects/lmai/repos/tracing_auto_instrumentation/src/tracing_auto_instrumentation/openai/async_openai_wrapper.py\", line 313, in _create_impl\n",
      "    yield raw_response\n",
      "GeneratorExit\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/eval/lib/python3.12/site-packages/opentelemetry/context/__init__.py\", line 154, in detach\n",
      "    _RUNTIME_CONTEXT.detach(token)\n",
      "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/eval/lib/python3.12/site-packages/opentelemetry/context/contextvars_context.py\", line 50, in detach\n",
      "    self._current_context.reset(token)  # type: ignore\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: <Token var=<ContextVar name='current_context' default={} at 0x108267240> at 0x138496c80> was created in a different Context\n",
      "[DEBUG] 2024-07-03 12:23:23,771 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/trace/create HTTP/11\" 200 10\n",
      "[DEBUG] 2024-07-03 12:23:23,775 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
      "[DEBUG] 2024-07-03 12:23:24,295 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/rag_query_traces/create HTTP/11\" 200 None\n",
      "[DEBUG] 2024-07-03 12:23:24,300 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
      "[DEBUG] 2024-07-03 12:23:24,681 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/rag_events/create HTTP/11\" 200 None\n"
     ]
    }
   ],
   "source": [
    "# Without Streaming\n",
    "await execute_async_tool_call_and_display_response(stream=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1376a7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG] 2024-07-03 12:24:03,778 _base_client.py:446: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"}, {'role': 'user', 'content': \"What's the weather like today in Glasgow, Scotland?\"}], 'model': 'gpt-3.5-turbo', 'stream': True, 'tool_choice': None, 'tools': [{'type': 'function', 'function': {'name': 'get_current_weather', 'description': 'Get the current weather', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'format': {'type': 'string', 'enum': ['celsius', 'fahrenheit'], 'description': 'The temperature unit to use. Infer this from the users location.'}}, 'required': ['location', 'format']}}}, {'type': 'function', 'function': {'name': 'get_n_day_weather_forecast', 'description': 'Get an N-day weather forecast', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'format': {'type': 'string', 'enum': ['celsius', 'fahrenheit'], 'description': 'The temperature unit to use. Infer this from the users location.'}, 'num_days': {'type': 'integer', 'description': 'The number of days to forecast'}}, 'required': ['location', 'format', 'num_days']}}}]}}\n",
      "[DEBUG] 2024-07-03 12:24:03,780 _trace.py:85: close.started\n",
      "[DEBUG] 2024-07-03 12:24:03,782 _trace.py:85: close.complete\n",
      "[DEBUG] 2024-07-03 12:24:03,783 _trace.py:85: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "[DEBUG] 2024-07-03 12:24:03,832 _trace.py:85: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x13807bd40>\n",
      "[DEBUG] 2024-07-03 12:24:03,833 _trace.py:85: start_tls.started ssl_context=<ssl.SSLContext object at 0x138560cd0> server_hostname='api.openai.com' timeout=5.0\n",
      "[DEBUG] 2024-07-03 12:24:03,863 _trace.py:85: start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x13807bc20>\n",
      "[DEBUG] 2024-07-03 12:24:03,865 _trace.py:85: send_request_headers.started request=<Request [b'POST']>\n",
      "[DEBUG] 2024-07-03 12:24:03,866 _trace.py:85: send_request_headers.complete\n",
      "[DEBUG] 2024-07-03 12:24:03,866 _trace.py:85: send_request_body.started request=<Request [b'POST']>\n",
      "[DEBUG] 2024-07-03 12:24:03,868 _trace.py:85: send_request_body.complete\n",
      "[DEBUG] 2024-07-03 12:24:03,868 _trace.py:85: receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about to print response\n",
      "response=<async_generator object AsyncCompletionsWrapper._create_impl at 0x1385746c0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG] 2024-07-03 12:24:04,364 _trace.py:85: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 03 Jul 2024 19:24:04 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'lastmile-ai'), (b'openai-processing-ms', b'281'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'4999939'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_cfc598a4da51b75afdb15e5f11621419'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89d940ec5cfc97fd-SJC'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "[INFO] 2024-07-03 12:24:04,368 _client.py:1773: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[DEBUG] 2024-07-03 12:24:04,369 _base_client.py:1558: HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n",
      "[DEBUG] 2024-07-03 12:24:04,372 _trace.py:85: receive_response_body.started request=<Request [b'POST']>\n",
      "[DEBUG] 2024-07-03 12:24:04,444 _trace.py:85: receive_response_body.complete\n",
      "[DEBUG] 2024-07-03 12:24:04,445 _trace.py:85: response_closed.started\n",
      "[DEBUG] 2024-07-03 12:24:04,446 _trace.py:85: response_closed.complete\n",
      "[DEBUG] 2024-07-03 12:24:04,554 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/trace/create HTTP/11\" 200 10\n",
      "[DEBUG] 2024-07-03 12:24:04,558 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk=ChatCompletionChunk(id='chatcmpl-9gzsBoO26gbtNZJJMquAvhrfKJjyB', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role='assistant', tool_calls=[ChoiceDeltaToolCall(index=0, id='call_YBT3Gkln0iO10ILkYEROawhu', function=ChoiceDeltaToolCallFunction(arguments='', name='get_current_weather'), type='function')]), finish_reason=None, index=0, logprobs=None)], created=1720034643, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "chunk=ChatCompletionChunk(id='chatcmpl-9gzsBoO26gbtNZJJMquAvhrfKJjyB', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='{\"', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1720034643, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "chunk=ChatCompletionChunk(id='chatcmpl-9gzsBoO26gbtNZJJMquAvhrfKJjyB', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='location', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1720034643, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "chunk=ChatCompletionChunk(id='chatcmpl-9gzsBoO26gbtNZJJMquAvhrfKJjyB', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\":\"', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1720034643, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "chunk=ChatCompletionChunk(id='chatcmpl-9gzsBoO26gbtNZJJMquAvhrfKJjyB', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='G', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1720034643, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "chunk=ChatCompletionChunk(id='chatcmpl-9gzsBoO26gbtNZJJMquAvhrfKJjyB', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='las', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1720034643, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "chunk=ChatCompletionChunk(id='chatcmpl-9gzsBoO26gbtNZJJMquAvhrfKJjyB', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='gow', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1720034643, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "chunk=ChatCompletionChunk(id='chatcmpl-9gzsBoO26gbtNZJJMquAvhrfKJjyB', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=',', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1720034643, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "chunk=ChatCompletionChunk(id='chatcmpl-9gzsBoO26gbtNZJJMquAvhrfKJjyB', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments=' Scotland', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1720034643, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "chunk=ChatCompletionChunk(id='chatcmpl-9gzsBoO26gbtNZJJMquAvhrfKJjyB', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\",\"', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1720034643, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "chunk=ChatCompletionChunk(id='chatcmpl-9gzsBoO26gbtNZJJMquAvhrfKJjyB', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='format', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1720034643, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "chunk=ChatCompletionChunk(id='chatcmpl-9gzsBoO26gbtNZJJMquAvhrfKJjyB', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\":\"', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1720034643, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "chunk=ChatCompletionChunk(id='chatcmpl-9gzsBoO26gbtNZJJMquAvhrfKJjyB', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='c', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1720034643, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "chunk=ChatCompletionChunk(id='chatcmpl-9gzsBoO26gbtNZJJMquAvhrfKJjyB', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='elsius', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1720034643, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "chunk=ChatCompletionChunk(id='chatcmpl-9gzsBoO26gbtNZJJMquAvhrfKJjyB', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=[ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='\"}', name=None), type=None)]), finish_reason=None, index=0, logprobs=None)], created=1720034643, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n",
      "chunk=ChatCompletionChunk(id='chatcmpl-9gzsBoO26gbtNZJJMquAvhrfKJjyB', choices=[Choice(delta=ChoiceDelta(content=None, function_call=None, role=None, tool_calls=None), finish_reason='tool_calls', index=0, logprobs=None)], created=1720034643, model='gpt-3.5-turbo-0125', object='chat.completion.chunk', system_fingerprint=None, usage=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[DEBUG] 2024-07-03 12:24:04,950 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/rag_query_traces/create HTTP/11\" 200 None\n",
      "[DEBUG] 2024-07-03 12:24:04,955 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
      "[DEBUG] 2024-07-03 12:24:05,333 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/rag_events/create HTTP/11\" 200 None\n"
     ]
    }
   ],
   "source": [
    "# With Streaming\n",
    "await execute_async_tool_call_and_display_response(stream=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
