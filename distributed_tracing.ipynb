{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Distributed Tracing with LastMile Tracing SDK\n",
                "\n",
                "This Python notebook demonstrates how to implement distributed tracing using the LastMile Tracing SDK. Distributed tracing allows you to track and analyze the flow of requests across multiple services or components in a distributed system.\n",
                "\n",
                "In this notebook, we showcase a simple example where a client sends a request to a server to generate a riddle. The server uses OpenAI's GPT-3.5-turbo model to generate the riddle and returns it to the client. The LastMile Tracing SDK is used to instrument the code and capture the trace information.\n",
                "\n",
                "The notebook covers the following key aspects:\n",
                "- Setting up and configuring the LastMile Tracing SDK\n",
                "- Instrumenting the server code to create spans and capture trace information\n",
                "- Instrumenting the client code to propagate the trace context to the server\n",
                "- Visualizing Trace Data through LastMile's Trace Viewer\n",
                "\n",
                "\n",
                "Prerequisites:\n",
                "- Python 3.x\n",
                "- Required libraries: \n",
                "- - LastMile Tracing SDK `lastmile-eval`\n",
                "- - `multiprocess`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install lastmile-eval --upgrade\n",
                "# fork of multiprocessing, uses dill instead of pickle.\n",
                "!pip install multiprocess\n",
                "!pip install flask"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "LastMile API token is set.\n"
                    ]
                }
            ],
            "source": [
                "# Verify that the LastMile API token is set\n",
                "from lastmile_eval.rag.debugger.common.utils import get_lastmile_api_token\n",
                "try:\n",
                "    get_lastmile_api_token(None)\n",
                "finally:\n",
                "    print(\"LastMile API token is set.\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/Users/ankush/anaconda3/envs/main/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n",
                        "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2024-05-10 12:13:34,114 - Starting new HTTPS connection (1): lastmileai.dev:443\n",
                        "2024-05-10 12:13:34,202 - https://lastmileai.dev:443 \"GET /api/evaluation_projects/list?name=generate_riddle HTTP/1.1\" 200 330\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        " * Serving Flask app '__main__'\n",
                        " * Debug mode: off\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2024-05-10 12:13:34,210 - \u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
                        " * Running on http://127.0.0.1:1234\n",
                        "2024-05-10 12:13:34,211 - \u001b[33mPress CTRL+C to quit\u001b[0m\n"
                    ]
                }
            ],
            "source": [
                "import json\n",
                "import os\n",
                "\n",
                "import multiprocess\n",
                "from flask import Flask, request\n",
                "from openai import OpenAI\n",
                "\n",
                "from lastmile_eval.rag.debugger.api import LastMileTracer\n",
                "from lastmile_eval.rag.debugger.tracing import get_lastmile_tracer\n",
                "\n",
                "\n",
                "def server():\n",
                "    \"\"\"\n",
                "    Starts a Flask server that exposes an endpoint for generating riddles.\n",
                "    The server uses the LastMileTracer for distributed tracing.\n",
                "    \"\"\"\n",
                "    \n",
                "    tracer: LastMileTracer = get_lastmile_tracer(\n",
                "        tracer_name=\"generate_riddle\", # project name\n",
                "    )\n",
                "    app = Flask(__name__)\n",
                "\n",
                "    @app.route(\"/generate\")\n",
                "    def generate()-> str:\n",
                "        \"\"\"\n",
                "        Endpoint that generates a riddle using OpenAI's GPT-3.5-turbo model.\n",
                "        If the OPENAI_API_KEY environment variable is not set, a default riddle is returned.\n",
                "        The generated riddle is returned as a JSON string.\n",
                "        \"\"\"\n",
                "        span_context = request.headers.get(\"span\") # Expect a span to be passed to this endpoint\n",
                "        with tracer.start_as_current_span(\"generate_endpoint\", context = span_context):\n",
                "            riddle = \"\"\n",
                "            if os.environ.get(\"OPENAI_API_KEY\") is not None:\n",
                "                response = OpenAI().chat.completions.create(messages = [{\"role\": \"user\", \"content\":\"tell me a riddle\"}], model = \"gpt-3.5-turbo\")\n",
                "                riddle = response.choices[0].message.content or \"debugging me can be quite a trick.\"\n",
                "            else:\n",
                "                riddle = \"I'm a key that unlocks no doors, But fill me in, and power soars. In the realm where AI plays\"\n",
                "            return json.dumps(riddle)\n",
                "    app.run(port=1234, debug=False)\n",
                "\n",
                "# Start Server in a Subprocess to avoid blocking execution of succeeding cells\n",
                "process = multiprocess.Process(target=server)\n",
                "process.start()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2024-05-10 12:13:22,449 - Starting new HTTPS connection (1): lastmileai.dev:443\n",
                        "2024-05-10 12:13:22,536 - https://lastmileai.dev:443 \"GET /api/evaluation_projects/list?name=generate_riddle HTTP/1.1\" 200 330\n",
                        "2024-05-10 12:13:22,550 - Starting new HTTP connection (1): 127.0.0.1:1234\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "sending request to subprocess server, with span context.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2024-05-10 12:13:24,020 - http://127.0.0.1:1234 \"GET /generate HTTP/1.1\" 200 166\n",
                        "2024-05-10 12:13:24,022 - Starting new HTTPS connection (1): lastmileai.dev:443\n",
                        "2024-05-10 12:13:24,101 - https://lastmileai.dev:443 \"POST /api/trace/create HTTP/1.1\" 200 10\n",
                        "2024-05-10 12:13:24,104 - Starting new HTTPS connection (1): lastmileai.dev:443\n",
                        "2024-05-10 12:13:24,187 - https://lastmileai.dev:443 \"POST /api/rag_query_traces/create HTTP/1.1\" 200 495\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "'\"I am taken from a mine, and shut up in a wooden case, from which I am never released, and yet I am used by almost every person. What am I? \\\\n\\\\n(answer: pencil lead)\"'"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import requests\n",
                "from opentelemetry import trace\n",
                "\n",
                "from lastmile_eval.rag.debugger.api import LastMileTracer\n",
                "from lastmile_eval.rag.debugger.tracing import export_span, get_lastmile_tracer\n",
                "\n",
                "tracer2: LastMileTracer = get_lastmile_tracer(\n",
                "    tracer_name=\"generate_riddle\", # project name\n",
                ")\n",
                "@tracer2.start_as_current_span(\"client\")\n",
                "def client_say_riddle():\n",
                "    '''\n",
                "    In order distribute the trace context, we need to export the span context to a string\n",
                "    and pass it as a header to the server.\n",
                "    All you need to do is call `export_span(trace.get_current_span())`\n",
                "    '''\n",
                "    try:\n",
                "        print(\"sending request to subprocess server, with span context.\")\n",
                "        # In order distribute the trace context, we need to export the span context to a string\n",
                "        # and pass it as a header to the server.\n",
                "        # All you need to do is call `export_span(trace.get_current_span())``\n",
                "        response = requests.get('http://127.0.0.1:1234/generate', headers={\"span\": export_span(trace.get_current_span())})\n",
                "        return response.text\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(e)\n",
                "client_say_riddle()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check out the UI to see the trace\n",
                "!rag-debug launch"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "main",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
