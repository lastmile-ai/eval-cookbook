{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Leveraging AI for Accurate PRs: A Guide to Using OpenAI for Title and Description Generation, and Evaluating with LastMile Eval\n",
                "\n",
                "In this notebook, we will explore how to leverage OpenAI's language model to generate accurate and descriptive titles and descriptions for pull requests (PRs). We will then use the LastMile Eval library to evaluate the quality of the generated content.\n",
                "\n",
                "## Prerequisites\n",
                "\n",
                "Before we begin, make sure you have the following libraries installed:\n",
                "\n",
                "- `requests`: Used for making HTTP requests to the GitHub API.\n",
                "- `openai`: The OpenAI library for interacting with the OpenAI API.\n",
                "- `lastmile-eval`: The LastMile Eval library for evaluating the generated content.\n",
                "\n",
                "You can install these libraries using the following commands:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install requests\n",
                "!pip install openai\n",
                "!pip install lastmile-eval"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Fetching Pull Request Diffs\n",
                "\n",
                "We start by defining a function `get_pull_request_diff` that takes a pull request link and fetches the diff of the pull request using the GitHub API."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "\n",
                "merged_prs = [\n",
                "    \"https://github.com/keras-team/keras/pull/19720\",\n",
                "    \"https://github.com/keras-team/keras/pull/19728\",\n",
                "    \"https://github.com/keras-team/keras/pull/19729\"\n",
                "]\n",
                "\n",
                "def get_pull_request_diff(pr_link: str):\n",
                "    diff_suffix = \".diff\"\n",
                "    diff_url = f'{pr_link}{diff_suffix}'\n",
                "\n",
                "    # Send a GET request to the GitHub API\n",
                "    response = requests.get(diff_url)\n",
                "    return response.text"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's take a look at the diff of the first PR:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "diff --git a/keras/src/export/export_lib.py b/keras/src/export/export_lib.py\n",
                        "index 1157630da0e..e3749c2b33c 100644\n",
                        "--- a/keras/src/export/export_lib.py\n",
                        "+++ b/keras/src/export/export_lib.py\n",
                        "@@ -621,18 +621,17 @@ def export_model(model, filepath):\n",
                        "...\n"
                    ]
                }
            ],
            "source": [
                "pr_diff = get_pull_request_diff(merged_prs[0])\n",
                "print(pr_diff)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Generating Pull Request Description\n",
                "\n",
                "Next, we use OpenAI's language model to generate a description for the pull request based on the diff. Make sure to set your OpenAI API key in the environment variable `OPENAI_API_KEY`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "- Refactored `_get_save_spec` to `_get_input_signature` in `export_lib.py`\n",
                        "- Updated the function to return a list comprehension for input signatures based on shapes dictionary values\n",
                        "- Added a new test `test_model_with_multiple_inputs` in `export_lib_test.py` to test the model with multiple inputs and batch sizes\n"
                    ]
                }
            ],
            "source": [
                "import openai\n",
                "\n",
                "client = openai.OpenAI()\n",
                "system_prompt_template = f\"You are a developer who writes amazing code. You are working on a project and you need to generate a pull request description for a pull request diff. When given a diff, generate a description for the pull request. Say only the description with formatting\"\n",
                "response = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"system\", \"content\": system_prompt_template}, {\"role\": \"user\", \"content\": pr_diff}])\n",
                "\n",
                "generated_pr_description = response.choices[0].message.content \n",
                "print(generated_pr_description)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Generating Pull Request Title\n",
                "\n",
                "We can also generate a concise and descriptive title for the pull request based on the generated description."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Refactor input signature naming convention and add test for multiple inputs and batch sizes\n"
                    ]
                }
            ],
            "source": [
                "system_prompt_template = f\"You are a developer who writes amazing code. You are working on a project and you need to generate a pull request title from a pull request description. When given a diff, generate a title for the pull request. Say only the title\"\n",
                "response = client.chat.completions.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"system\", \"content\": system_prompt_template}, {\"role\": \"user\", \"content\": generated_pr_description}])\n",
                "generated_pr_title = response.choices[0].message.content \n",
                "print(generated_pr_title)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Evaluating Generated Content\n",
                "\n",
                "Finally, we use the LastMile Eval library to evaluate the quality of the generated description and title. The `calculate_summarization_score` function asks GPT-3.5 to generate a list of float scores indicating the summary quality of each input-reference pair, where 1.0 denotes 'good' and 0.0 denotes otherwise."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n",
                        "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:01<00:00 |  1.55s/it\n",
                        "üêå!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n",
                        "llm_classify |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 (100.0%) | ‚è≥ 00:00<00:00 |  1.25it/s"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Description score: [1.0]\n",
                        "Title score: [1.0]\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "from lastmile_eval.text import calculate_summarization_score\n",
                "\n",
                "description_score = calculate_summarization_score([generated_pr_description], [pr_diff], model_name=\"gpt-3.5-turbo\")\n",
                "title_score = calculate_summarization_score([generated_pr_title], [generated_pr_description], model_name=\"gpt-3.5-turbo\")\n",
                "\n",
                "print(f\"Description score: {description_score}\")\n",
                "print(f\"Title score: {title_score}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The scores indicate that both the generated description and title are of good quality and accurately summarize the pull request diff.\n",
                "\n",
                "That's it! You now have a notebook that demonstrates how to generate accurate and descriptive pull request titles and descriptions using OpenAI, and evaluate their quality using LastMile Eval."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "empty2",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
