{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZLoBuonVDCS"
      },
      "source": [
        "## Step 1: Import modules and packages, download reference docs\n",
        "\n",
        "This goes through entire LastMile AI Eval flow from\n",
        "\n",
        "1. create ingestion trace\n",
        "2. generate a query + ground truth context pair per each node context in a document\n",
        "  - taking those queries and running rag query traces to get actual retrieved context\n",
        "3. listing query traces I want to include in a test set (defaults to last N queries for now)\n",
        "4. create test Set with given query_traces, as well as storing the ground truth for the associated context for each query\n",
        "5. create evaluation metrics based on ones provided by Llama Index\n",
        "   - note: this is mainly from Llama Index, so the evaluation metrics are only focused on retrieval, nothing on outputs (though I store those as output events too)\n",
        "6. create evaluation set by feeding these metrics with test set we just created\n",
        "\n",
        "Some notes:\n",
        "- no manual id grepping needed --> all taken care of by helper functions\n",
        "probably needs to be better designed in future, just was focused on getting unblocked\n",
        "- need to refactor ingestion_trace_id to map to trace-level, not marking rag query event level (right now it doesn't work, I'll add that later)\n",
        "- some other small API convenience functions need to be added to the API, such as a helper function for `list_evaluation_sets()`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeuZV8CZVDCT",
        "outputId": "f8c5de16-89a1-4ddd-f21d-32c3a1240207"
      },
      "outputs": [],
      "source": [
        "!pip install chromadb \n",
        "!pip install llama-index\n",
        "!pip install lastmile-eval --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "- Install required Packages\n",
        "- Download Sample data\n",
        "- Launch the Rag Debugger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "apw5FNNMVDCV",
        "outputId": "c859dbba-db3f-4e31-9110-7a46327f5a0f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import dotenv\n",
        "# Set API Keys\n",
        "# You can get your OPENAI_API_KEY from https://platform.openai.com/api-keys\n",
        "os.environ[\"LASTMILE_API_TOKEN\"] = \"eyJhbGciOiJkaXIiLCJlbmMiOiJBMjU2R0NNIn0..-qVMrf96rDw7RsRO.Baq59Zltreu0gmNEBOrAB85U0vq12aoqpqPbV8BDPaeOGx0QA_Mpf1VNRcAnlUz42XJSK-GEO0_Sp0ffnOd0SjPJWH2gB8YYUUVeOLsZUOM3v81D8hf7Se1otbQnUYTcmiVpXbd1UV0aDX3Cw4gj3-LsFbcVVeuc0trohbk16dyXGvCucsr8SnTUse0wNkh-hmFfvh_N8XgjKoT8yW5JWeCug4jEntygFAdxstOD.USrcXzLcwuE-2UZ8a2hukQ\"\n",
        "dotenv.load_dotenv()\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "LASTMILE_API_TOKEN = os.getenv(\"LASTMILE_API_TOKEN\")\n",
        "assert len(OPENAI_API_KEY) > 0\n",
        "assert len(LASTMILE_API_TOKEN) > 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a seperate terminal, run the following command to launch the Rag Debugger:\n",
        "\n",
        "\n",
        "!rag-debug launch\n",
        "\n",
        "Open up your webbrowser and navigate to the url provided by the Rag Debugger. This will look like `http://localhost:8080/`\n",
        "This notebook is an interactive tutorial that will show you how to use rag-debugger with the tracing API.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY5tB8_mVDCV"
      },
      "source": [
        "## Step 2: Run and Trace Ingestion Pipeline\n",
        "\n",
        "Lets make a basic ingestion pipeline using chromadb. We will use a sample essay by paul graham.\n",
        "\n",
        "- Run the following cell to download the sample data\n",
        "- Run the following cell to create an ingestion pipeline, which is traced with the @traced decorator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download sample data\n",
        "!mkdir -p 'data/paul_graham/'\n",
        "!curl 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -o 'data/paul_graham/paul_graham_essay.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpaKBoOAVDCV"
      },
      "outputs": [],
      "source": [
        "from lastmile_eval.rag.debugger.tracing import (\n",
        "    get_lastmile_tracer)\n",
        "from lastmile_eval.rag.debugger.tracing.decorators import (\n",
        "    traced,\n",
        ")\n",
        "\n",
        "import chromadb\n",
        "import os\n",
        "from lastmile_eval.rag.debugger.api import LastMileTracer\n",
        "chroma_client = chromadb.Client()\n",
        "\n",
        "# Instantiate a tracer object\n",
        "tracer: LastMileTracer = get_lastmile_tracer(\"Paul-Graham\")\n",
        "\n",
        "\n",
        "@traced(tracer) #Decorate the function with the tracer\n",
        "def chunk_document(file_path: str, chunk_size: int = 1000) -> list[str]:\n",
        "    \"\"\"\n",
        "    Chunk a text file into a list of strings based on the specified chunk size.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the text file.\n",
        "        chunk_size (int): The desired number of characters in each chunk.\n",
        "\n",
        "    Returns:\n",
        "        list[str]: A list of strings, where each string represents a chunk of text.\n",
        "    \"\"\"\n",
        "    with open(file_path, \"r\") as file:\n",
        "        text = file.read()\n",
        "\n",
        "    chunks: list[str] = []\n",
        "    for i in range(0, len(text), chunk_size):\n",
        "        chunks.append(text[i:i + chunk_size])\n",
        "\n",
        "    return chunks\n",
        "\n",
        "@traced(tracer)\n",
        "def run_ingestion_flow() -> chromadb.Collection:\n",
        "    collection = chroma_client.create_collection(name=\"my_collection\")\n",
        "    tracer.mark_rag_ingestion_trace_event(\"Ingesting Paul Graham's essay\")\n",
        "    \n",
        "    document_chunks = chunk_document(\"data/paul_graham/paul_graham_essay.txt\")\n",
        "    document_ids = [f\"chunk_{i}\" for i in range(len(document_chunks))]\n",
        "\n",
        "    collection.add(\n",
        "        ids=document_ids,\n",
        "        documents=document_chunks, # ex: [\"What I Worked On\", \"February 2021\", ...]\n",
        "    )\n",
        "    return collection\n",
        "\n",
        "collection:chromadb.Collection = run_ingestion_flow()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IIzk6G8VDCV",
        "outputId": "c0671c6a-cddb-4bf3-995e-d0a3d8403376"
      },
      "outputs": [],
      "source": [
        "# Let's print the trace data from Jaeger to\n",
        "# show you what it looks like (search for \"operationName\" in the data)\n",
        "\n",
        "from lastmile_eval.rag.debugger.tracing import (\n",
        "    get_latest_ingestion_trace_id,\n",
        "    get_trace_data,\n",
        ")\n",
        "\n",
        "ingestion_trace_id = get_latest_ingestion_trace_id()\n",
        "get_trace_data(ingestion_trace_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "Znk3omg3VDCW",
        "outputId": "9509c207-cd9e-4a0a-dda4-3730fd6eabc0"
      },
      "outputs": [],
      "source": [
        "# Now let's fetch the trace event data from our postgres table\n",
        "# Notice that the `traceId` column matches with the raw trace data\n",
        "\n",
        "from lastmile_eval.rag.debugger.tracing import list_ingestion_trace_events\n",
        "import pandas as pd\n",
        "\n",
        "ingestion_trace_events = list_ingestion_trace_events(take=1)\n",
        "pd.DataFrame.from_records(ingestion_trace_events[\"ingestionTraces\"]).rename(  # type: ignore[fixme]\n",
        "    columns={\"id\": \"ragIngestionTraceEventId\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ4hNS0pVDCW"
      },
      "source": [
        "## Part 3: Run and Trace Query Pipeline\n",
        "\n",
        "Now that we have an ingestion pipeline built, lets build a query pipeline. We will use an OpenAI model to generate responses to user queries. We will trace this pipeline with the @traced decorator. \n",
        "Note: the query pipeline and ingestion pipeline are separate, so we will need to link them together with the ingestion_trace_id."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "H2Ita6wMVDCW"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from lastmile_eval.rag.debugger.api import (\n",
        "    QueryReceived,\n",
        "    ContextRetrieved,\n",
        "    PromptResolved,\n",
        "    LLMOutputReceived,\n",
        ")\n",
        "\n",
        "LLM_NAME = \"gpt-3.5-turbo\"\n",
        "\n",
        "\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "Context information is below.\n",
        "---------------------\n",
        "{context_str}\n",
        "---------------------\n",
        "Given the context information and not prior knowledge, answer the query.\n",
        "Query: {query_str}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "@traced(tracer, name=\"retrieve-context\") #Decorate the function with the tracer\n",
        "def retrieve_context(query_string: str, top_k: int = 5) -> list[str]:\n",
        "    \"\"\"\n",
        "    Retrieve the top-k most relevant contexts based on the query string\n",
        "    from the chroma db collection\n",
        "    \"\"\"\n",
        "    tracer.register_param(\"similarity_top_k\", top_k)\n",
        "    chroma_retrival_results = collection.query(query_texts=query_string, n_results=top_k)\n",
        "    documents_parsed_as_strings = [document for document in chroma_retrival_results.get(\"documents\")[0]]\n",
        "    tracer.mark_rag_query_trace_event(\n",
        "            ContextRetrieved(context=documents_parsed_as_strings), get_latest_ingestion_trace_id()\n",
        "        )\n",
        "    return documents_parsed_as_strings\n",
        "\n",
        "@traced(tracer, name=\"resolve-prompt\")\n",
        "def resolve_prompt(user_query: str, retrieved_contexts: list[str]):\n",
        "    resolved_prompt = PROMPT_TEMPLATE.replace(\n",
        "        \"{context_str}\", \"\\n\\n\\n\".join(retrieved_contexts)\n",
        "    ).replace(\"{query_str}\", user_query)\n",
        "    tracer.mark_rag_query_trace_event(\n",
        "        PromptResolved(fully_resolved_prompt=resolved_prompt), get_latest_ingestion_trace_id()\n",
        "    )\n",
        "    return resolved_prompt\n",
        "\n",
        "\n",
        "@traced(tracer=tracer, name=\"query-root-span\") # You can also provide a custom name for the root span\n",
        "def run_query_flow(user_query: str, ingestion_trace_id: str):\n",
        "    tracer.mark_rag_query_trace_event(\n",
        "        QueryReceived(query=user_query), ingestion_trace_id\n",
        "    )\n",
        "\n",
        "    retrieved_contexts = retrieve_context(user_query, top_k=3)\n",
        "\n",
        "    resolved_prompt = resolve_prompt(user_query, retrieved_contexts)\n",
        "\n",
        "    with tracer.start_as_current_span(\"call-llm\") as _llm_span:\n",
        "        openai_client = openai.Client(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "        response = openai_client.chat.completions.create(\n",
        "            model=LLM_NAME,\n",
        "            messages=[{\"role\": \"user\", \"content\": resolved_prompt}],\n",
        "        )\n",
        "        output: str = response.choices[0].message.content\n",
        "        tracer.mark_rag_query_trace_event(\n",
        "            LLMOutputReceived(llm_output=output), get_latest_ingestion_trace_id()\n",
        "        )\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMQ5nHcYVDCW"
      },
      "outputs": [],
      "source": [
        "# TODO: Right now the ingestion_trace_id within mark_rag_query_trace_event is\n",
        "# no-op due to changes in assumptions, I'll fix later\n",
        "response = run_query_flow(\"What did the author do growing up?\", ingestion_trace_id)\n",
        "\n",
        "print(f\"Response: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Checkpoint: At this point, we have a traced ingestion pipeline and a traced query pipeline. Open up the Rag Debugger and navigate to the traces tab. You should see the traces for the ingestion pipeline and the query pipeline.\n",
        "|  View All Traces    | narrowed Ingestion Trace View |\n",
        "| -------- | ------- |\n",
        "| ![OpenAI Logo](https://github-production-user-asset-6210df.s3.amazonaws.com/141073967/330911464-10debfac-a32a-4349-bbd2-fd3e5da95fcb.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240515%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240515T180450Z&X-Amz-Expires=300&X-Amz-Signature=1f66773e767b26fb65d406b13284c349284e1a17112555a13cc9bea8eb195fa4&X-Amz-SignedHeaders=host&actor_id=141073967&key_id=0&repo_id=768880246)  | ![](https://github-production-user-asset-6210df.s3.amazonaws.com/141073967/330911533-68e1a305-5734-432a-bffe-896fae10bf1a.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240515%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240515T180514Z&X-Amz-Expires=300&X-Amz-Signature=c5c68b0ddc0b18f6326d3290086f4150c91b182e4605f23adbe4974a9d369212&X-Amz-SignedHeaders=host&actor_id=141073967&key_id=0&repo_id=768880246) |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "GTXnkr4XVDCW",
        "outputId": "9883c082-42bd-4e30-fa24-a9a15be560c3"
      },
      "outputs": [],
      "source": [
        "# Just like what we did with the ingestion trace,\n",
        "# let's print out what this looks like in the PostGres data, as well as the\n",
        "# pure trace data again\n",
        "from lastmile_eval.rag.debugger.tracing import (\n",
        "    list_query_trace_events,\n",
        ")\n",
        "\n",
        "query_trace_events = list_query_trace_events(take=1)\n",
        "query_trace_events_df = pd.DataFrame.from_records(query_trace_events[\"queryTraces\"]).rename(  # type: ignore[fixme]\n",
        "    columns={\"id\": \"ragQueryTraceEventsId\"}\n",
        ")\n",
        "query_trace_events_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PepT9DXcm4B1",
        "outputId": "66e1f665-5006-432b-ca5a-1201f01416f2"
      },
      "outputs": [],
      "source": [
        "# This is what the trace data looks like\n",
        "from lastmile_eval.rag.debugger.tracing import (\n",
        "    get_trace_data,\n",
        ")\n",
        "\n",
        "# Fetch the first trace id from the query trace events\n",
        "query_trace_id = query_trace_events_df.iloc[0][\"traceId\"]\n",
        "get_trace_data(query_trace_id)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPMbbFE4VDCX"
      },
      "source": [
        "## Part 4: Create Test Sets and Run Evaluators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCpbwqGqVDCX",
        "outputId": "1a43a2a8-4bda-4960-8850-0f402e7b3772"
      },
      "outputs": [],
      "source": [
        "# Sample Query Test Set\n",
        "from openai import OpenAI\n",
        "from llama_index.core.evaluation import generate_question_context_pairs\n",
        "\n",
        "queries = [ \n",
        "    \"What two main things did Paul Graham work on before college, outside of school?\", \n",
        "    \"What was the key realization Paul Graham had about artificial intelligence during his first year of grad school at Harvard?\", \n",
        "    \"How did Paul Graham and his partner Robert Morris get their initial idea and start working on what became their startup Viaweb?\", \n",
        "    \"What were some of the novel approaches and advantages that Y Combinator introduced compared to traditional venture capital firms when it first started?\", \n",
        "    \"What ambitious programming language project did Paul Graham work on intensively for 4 years from 2015-2019, and what was unique about the goal and approach of this language called Bel?\" \n",
        "             ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a6rPAZTVDCX",
        "outputId": "c48cfeac-98ef-46a5-e1aa-3f3c07293101"
      },
      "outputs": [],
      "source": [
        "# Run these queries through the `run_query_flow()` method\n",
        "\n",
        "expected_node_ids: list[str] = []\n",
        "for i, query in enumerate(queries):\n",
        "    run_query_flow(query, ingestion_trace_id)\n",
        "\n",
        "    print(f\"Finished running {i+1}/5 queries...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "6zjuOAs2VDCX",
        "outputId": "804da333-a754-4972-c3d4-789c5f1e94f3"
      },
      "outputs": [],
      "source": [
        "from lastmile_eval.rag.debugger.tracing import list_query_trace_events\n",
        "\n",
        "# Fetch the trace data. Visualize with a pandas dataframe.\n",
        "query_trace_events = list_query_trace_events(take=5)\n",
        "query_trace_events_df = pd.DataFrame.from_records(query_trace_events[\"queryTraces\"]).rename(  # type: ignore[fixme]\n",
        "    columns={\"id\": \"ragQueryTraceId\"}\n",
        ")\n",
        "query_trace_events_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5  - Run Online evaluations with the RAG query function\n",
        "\n",
        "This is a convenience function that runs your query logic for you and then your specified evaluators. You can use it to more conveniently evaluate your RAG query input/output pairs. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from lastmile_eval.rag.debugger.api.evaluation import (\n",
        "    run_and_evaluate_outputs,\n",
        "    get_default_rag_trace_level_metrics\n",
        ")\n",
        "\n",
        "# Evaluate the relevance of the returned answer to the ground-truth answer.\n",
        "trace_level_evaluators = get_default_rag_trace_level_metrics(    \n",
        "    names={\"relevance\"},\n",
        "    lastmile_api_token=LASTMILE_API_TOKEN\n",
        ")\n",
        "\n",
        "inputs = queries # From the previous cell\n",
        "\n",
        "ground_truth_answers = [\n",
        "    \"The author first interacted with programming on a mainframe computer, using punch cards to input Fortran code, which was a challenging and time-consuming process\",\n",
        "    \"The transition from the IBM 1401 to microcomputers like the TRS-80 represented a significant step forward in terms of both programming capabilities and user interaction.\",\n",
        "    \"A turning point came after reading Nick Bostrom's \\\"Superintelligence,\\\" which presented a persuasive argument on the potential of Artificial Intelligence (AI)\",\n",
        "    \"Heinlein's \\\"The Moon is a Harsh Mistress\\\" and Terry Winograd's SHRDLU heavily influenced the author's decision to pursue AI\",\n",
        "    \"The author considered the AI practices during his first year of grad school as a \\\"hoax\\\" because they didn't meet his expectations for understanding and interpreting natural language accurately.\",\n",
        "]\n",
        "\n",
        "evaluate_result = run_and_evaluate_outputs(\n",
        "    \"my_project_id\",\n",
        "    trace_level_evaluators=trace_level_evaluators,\n",
        "    dataset_level_evaluators={},\n",
        "    rag_query_fn=partial(\n",
        "        run_query_flow,\n",
        "        ingestion_trace_id=ingestion_trace_id\n",
        "    ),\n",
        "    inputs=inputs,\n",
        "    ground_truth=ground_truth_answers\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
