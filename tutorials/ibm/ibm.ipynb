{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# _____ ______  _______      _  _  _ _______ _______ _______  _____  __   _      _     _\n",
    "#   |   |_____] |  |  |      |  |  | |_____|    |    |______ |     | | \\  |       \\___/ \n",
    "# __|__ |_____] |  |  |      |__|__| |     |    |    ______| |_____| |  \\_|      _/   \\_                                                                                       \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"tracing_auto_instrumentation[ibm]\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "from ibm_watsonx_ai.foundation_models import Model\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "\n",
    "from lastmile_eval.rag.debugger.api import LastMileTracer\n",
    "from lastmile_eval.rag.debugger.tracing.sdk import get_lastmile_tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mode(Enum):\n",
    "    GENERATE = \"GENERATE\"\n",
    "    GENERATE_TEXT = \"GENERATE_TEXT\"\n",
    "\n",
    "\n",
    "def init_watson_model() -> Model:\n",
    "    # To display example params enter\n",
    "    GenParams().get_example_values()\n",
    "    generate_params = {GenParams.MAX_NEW_TOKENS: 25}\n",
    "\n",
    "    watson_model = Model(\n",
    "        model_id=ModelTypes.GRANITE_13B_CHAT_V2,\n",
    "        params=generate_params,\n",
    "        credentials=dict(\n",
    "            api_key=os.getenv(\"WATSONX_API_KEY\"),\n",
    "            url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "        ),\n",
    "        space_id=os.getenv(\"WATSONX_SPACE_ID\"),\n",
    "        verify=None,\n",
    "        validate=True,\n",
    "    )\n",
    "\n",
    "    return watson_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] 2024-06-19 16:24:22,154 __init__.py:518: Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2024-06-19 16:24:23,143 client.py:390: Client successfully initialized\n",
      "[INFO] 2024-06-19 16:24:24,100 wml_resource.py:91: Successfully finished Get available foundation models for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2024-06-11&space_id=c6dd344f-1aab-4b5e-8edb-2a21fa1e0de0&filters=function_text_generation&limit=200'\n",
      "[DEBUG] 2024-06-19 16:24:24,100 wml_resource.py:96: Response(GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2024-06-11&space_id=c6dd344f-1aab-4b5e-8edb-2a21fa1e0de0&filters=function_text_generation&limit=200): {\"total_count\":20,\"limit\":200,\"first\":{\"href\":\"https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2024-06-11&space_id=c6dd344f-1aab-4b5e-8edb-2a21fa1e0de0&filters=function_text_generation&limit=200\"},\"resources\":[{\"model_id\":\"bigscience/mt0-xxl\",\"label\":\"mt0-xxl-13b\",\"provider\":\"BigScience\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"An instruction-tuned iteration on mT5.\",\"long_description\":\"mt0-xxl (13B) is an instruction-tuned iteration on mT5. Like BLOOMZ, it was fine-tuned on a cross-lingual task mixture dataset (xP3) using multitask prompted finetuning (MTF).\",\"input_tier\":\"class_2\",\"output_tier\":\"class_2\",\"number_params\":\"13b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":3}},{\"id\":\"summarization\",\"ratings\":{\"quality\":3}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":2}},{\"id\":\"classification\",\"ratings\":{\"quality\":3}},{\"id\":\"generation\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":2}}],\"model_limits\":{\"max_sequence_length\":4096},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":700},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-07-07\"}]},{\"model_id\":\"codellama/codellama-34b-instruct-hf\",\"label\":\"codellama-34b-instruct-hf\",\"provider\":\"Code Llama\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Code Llama is an AI model built on top of Llama 2, fine-tuned for generating and discussing code.\",\"long_description\":\"Code Llama is a pretrained and fine-tuned generative text models with 34 billion parameters. This model is designed for general code synthesis and understanding.\",\"input_tier\":\"class_2\",\"output_tier\":\"class_2\",\"number_params\":\"34b\",\"min_shot_size\":0,\"task_ids\":[\"code\"],\"tasks\":[{\"id\":\"code\"}],\"model_limits\":{\"max_sequence_length\":16384},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":8192},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8192},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8192}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-03-14\"}]},{\"model_id\":\"google/flan-t5-xl\",\"label\":\"flan-t5-xl-3b\",\"provider\":\"Google\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"prompt_tune_inferable\"},{\"id\":\"prompt_tune_trainable\"},{\"id\":\"text_generation\"}],\"short_description\":\"A pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format.\",\"long_description\":\"flan-t5-xl (3B) is a 3 billion parameter model based on the Flan-T5 family. It is a pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format, and fine-tuned on the Fine-tuned Language Net (FLAN) with instructions for better zero-shot and few-shot performance.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"3b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\",\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"retrieval_augmented_generation\"},{\"id\":\"classification\",\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"generation\",\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":4096,\"training_data_max_records\":10000},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4095},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-12-07\"}],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"random\"},\"init_text\":{\"default\":\"text\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":20,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"Input: {{input}} Output:\"},\"batch_size\":{\"default\":16,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":256},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":128},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":16,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.3,\"min\":0.00001,\"max\":0.5}}},{\"model_id\":\"google/flan-t5-xxl\",\"label\":\"flan-t5-xxl-11b\",\"provider\":\"Google\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"flan-t5-xxl is an 11 billion parameter model based on the Flan-T5 family.\",\"long_description\":\"flan-t5-xxl (11B) is an 11 billion parameter model based on the Flan-T5 family. It is a pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format, and fine-tuned on the Fine-tuned Language Net (FLAN) with instructions for better zero-shot and few-shot performance.\",\"input_tier\":\"class_2\",\"output_tier\":\"class_2\",\"number_params\":\"11b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":3}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":4096},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":700},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-07-07\"}]},{\"model_id\":\"google/flan-ul2\",\"label\":\"flan-ul2-20b\",\"provider\":\"Google\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"flan-ul2 is an encoder decoder model based on the T5 architecture and instruction-tuned using the Fine-tuned Language Net.\",\"long_description\":\"flan-ul2 (20B) is an encoder decoder model based on the T5 architecture and instruction-tuned using the Fine-tuned Language Net (FLAN). Compared to the original UL2 model, flan-ul2 (20B) is more usable for few-shot in-context learning because it was trained with a three times larger receptive field. flan-ul2 (20B) outperforms flan-t5 (11B) by an overall relative improvement of +3.2%.\",\"input_tier\":\"class_3\",\"output_tier\":\"class_3\",\"number_params\":\"20b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":4}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":4096},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":700},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-07-07\"}]},{\"model_id\":\"ibm-mistralai/merlinite-7b\",\"label\":\"merlinite-7b\",\"provider\":\"Mistral AI\",\"tuned_by\":\"IBM\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Merlinite-7b is a Mistral-7b-derivative model trained with the LAB methodology, using Mixtral-8x7b-Instruct as a teacher model.\",\"long_description\":\"This model is made with AutoGPTQ, which mainly leverages the quantization technique to 'compress' the model weights from FP16 to 4-bit INT and performs 'decompression' on-the-fly before computation (in FP16). As a result, the GPU memory, and the data transferring between GPU memory and GPU compute engine, compared to the original FP16 model, is greatly reduced. The major quantization parameters used in the process are listed below.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"7b\",\"min_shot_size\":1,\"task_ids\":[\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":3}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":32768},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":8192},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8192},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8192}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-18\"}]},{\"model_id\":\"ibm-mistralai/mixtral-8x7b-instruct-v01-q\",\"label\":\"mixtral-8x7b-instruct-v01-q\",\"provider\":\"Mistral AI\",\"tuned_by\":\"IBM\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Mixtral-8-7b-instruct-v01-gptq model is made with AutoGPTQ, which mainly leverages the quantization technique to 'compress' the model weights from FP16 to 4-bit INT and performs 'decompression' on-the-fly before computation (in FP16)\",\"long_description\":\"This model is made with AutoGPTQ, which mainly leverages the quantization technique to 'compress' the model weights from FP16 to 4-bit INT and performs 'decompression' on-the-fly before computation (in FP16). As a result, the GPU memory, and the data transferring between GPU memory and GPU compute engine, compared to the original FP16 model, is greatly reduced. The major quantization parameters used in the process are listed below.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"46.7b\",\"min_shot_size\":1,\"task_ids\":[\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":3}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":32768},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-02-15\"},{\"id\":\"constricted\",\"label\":\"deprecated and constricted\",\"start_date\":\"2024-04-19\",\"alternative_model_ids\":[\"ibm-mistralai/mixtral-8x7b-instruct-v01\"]},{\"id\":\"withdrawn\",\"start_date\":\"2024-06-20\",\"alternative_model_ids\":[\"ibm-mistralai/mixtral-8x7b-instruct-v01\"]}]},{\"model_id\":\"ibm/granite-13b-chat-v2\",\"label\":\"granite-13b-chat-v2\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"13b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":3}},{\"id\":\"summarization\",\"ratings\":{\"quality\":2}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":2}},{\"id\":\"classification\",\"ratings\":{\"quality\":3}},{\"id\":\"generation\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":2}}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":8191},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8191},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8191}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-12-01\"}],\"versions\":[{\"version\":\"2.1.0\",\"available_date\":\"2024-02-15\"},{\"version\":\"2.0.0\",\"available_date\":\"2023-12-01\"}]},{\"model_id\":\"ibm/granite-13b-instruct-v2\",\"label\":\"granite-13b-instruct-v2\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"prompt_tune_inferable\"},{\"id\":\"prompt_tune_trainable\"},{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"13b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":3}},{\"id\":\"summarization\",\"ratings\":{\"quality\":2},\"tags\":[\"function_prompt_tune_trainable\"],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"text\"},\"init_text\":{\"default\":\"Please write a summary highlighting the main points of the following text:\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":40,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"Please write a summary highlighting the main points of the following text: {{input}}\"},\"batch_size\":{\"default\":8,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":1024},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":512},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":1,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.0002,\"min\":0.00001,\"max\":0.5}}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":2}},{\"id\":\"classification\",\"ratings\":{\"quality\":3},\"tags\":[\"function_prompt_tune_trainable\"],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"text\"},\"init_text\":{\"default\":\"Classify the text:\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":20,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"Input: {{input}} Output:\"},\"batch_size\":{\"default\":8,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":1024},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":512},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":32,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.0006,\"min\":0.00001,\"max\":0.5}}},{\"id\":\"generation\",\"tags\":[\"function_prompt_tune_trainable\"],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"text\"},\"init_text\":{\"default\":\"text\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":20,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"{{input}}\"},\"batch_size\":{\"default\":16,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":1024},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":512},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":16,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.0002,\"min\":0.00001,\"max\":0.5}}},{\"id\":\"extraction\",\"ratings\":{\"quality\":2}}],\"model_limits\":{\"max_sequence_length\":8192,\"training_data_max_records\":10000},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":8191},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8191},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8191}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-12-01\"}],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"random\"},\"init_text\":{\"default\":\"text\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":20,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"{{input}}\"},\"batch_size\":{\"default\":16,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":1024},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":512},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":16,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.0002,\"min\":0.00001,\"max\":0.5}}},{\"model_id\":\"ibm/granite-20b-code-instruct\",\"label\":\"granite-20b-code-instruct\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"20b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\"},{\"id\":\"classification\"},{\"id\":\"generation\"},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-06\"}]},{\"model_id\":\"ibm/granite-20b-multilingual\",\"label\":\"granite-20b-multilingual\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"20b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":3}},{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":3}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-03-14\"}],\"versions\":[{\"version\":\"1.1.0\",\"available_date\":\"2024-04-18\"},{\"version\":\"1.0.0\",\"available_date\":\"2024-03-14\"}]},{\"model_id\":\"ibm/granite-34b-code-instruct\",\"label\":\"granite-34b-code-instruct\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"34b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\"},{\"id\":\"classification\"},{\"id\":\"generation\"},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":16384},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":8192},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8192},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8192}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-06\"}]},{\"model_id\":\"ibm/granite-3b-code-instruct\",\"label\":\"granite-3b-code-instruct\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"3b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\"},{\"id\":\"classification\"},{\"id\":\"generation\"},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-09\"}]},{\"model_id\":\"ibm/granite-7b-lab\",\"label\":\"granite-7b-lab\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"7b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\"},{\"id\":\"retrieval_augmented_generation\"},{\"id\":\"classification\"},{\"id\":\"generation\"},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":4096},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4095},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-18\"}]},{\"model_id\":\"ibm/granite-8b-code-instruct\",\"label\":\"granite-8b-code-instruct\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"8b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\"},{\"id\":\"classification\"},{\"id\":\"generation\"},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-09\"}]},{\"model_id\":\"meta-llama/llama-2-13b-chat\",\"label\":\"llama-2-13b-chat\",\"provider\":\"Meta\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"prompt_tune_inferable\"},{\"id\":\"prompt_tune_trainable\"},{\"id\":\"text_generation\"}],\"short_description\":\"Llama-2-13b-chat is an auto-regressive language model that uses an optimized transformer architecture.\",\"long_description\":\"Llama-2-13b-chat is a pretrained and fine-tuned generative text model with 13 billion parameters, optimized for dialogue use cases.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"13b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":3},\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":4}},{\"id\":\"classification\",\"ratings\":{\"quality\":4},\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"generation\",\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":4096,\"training_data_max_records\":10000},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":2048},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-11-09\"}],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"random\"},\"init_text\":{\"default\":\"text\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":20,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"{{input}}\"},\"batch_size\":{\"default\":8,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":1024},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":512},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":16,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.002,\"min\":0.00001,\"max\":0.5}}},{\"model_id\":\"meta-llama/llama-2-70b-chat\",\"label\":\"llama-2-70b-chat\",\"provider\":\"Meta\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Llama-2-70b-chat is an auto-regressive language model that uses an optimized transformer architecture.\",\"long_description\":\"Llama-2-70b-chat is a pretrained and fine-tuned generative text model with 70 billion parameters, optimized for dialogue use cases.\",\"input_tier\":\"class_2\",\"output_tier\":\"class_2\",\"number_params\":\"70b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":3}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":4}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":4096},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":900},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-09-07\"}]},{\"model_id\":\"meta-llama/llama-3-70b-instruct\",\"label\":\"llama-3-70b-instruct\",\"provider\":\"Meta\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Llama-3-70b-instruct is an auto-regressive language model that uses an optimized transformer architecture.\",\"long_description\":\"Llama-3-70b-instruct is a pretrained and fine-tuned generative text model with 70 billion parameters, optimized for dialogue use cases.\",\"input_tier\":\"class_2\",\"output_tier\":\"class_2\",\"number_params\":\"70b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":3}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":4}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-18\"}]},{\"model_id\":\"meta-llama/llama-3-8b-instruct\",\"label\":\"llama-3-8b-instruct\",\"provider\":\"Meta\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Llama-3-8b-instruct is an auto-regressive language model that uses an optimized transformer architecture.\",\"long_description\":\"Llama-3-8b-instruct is a pretrained and fine-tuned generative text model with 8 billion parameters, optimized for dialogue use cases.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"8b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":3}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":4}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-18\"}]},{\"model_id\":\"mistralai/mixtral-8x7b-instruct-v01\",\"label\":\"mixtral-8x7b-instruct-v01\",\"provider\":\"Mistral AI\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts.\",\"long_description\":\"This model is made with AutoGPTQ, which mainly leverages the quantization technique to 'compress' the model weights from FP16 to 4-bit INT and performs 'decompression' on-the-fly before computation (in FP16). As a result, the GPU memory, and the data transferring between GPU memory and GPU compute engine, compared to the original FP16 model, is greatly reduced. The major quantization parameters used in the process are listed below.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"46.7b\",\"min_shot_size\":1,\"task_ids\":[\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":3}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":32768},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":16384},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":16384},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":16384}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-17\"}]}]}\n",
      "[INFO] 2024-06-19 16:24:24,559 wml_resource.py:91: Successfully finished Get next details for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2024-06-11&space_id=c6dd344f-1aab-4b5e-8edb-2a21fa1e0de0&filters=function_text_generation&limit=200'\n",
      "[DEBUG] 2024-06-19 16:24:24,560 wml_resource.py:96: Response(GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2024-06-11&space_id=c6dd344f-1aab-4b5e-8edb-2a21fa1e0de0&filters=function_text_generation&limit=200): {\"total_count\":20,\"limit\":200,\"first\":{\"href\":\"https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2024-06-11&space_id=c6dd344f-1aab-4b5e-8edb-2a21fa1e0de0&filters=function_text_generation&limit=200\"},\"resources\":[{\"model_id\":\"bigscience/mt0-xxl\",\"label\":\"mt0-xxl-13b\",\"provider\":\"BigScience\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"An instruction-tuned iteration on mT5.\",\"long_description\":\"mt0-xxl (13B) is an instruction-tuned iteration on mT5. Like BLOOMZ, it was fine-tuned on a cross-lingual task mixture dataset (xP3) using multitask prompted finetuning (MTF).\",\"input_tier\":\"class_2\",\"output_tier\":\"class_2\",\"number_params\":\"13b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":3}},{\"id\":\"summarization\",\"ratings\":{\"quality\":3}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":2}},{\"id\":\"classification\",\"ratings\":{\"quality\":3}},{\"id\":\"generation\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":2}}],\"model_limits\":{\"max_sequence_length\":4096},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":700},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-07-07\"}]},{\"model_id\":\"codellama/codellama-34b-instruct-hf\",\"label\":\"codellama-34b-instruct-hf\",\"provider\":\"Code Llama\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Code Llama is an AI model built on top of Llama 2, fine-tuned for generating and discussing code.\",\"long_description\":\"Code Llama is a pretrained and fine-tuned generative text models with 34 billion parameters. This model is designed for general code synthesis and understanding.\",\"input_tier\":\"class_2\",\"output_tier\":\"class_2\",\"number_params\":\"34b\",\"min_shot_size\":0,\"task_ids\":[\"code\"],\"tasks\":[{\"id\":\"code\"}],\"model_limits\":{\"max_sequence_length\":16384},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":8192},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8192},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8192}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-03-14\"}]},{\"model_id\":\"google/flan-t5-xl\",\"label\":\"flan-t5-xl-3b\",\"provider\":\"Google\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"prompt_tune_inferable\"},{\"id\":\"prompt_tune_trainable\"},{\"id\":\"text_generation\"}],\"short_description\":\"A pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format.\",\"long_description\":\"flan-t5-xl (3B) is a 3 billion parameter model based on the Flan-T5 family. It is a pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format, and fine-tuned on the Fine-tuned Language Net (FLAN) with instructions for better zero-shot and few-shot performance.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"3b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\",\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"retrieval_augmented_generation\"},{\"id\":\"classification\",\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"generation\",\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":4096,\"training_data_max_records\":10000},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4095},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-12-07\"}],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"random\"},\"init_text\":{\"default\":\"text\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":20,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"Input: {{input}} Output:\"},\"batch_size\":{\"default\":16,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":256},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":128},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":16,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.3,\"min\":0.00001,\"max\":0.5}}},{\"model_id\":\"google/flan-t5-xxl\",\"label\":\"flan-t5-xxl-11b\",\"provider\":\"Google\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"flan-t5-xxl is an 11 billion parameter model based on the Flan-T5 family.\",\"long_description\":\"flan-t5-xxl (11B) is an 11 billion parameter model based on the Flan-T5 family. It is a pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format, and fine-tuned on the Fine-tuned Language Net (FLAN) with instructions for better zero-shot and few-shot performance.\",\"input_tier\":\"class_2\",\"output_tier\":\"class_2\",\"number_params\":\"11b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":3}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":4096},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":700},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-07-07\"}]},{\"model_id\":\"google/flan-ul2\",\"label\":\"flan-ul2-20b\",\"provider\":\"Google\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"flan-ul2 is an encoder decoder model based on the T5 architecture and instruction-tuned using the Fine-tuned Language Net.\",\"long_description\":\"flan-ul2 (20B) is an encoder decoder model based on the T5 architecture and instruction-tuned using the Fine-tuned Language Net (FLAN). Compared to the original UL2 model, flan-ul2 (20B) is more usable for few-shot in-context learning because it was trained with a three times larger receptive field. flan-ul2 (20B) outperforms flan-t5 (11B) by an overall relative improvement of +3.2%.\",\"input_tier\":\"class_3\",\"output_tier\":\"class_3\",\"number_params\":\"20b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":4}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":4096},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":700},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-07-07\"}]},{\"model_id\":\"ibm-mistralai/merlinite-7b\",\"label\":\"merlinite-7b\",\"provider\":\"Mistral AI\",\"tuned_by\":\"IBM\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Merlinite-7b is a Mistral-7b-derivative model trained with the LAB methodology, using Mixtral-8x7b-Instruct as a teacher model.\",\"long_description\":\"This model is made with AutoGPTQ, which mainly leverages the quantization technique to 'compress' the model weights from FP16 to 4-bit INT and performs 'decompression' on-the-fly before computation (in FP16). As a result, the GPU memory, and the data transferring between GPU memory and GPU compute engine, compared to the original FP16 model, is greatly reduced. The major quantization parameters used in the process are listed below.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"7b\",\"min_shot_size\":1,\"task_ids\":[\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":3}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":32768},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":8192},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8192},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8192}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-18\"}]},{\"model_id\":\"ibm-mistralai/mixtral-8x7b-instruct-v01-q\",\"label\":\"mixtral-8x7b-instruct-v01-q\",\"provider\":\"Mistral AI\",\"tuned_by\":\"IBM\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Mixtral-8-7b-instruct-v01-gptq model is made with AutoGPTQ, which mainly leverages the quantization technique to 'compress' the model weights from FP16 to 4-bit INT and performs 'decompression' on-the-fly before computation (in FP16)\",\"long_description\":\"This model is made with AutoGPTQ, which mainly leverages the quantization technique to 'compress' the model weights from FP16 to 4-bit INT and performs 'decompression' on-the-fly before computation (in FP16). As a result, the GPU memory, and the data transferring between GPU memory and GPU compute engine, compared to the original FP16 model, is greatly reduced. The major quantization parameters used in the process are listed below.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"46.7b\",\"min_shot_size\":1,\"task_ids\":[\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":3}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":32768},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-02-15\"},{\"id\":\"constricted\",\"label\":\"deprecated and constricted\",\"start_date\":\"2024-04-19\",\"alternative_model_ids\":[\"ibm-mistralai/mixtral-8x7b-instruct-v01\"]},{\"id\":\"withdrawn\",\"start_date\":\"2024-06-20\",\"alternative_model_ids\":[\"ibm-mistralai/mixtral-8x7b-instruct-v01\"]}]},{\"model_id\":\"ibm/granite-13b-chat-v2\",\"label\":\"granite-13b-chat-v2\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"13b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":3}},{\"id\":\"summarization\",\"ratings\":{\"quality\":2}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":2}},{\"id\":\"classification\",\"ratings\":{\"quality\":3}},{\"id\":\"generation\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":2}}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":8191},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8191},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8191}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-12-01\"}],\"versions\":[{\"version\":\"2.1.0\",\"available_date\":\"2024-02-15\"},{\"version\":\"2.0.0\",\"available_date\":\"2023-12-01\"}]},{\"model_id\":\"ibm/granite-13b-instruct-v2\",\"label\":\"granite-13b-instruct-v2\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"prompt_tune_inferable\"},{\"id\":\"prompt_tune_trainable\"},{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"13b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":3}},{\"id\":\"summarization\",\"ratings\":{\"quality\":2},\"tags\":[\"function_prompt_tune_trainable\"],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"text\"},\"init_text\":{\"default\":\"Please write a summary highlighting the main points of the following text:\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":40,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"Please write a summary highlighting the main points of the following text: {{input}}\"},\"batch_size\":{\"default\":8,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":1024},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":512},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":1,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.0002,\"min\":0.00001,\"max\":0.5}}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":2}},{\"id\":\"classification\",\"ratings\":{\"quality\":3},\"tags\":[\"function_prompt_tune_trainable\"],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"text\"},\"init_text\":{\"default\":\"Classify the text:\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":20,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"Input: {{input}} Output:\"},\"batch_size\":{\"default\":8,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":1024},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":512},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":32,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.0006,\"min\":0.00001,\"max\":0.5}}},{\"id\":\"generation\",\"tags\":[\"function_prompt_tune_trainable\"],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"text\"},\"init_text\":{\"default\":\"text\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":20,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"{{input}}\"},\"batch_size\":{\"default\":16,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":1024},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":512},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":16,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.0002,\"min\":0.00001,\"max\":0.5}}},{\"id\":\"extraction\",\"ratings\":{\"quality\":2}}],\"model_limits\":{\"max_sequence_length\":8192,\"training_data_max_records\":10000},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":8191},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8191},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8191}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-12-01\"}],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"random\"},\"init_text\":{\"default\":\"text\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":20,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"{{input}}\"},\"batch_size\":{\"default\":16,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":1024},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":512},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":16,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.0002,\"min\":0.00001,\"max\":0.5}}},{\"model_id\":\"ibm/granite-20b-code-instruct\",\"label\":\"granite-20b-code-instruct\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"20b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\"},{\"id\":\"classification\"},{\"id\":\"generation\"},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-06\"}]},{\"model_id\":\"ibm/granite-20b-multilingual\",\"label\":\"granite-20b-multilingual\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"20b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":3}},{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":3}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-03-14\"}],\"versions\":[{\"version\":\"1.1.0\",\"available_date\":\"2024-04-18\"},{\"version\":\"1.0.0\",\"available_date\":\"2024-03-14\"}]},{\"model_id\":\"ibm/granite-34b-code-instruct\",\"label\":\"granite-34b-code-instruct\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"34b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\"},{\"id\":\"classification\"},{\"id\":\"generation\"},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":16384},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":8192},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8192},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8192}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-06\"}]},{\"model_id\":\"ibm/granite-3b-code-instruct\",\"label\":\"granite-3b-code-instruct\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"3b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\"},{\"id\":\"classification\"},{\"id\":\"generation\"},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-09\"}]},{\"model_id\":\"ibm/granite-7b-lab\",\"label\":\"granite-7b-lab\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"7b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\"},{\"id\":\"retrieval_augmented_generation\"},{\"id\":\"classification\"},{\"id\":\"generation\"},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":4096},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4095},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-18\"}]},{\"model_id\":\"ibm/granite-8b-code-instruct\",\"label\":\"granite-8b-code-instruct\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"8b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\"},{\"id\":\"classification\"},{\"id\":\"generation\"},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-09\"}]},{\"model_id\":\"meta-llama/llama-2-13b-chat\",\"label\":\"llama-2-13b-chat\",\"provider\":\"Meta\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"prompt_tune_inferable\"},{\"id\":\"prompt_tune_trainable\"},{\"id\":\"text_generation\"}],\"short_description\":\"Llama-2-13b-chat is an auto-regressive language model that uses an optimized transformer architecture.\",\"long_description\":\"Llama-2-13b-chat is a pretrained and fine-tuned generative text model with 13 billion parameters, optimized for dialogue use cases.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"13b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":3},\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":4}},{\"id\":\"classification\",\"ratings\":{\"quality\":4},\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"generation\",\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":4096,\"training_data_max_records\":10000},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":2048},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-11-09\"}],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"random\"},\"init_text\":{\"default\":\"text\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":20,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"{{input}}\"},\"batch_size\":{\"default\":8,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":1024},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":512},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":16,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.002,\"min\":0.00001,\"max\":0.5}}},{\"model_id\":\"meta-llama/llama-2-70b-chat\",\"label\":\"llama-2-70b-chat\",\"provider\":\"Meta\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Llama-2-70b-chat is an auto-regressive language model that uses an optimized transformer architecture.\",\"long_description\":\"Llama-2-70b-chat is a pretrained and fine-tuned generative text model with 70 billion parameters, optimized for dialogue use cases.\",\"input_tier\":\"class_2\",\"output_tier\":\"class_2\",\"number_params\":\"70b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":3}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":4}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":4096},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":900},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-09-07\"}]},{\"model_id\":\"meta-llama/llama-3-70b-instruct\",\"label\":\"llama-3-70b-instruct\",\"provider\":\"Meta\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Llama-3-70b-instruct is an auto-regressive language model that uses an optimized transformer architecture.\",\"long_description\":\"Llama-3-70b-instruct is a pretrained and fine-tuned generative text model with 70 billion parameters, optimized for dialogue use cases.\",\"input_tier\":\"class_2\",\"output_tier\":\"class_2\",\"number_params\":\"70b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":3}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":4}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-18\"}]},{\"model_id\":\"meta-llama/llama-3-8b-instruct\",\"label\":\"llama-3-8b-instruct\",\"provider\":\"Meta\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Llama-3-8b-instruct is an auto-regressive language model that uses an optimized transformer architecture.\",\"long_description\":\"Llama-3-8b-instruct is a pretrained and fine-tuned generative text model with 8 billion parameters, optimized for dialogue use cases.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"8b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":3}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":4}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-18\"}]},{\"model_id\":\"mistralai/mixtral-8x7b-instruct-v01\",\"label\":\"mixtral-8x7b-instruct-v01\",\"provider\":\"Mistral AI\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts.\",\"long_description\":\"This model is made with AutoGPTQ, which mainly leverages the quantization technique to 'compress' the model weights from FP16 to 4-bit INT and performs 'decompression' on-the-fly before computation (in FP16). As a result, the GPU memory, and the data transferring between GPU memory and GPU compute engine, compared to the original FP16 model, is greatly reduced. The major quantization parameters used in the process are listed below.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"46.7b\",\"min_shot_size\":1,\"task_ids\":[\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":3}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":32768},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":16384},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":16384},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":16384}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-17\"}]}]}\n"
     ]
    }
   ],
   "source": [
    "from tracing_auto_instrumentation.ibm import wrap_watson\n",
    "\n",
    "TRACE_NAME = \"elementary-my-dear-watson\"\n",
    "\n",
    "tracer: LastMileTracer = get_lastmile_tracer(\n",
    "    TRACE_NAME\n",
    ")\n",
    "watson_model: Model = init_watson_model()\n",
    "wrapper = wrap_watson(watson_model, tracer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_generation(mode: Mode, prompt: str) -> None:\n",
    "    logger.info(\"IBM WatsonX Generation Script Starting...\")\n",
    "\n",
    "    # n.b. required for multiple api keys, make sure `.env` is in your local path\n",
    "    dotenv.load_dotenv()\n",
    "    \n",
    "    response: str = \"\"\n",
    "    if Mode.GENERATE == mode:\n",
    "        logger.info(f\"running with mode: {mode}\")\n",
    "        response = wrapper.generate(prompt)\n",
    "    elif Mode.GENERATE_TEXT == mode:\n",
    "        logger.info(f\"running with mode: {mode}\")\n",
    "        response = wrapper.generate_text(prompt)\n",
    "    else:\n",
    "        logger.error(f\"unsupported mode: {mode}\")\n",
    "    logger.info(f\"response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2024-06-19 16:29:21,647 1619447352.py:2: IBM WatsonX Generation Script Starting...\n",
      "[INFO] 2024-06-19 16:29:21,649 1619447352.py:9: running with mode: Mode.GENERATE\n",
      "[INFO] 2024-06-19 16:29:22,598 wml_resource.py:91: Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2024-06-11'\n",
      "[DEBUG] 2024-06-19 16:29:22,598 wml_resource.py:96: Response(POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2024-06-11): {\"model_id\":\"ibm/granite-13b-chat-v2\",\"model_version\":\"2.1.0\",\"created_at\":\"2024-06-19T20:29:22.565Z\",\"results\":[{\"generated_text\":\"\\n\\nCertainly! One of Edison's most famous inventions was the phonograph, the world's first practical device for recording and\",\"generated_token_count\":25,\"input_token_count\":13,\"stop_reason\":\"max_tokens\"}]}\n",
      "[INFO] 2024-06-19 16:29:22,816 1619447352.py:16: response: {'model_id': 'ibm/granite-13b-chat-v2', 'model_version': '2.1.0', 'created_at': '2024-06-19T20:29:22.565Z', 'results': [{'generated_text': \"\\n\\nCertainly! One of Edison's most famous inventions was the phonograph, the world's first practical device for recording and\", 'generated_token_count': 25, 'input_token_count': 13, 'stop_reason': 'max_tokens'}]}\n"
     ]
    }
   ],
   "source": [
    "response = run_generation(\n",
    "    mode=Mode.GENERATE,\n",
    "    prompt=\"Can you please tell me an amusing anecdote about Thomas Edison?\",\n",
    ")\n",
    "\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
