{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# _____ ______  _______      _  _  _ _______ _______ _______  _____  __   _      _     _\n",
    "#   |   |_____] |  |  |      |  |  | |_____|    |    |______ |     | | \\  |       \\___/ \n",
    "# __|__ |_____] |  |  |      |__|__| |     |    |    ______| |_____| |  \\_|      _/   \\_                                                                                       \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"tracing_auto_instrumentation[ibm]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "from ibm_watsonx_ai.foundation_models import Model\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "\n",
    "from lastmile_eval.rag.debugger.api import LastMileTracer\n",
    "from lastmile_eval.rag.debugger.tracing.sdk import get_lastmile_tracer\n",
    "\n",
    "from tracing_auto_instrumentation.ibm import wrap_watson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mode(Enum):\n",
    "    GENERATE = \"GENERATE\"\n",
    "    GENERATE_TEXT = \"GENERATE_TEXT\"\n",
    "\n",
    "\n",
    "def init_watson_model() -> Model:\n",
    "    # To display example params enter\n",
    "    GenParams().get_example_values()\n",
    "    generate_params = {GenParams.MAX_NEW_TOKENS: 25}\n",
    "\n",
    "    watson_model = Model(\n",
    "        model_id=ModelTypes.GRANITE_13B_CHAT_V2,\n",
    "        params=generate_params,\n",
    "        credentials=dict(\n",
    "            api_key=os.getenv(\"WATSONX_API_KEY\"),\n",
    "            url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "        ),\n",
    "        space_id=os.getenv(\"WATSONX_SPACE_ID\"),\n",
    "        verify=None,\n",
    "        validate=True,\n",
    "    )\n",
    "\n",
    "    return watson_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2024-06-17 01:14:18,659 client.py:458: Client successfully initialized\n",
      "[INFO] 2024-06-17 01:14:19,671 wml_resource.py:93: Successfully finished Get available foundation models for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2024-05-10&space_id=c6dd344f-1aab-4b5e-8edb-2a21fa1e0de0&limit=200'\n",
      "[DEBUG] 2024-06-17 01:14:19,672 wml_resource.py:98: Response(GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2024-05-10&space_id=c6dd344f-1aab-4b5e-8edb-2a21fa1e0de0&limit=200): {\"total_count\":25,\"limit\":200,\"first\":{\"href\":\"https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2024-05-10&space_id=c6dd344f-1aab-4b5e-8edb-2a21fa1e0de0&limit=200\"},\"resources\":[{\"model_id\":\"baai/bge-large-en-v1\",\"label\":\"bge-large-en-v1\",\"provider\":\"baai\",\"source\":\"baai\",\"functions\":[{\"id\":\"embedding\"}],\"short_description\":\"An embedding model with version 1.5. It has 335 million parameters and an embedding dimension of 1024.\",\"long_description\":\"This model has multi-functionality like dense retrieval, sparse retrieval, multi-vector, Multi-Linguality, and Multi-Granularity(8192 tokens)\",\"input_tier\":\"class_c1\",\"output_tier\":\"class_c1\",\"number_params\":\"335m\",\"limits\":{\"lite\":{\"call_time\":\"5m0s\"},\"v2-professional\":{\"call_time\":\"10m0s\"},\"v2-standard\":{\"call_time\":\"10m0s\"}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-16\"}]},{\"model_id\":\"bigscience/mt0-xxl\",\"label\":\"mt0-xxl-13b\",\"provider\":\"BigScience\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"An instruction-tuned iteration on mT5.\",\"long_description\":\"mt0-xxl (13B) is an instruction-tuned iteration on mT5. Like BLOOMZ, it was fine-tuned on a cross-lingual task mixture dataset (xP3) using multitask prompted finetuning (MTF).\",\"input_tier\":\"class_2\",\"output_tier\":\"class_2\",\"number_params\":\"13b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":3}},{\"id\":\"summarization\",\"ratings\":{\"quality\":3}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":2}},{\"id\":\"classification\",\"ratings\":{\"quality\":3}},{\"id\":\"generation\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":2}}],\"model_limits\":{\"max_sequence_length\":4096},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":700},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-07-07\"}]},{\"model_id\":\"codellama/codellama-34b-instruct-hf\",\"label\":\"codellama-34b-instruct-hf\",\"provider\":\"Code Llama\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Code Llama is an AI model built on top of Llama 2, fine-tuned for generating and discussing code.\",\"long_description\":\"Code Llama is a pretrained and fine-tuned generative text models with 34 billion parameters. This model is designed for general code synthesis and understanding.\",\"input_tier\":\"class_2\",\"output_tier\":\"class_2\",\"number_params\":\"34b\",\"min_shot_size\":0,\"task_ids\":[\"code\"],\"tasks\":[{\"id\":\"code\"}],\"model_limits\":{\"max_sequence_length\":16384},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":8192},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8192},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8192}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-03-14\"}]},{\"model_id\":\"google/flan-t5-xl\",\"label\":\"flan-t5-xl-3b\",\"provider\":\"Google\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"prompt_tune_inferable\"},{\"id\":\"prompt_tune_trainable\"},{\"id\":\"text_generation\"}],\"short_description\":\"A pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format.\",\"long_description\":\"flan-t5-xl (3B) is a 3 billion parameter model based on the Flan-T5 family. It is a pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format, and fine-tuned on the Fine-tuned Language Net (FLAN) with instructions for better zero-shot and few-shot performance.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"3b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\",\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"retrieval_augmented_generation\"},{\"id\":\"classification\",\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"generation\",\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":4096,\"training_data_max_records\":10000},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4095},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-12-07\"}],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"random\"},\"init_text\":{\"default\":\"text\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":20,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"Input: {{input}} Output:\"},\"batch_size\":{\"default\":16,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":256},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":128},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":16,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.3,\"min\":0.00001,\"max\":0.5}}},{\"model_id\":\"google/flan-t5-xxl\",\"label\":\"flan-t5-xxl-11b\",\"provider\":\"Google\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"flan-t5-xxl is an 11 billion parameter model based on the Flan-T5 family.\",\"long_description\":\"flan-t5-xxl (11B) is an 11 billion parameter model based on the Flan-T5 family. It is a pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format, and fine-tuned on the Fine-tuned Language Net (FLAN) with instructions for better zero-shot and few-shot performance.\",\"input_tier\":\"class_2\",\"output_tier\":\"class_2\",\"number_params\":\"11b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":3}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":4096},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":700},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-07-07\"}]},{\"model_id\":\"google/flan-ul2\",\"label\":\"flan-ul2-20b\",\"provider\":\"Google\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"flan-ul2 is an encoder decoder model based on the T5 architecture and instruction-tuned using the Fine-tuned Language Net.\",\"long_description\":\"flan-ul2 (20B) is an encoder decoder model based on the T5 architecture and instruction-tuned using the Fine-tuned Language Net (FLAN). Compared to the original UL2 model, flan-ul2 (20B) is more usable for few-shot in-context learning because it was trained with a three times larger receptive field. flan-ul2 (20B) outperforms flan-t5 (11B) by an overall relative improvement of +3.2%.\",\"input_tier\":\"class_3\",\"output_tier\":\"class_3\",\"number_params\":\"20b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":4}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":4096},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":700},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-07-07\"}]},{\"model_id\":\"ibm-mistralai/merlinite-7b\",\"label\":\"merlinite-7b\",\"provider\":\"Mistral AI\",\"tuned_by\":\"IBM\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Merlinite-7b is a Mistral-7b-derivative model trained with the LAB methodology, using Mixtral-8x7b-Instruct as a teacher model.\",\"long_description\":\"This model is made with AutoGPTQ, which mainly leverages the quantization technique to 'compress' the model weights from FP16 to 4-bit INT and performs 'decompression' on-the-fly before computation (in FP16). As a result, the GPU memory, and the data transferring between GPU memory and GPU compute engine, compared to the original FP16 model, is greatly reduced. The major quantization parameters used in the process are listed below.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"7b\",\"min_shot_size\":1,\"task_ids\":[\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":3}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":32768},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":8192},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8192},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8192}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-18\"}]},{\"model_id\":\"ibm-mistralai/mixtral-8x7b-instruct-v01-q\",\"label\":\"mixtral-8x7b-instruct-v01-q\",\"provider\":\"Mistral AI\",\"tuned_by\":\"IBM\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Mixtral-8-7b-instruct-v01-gptq model is made with AutoGPTQ, which mainly leverages the quantization technique to 'compress' the model weights from FP16 to 4-bit INT and performs 'decompression' on-the-fly before computation (in FP16)\",\"long_description\":\"This model is made with AutoGPTQ, which mainly leverages the quantization technique to 'compress' the model weights from FP16 to 4-bit INT and performs 'decompression' on-the-fly before computation (in FP16). As a result, the GPU memory, and the data transferring between GPU memory and GPU compute engine, compared to the original FP16 model, is greatly reduced. The major quantization parameters used in the process are listed below.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"46.7b\",\"min_shot_size\":1,\"task_ids\":[\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":3}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":32768},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-02-15\"},{\"id\":\"constricted\",\"label\":\"deprecated and constricted\",\"start_date\":\"2024-04-19\",\"alternative_model_ids\":[\"ibm-mistralai/mixtral-8x7b-instruct-v01\"]},{\"id\":\"withdrawn\",\"start_date\":\"2024-06-20\",\"alternative_model_ids\":[\"ibm-mistralai/mixtral-8x7b-instruct-v01\"]}]},{\"model_id\":\"ibm/granite-13b-chat-v2\",\"label\":\"granite-13b-chat-v2\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"13b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":3}},{\"id\":\"summarization\",\"ratings\":{\"quality\":2}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":2}},{\"id\":\"classification\",\"ratings\":{\"quality\":3}},{\"id\":\"generation\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":2}}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":8191},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8191},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8191}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-12-01\"}],\"versions\":[{\"version\":\"2.1.0\",\"available_date\":\"2024-02-15\"},{\"version\":\"2.0.0\",\"available_date\":\"2023-12-01\"}]},{\"model_id\":\"ibm/granite-13b-instruct-v2\",\"label\":\"granite-13b-instruct-v2\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"prompt_tune_inferable\"},{\"id\":\"prompt_tune_trainable\"},{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"13b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":3}},{\"id\":\"summarization\",\"ratings\":{\"quality\":2},\"tags\":[\"function_prompt_tune_trainable\"],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"text\"},\"init_text\":{\"default\":\"Please write a summary highlighting the main points of the following text:\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":40,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"Please write a summary highlighting the main points of the following text: {{input}}\"},\"batch_size\":{\"default\":8,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":1024},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":512},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":1,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.0002,\"min\":0.00001,\"max\":0.5}}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":2}},{\"id\":\"classification\",\"ratings\":{\"quality\":3},\"tags\":[\"function_prompt_tune_trainable\"],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"text\"},\"init_text\":{\"default\":\"Classify the text:\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":20,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"Input: {{input}} Output:\"},\"batch_size\":{\"default\":8,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":1024},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":512},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":32,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.0006,\"min\":0.00001,\"max\":0.5}}},{\"id\":\"generation\",\"tags\":[\"function_prompt_tune_trainable\"],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"text\"},\"init_text\":{\"default\":\"text\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":20,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"{{input}}\"},\"batch_size\":{\"default\":16,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":1024},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":512},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":16,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.0002,\"min\":0.00001,\"max\":0.5}}},{\"id\":\"extraction\",\"ratings\":{\"quality\":2}}],\"model_limits\":{\"max_sequence_length\":8192,\"training_data_max_records\":10000},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":8191},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8191},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8191}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-12-01\"}],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"random\"},\"init_text\":{\"default\":\"text\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":20,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"{{input}}\"},\"batch_size\":{\"default\":16,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":1024},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":512},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":16,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.0002,\"min\":0.00001,\"max\":0.5}}},{\"model_id\":\"ibm/granite-20b-code-instruct\",\"label\":\"granite-20b-code-instruct\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"20b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\"},{\"id\":\"classification\"},{\"id\":\"generation\"},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-06\"}]},{\"model_id\":\"ibm/granite-20b-multilingual\",\"label\":\"granite-20b-multilingual\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"20b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":3}},{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":3}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-03-14\"}],\"versions\":[{\"version\":\"1.1.0\",\"available_date\":\"2024-04-18\"},{\"version\":\"1.0.0\",\"available_date\":\"2024-03-14\"}]},{\"model_id\":\"ibm/granite-34b-code-instruct\",\"label\":\"granite-34b-code-instruct\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"34b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\"},{\"id\":\"classification\"},{\"id\":\"generation\"},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":16384},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":8192},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8192},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8192}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-06\"}]},{\"model_id\":\"ibm/granite-3b-code-instruct\",\"label\":\"granite-3b-code-instruct\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"3b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\"},{\"id\":\"classification\"},{\"id\":\"generation\"},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-09\"}]},{\"model_id\":\"ibm/granite-7b-lab\",\"label\":\"granite-7b-lab\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"7b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\"},{\"id\":\"retrieval_augmented_generation\"},{\"id\":\"classification\"},{\"id\":\"generation\"},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":4096},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4095},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-18\"}]},{\"model_id\":\"ibm/granite-8b-code-instruct\",\"label\":\"granite-8b-code-instruct\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"8b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\"},{\"id\":\"classification\"},{\"id\":\"generation\"},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-09\"}]},{\"model_id\":\"ibm/slate-125m-english-rtrvr\",\"label\":\"slate-125m-english-rtrvr\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"embedding\"}],\"short_description\":\"An embedding model. It has 125 million parameters and an embedding dimension of 768.\",\"long_description\":\"This model follows the standard 'sentence transformers' approach, relying on bi-encoders. It generates embeddings for various inputs such as queries, passages, or documents. The training objective is to maximize cosine similarity between two text pieces: text A (query text) and text B (passage text). This process yields sentence embeddings q and p, allowing for comparison through cosine similarity.\",\"input_tier\":\"class_c1\",\"output_tier\":\"class_c1\",\"number_params\":\"125m\",\"limits\":{\"lite\":{\"call_time\":\"5m0s\"},\"v2-professional\":{\"call_time\":\"10m0s\"},\"v2-standard\":{\"call_time\":\"10m0s\"}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-18\"}]},{\"model_id\":\"ibm/slate-30m-english-rtrvr\",\"label\":\"slate-30m-english-rtrvr\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"embedding\"}],\"short_description\":\"An embedding model. It has 30 million parameters and an embedding dimension of 384.\",\"long_description\":\"This model follows the standard 'sentence transformers' approach, relying on bi-encoders. It generates embeddings for various inputs such as queries, passages, or documents. The training objective is to maximize cosine similarity between two text pieces: text A (query text) and text B (passage text). This process yields sentence embeddings q and p, allowing for comparison through cosine similarity.\",\"input_tier\":\"class_c1\",\"output_tier\":\"class_c1\",\"number_params\":\"30m\",\"limits\":{\"lite\":{\"call_time\":\"5m0s\"},\"v2-professional\":{\"call_time\":\"10m0s\"},\"v2-standard\":{\"call_time\":\"10m0s\"}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-18\"}]},{\"model_id\":\"intfloat/multilingual-e5-large\",\"label\":\"multilingual-e5-large\",\"provider\":\"intfloat\",\"source\":\"intfloat\",\"functions\":[{\"id\":\"embedding\"}],\"short_description\":\"An embedding model. It has 560 million parameters, has 24 layers and the embedding size is 1024.\",\"long_description\":\"This model gets continually trained on a mixture of multilingual datasets. It supports 100 languages from xlm-roberta.\",\"input_tier\":\"class_c1\",\"output_tier\":\"class_c1\",\"number_params\":\"560m\",\"limits\":{\"lite\":{\"call_time\":\"5m0s\"},\"v2-professional\":{\"call_time\":\"10m0s\"},\"v2-standard\":{\"call_time\":\"10m0s\"}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-16\"}]},{\"model_id\":\"meta-llama/llama-2-13b-chat\",\"label\":\"llama-2-13b-chat\",\"provider\":\"Meta\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"prompt_tune_inferable\"},{\"id\":\"prompt_tune_trainable\"},{\"id\":\"text_generation\"}],\"short_description\":\"Llama-2-13b-chat is an auto-regressive language model that uses an optimized transformer architecture.\",\"long_description\":\"Llama-2-13b-chat is a pretrained and fine-tuned generative text model with 13 billion parameters, optimized for dialogue use cases.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"13b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":3},\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":4}},{\"id\":\"classification\",\"ratings\":{\"quality\":4},\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"generation\",\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":4096,\"training_data_max_records\":10000},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":2048},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-11-09\"}],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"random\"},\"init_text\":{\"default\":\"text\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":20,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"{{input}}\"},\"batch_size\":{\"default\":8,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":1024},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":512},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":16,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.002,\"min\":0.00001,\"max\":0.5}}},{\"model_id\":\"meta-llama/llama-2-70b-chat\",\"label\":\"llama-2-70b-chat\",\"provider\":\"Meta\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Llama-2-70b-chat is an auto-regressive language model that uses an optimized transformer architecture.\",\"long_description\":\"Llama-2-70b-chat is a pretrained and fine-tuned generative text model with 70 billion parameters, optimized for dialogue use cases.\",\"input_tier\":\"class_2\",\"output_tier\":\"class_2\",\"number_params\":\"70b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":3}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":4}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":4096},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":900},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-09-07\"}]},{\"model_id\":\"meta-llama/llama-3-70b-instruct\",\"label\":\"llama-3-70b-instruct\",\"provider\":\"Meta\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Llama-3-70b-instruct is an auto-regressive language model that uses an optimized transformer architecture.\",\"long_description\":\"Llama-3-70b-instruct is a pretrained and fine-tuned generative text model with 70 billion parameters, optimized for dialogue use cases.\",\"input_tier\":\"class_2\",\"output_tier\":\"class_2\",\"number_params\":\"70b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":3}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":4}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-18\"}]},{\"model_id\":\"meta-llama/llama-3-8b-instruct\",\"label\":\"llama-3-8b-instruct\",\"provider\":\"Meta\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Llama-3-8b-instruct is an auto-regressive language model that uses an optimized transformer architecture.\",\"long_description\":\"Llama-3-8b-instruct is a pretrained and fine-tuned generative text model with 8 billion parameters, optimized for dialogue use cases.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"8b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":3}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":4}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-18\"}]},{\"model_id\":\"mistralai/mixtral-8x7b-instruct-v01\",\"label\":\"mixtral-8x7b-instruct-v01\",\"provider\":\"Mistral AI\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts.\",\"long_description\":\"This model is made with AutoGPTQ, which mainly leverages the quantization technique to 'compress' the model weights from FP16 to 4-bit INT and performs 'decompression' on-the-fly before computation (in FP16). As a result, the GPU memory, and the data transferring between GPU memory and GPU compute engine, compared to the original FP16 model, is greatly reduced. The major quantization parameters used in the process are listed below.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"46.7b\",\"min_shot_size\":1,\"task_ids\":[\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":3}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":32768},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":16384},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":16384},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":16384}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-17\"}]},{\"model_id\":\"sentence-transformers/all-minilm-l12-v2\",\"label\":\"all-minilm-l12-v2\",\"provider\":\"sentence-transformers\",\"source\":\"sentence-transformers\",\"functions\":[{\"id\":\"embedding\"}],\"short_description\":\"An embedding model with 128 token limit. It has 33.4 million parameters and an embedding dimension of 384.\",\"long_description\":\"This model follows sentence transformers approach, it maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\",\"input_tier\":\"class_c1\",\"output_tier\":\"class_c1\",\"number_params\":\"33.4m\",\"limits\":{\"lite\":{\"call_time\":\"5m0s\"},\"v2-professional\":{\"call_time\":\"10m0s\"},\"v2-standard\":{\"call_time\":\"10m0s\"}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-16\"}]}]}\n",
      "[INFO] 2024-06-17 01:14:19,990 wml_resource.py:93: Successfully finished Get next details for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2024-05-10&space_id=c6dd344f-1aab-4b5e-8edb-2a21fa1e0de0&limit=200'\n",
      "[DEBUG] 2024-06-17 01:14:19,990 wml_resource.py:98: Response(GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2024-05-10&space_id=c6dd344f-1aab-4b5e-8edb-2a21fa1e0de0&limit=200): {\"total_count\":25,\"limit\":200,\"first\":{\"href\":\"https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2024-05-10&space_id=c6dd344f-1aab-4b5e-8edb-2a21fa1e0de0&limit=200\"},\"resources\":[{\"model_id\":\"baai/bge-large-en-v1\",\"label\":\"bge-large-en-v1\",\"provider\":\"baai\",\"source\":\"baai\",\"functions\":[{\"id\":\"embedding\"}],\"short_description\":\"An embedding model with version 1.5. It has 335 million parameters and an embedding dimension of 1024.\",\"long_description\":\"This model has multi-functionality like dense retrieval, sparse retrieval, multi-vector, Multi-Linguality, and Multi-Granularity(8192 tokens)\",\"input_tier\":\"class_c1\",\"output_tier\":\"class_c1\",\"number_params\":\"335m\",\"limits\":{\"lite\":{\"call_time\":\"5m0s\"},\"v2-professional\":{\"call_time\":\"10m0s\"},\"v2-standard\":{\"call_time\":\"10m0s\"}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-16\"}]},{\"model_id\":\"bigscience/mt0-xxl\",\"label\":\"mt0-xxl-13b\",\"provider\":\"BigScience\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"An instruction-tuned iteration on mT5.\",\"long_description\":\"mt0-xxl (13B) is an instruction-tuned iteration on mT5. Like BLOOMZ, it was fine-tuned on a cross-lingual task mixture dataset (xP3) using multitask prompted finetuning (MTF).\",\"input_tier\":\"class_2\",\"output_tier\":\"class_2\",\"number_params\":\"13b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":3}},{\"id\":\"summarization\",\"ratings\":{\"quality\":3}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":2}},{\"id\":\"classification\",\"ratings\":{\"quality\":3}},{\"id\":\"generation\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":2}}],\"model_limits\":{\"max_sequence_length\":4096},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":700},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-07-07\"}]},{\"model_id\":\"codellama/codellama-34b-instruct-hf\",\"label\":\"codellama-34b-instruct-hf\",\"provider\":\"Code Llama\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Code Llama is an AI model built on top of Llama 2, fine-tuned for generating and discussing code.\",\"long_description\":\"Code Llama is a pretrained and fine-tuned generative text models with 34 billion parameters. This model is designed for general code synthesis and understanding.\",\"input_tier\":\"class_2\",\"output_tier\":\"class_2\",\"number_params\":\"34b\",\"min_shot_size\":0,\"task_ids\":[\"code\"],\"tasks\":[{\"id\":\"code\"}],\"model_limits\":{\"max_sequence_length\":16384},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":8192},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8192},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8192}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-03-14\"}]},{\"model_id\":\"google/flan-t5-xl\",\"label\":\"flan-t5-xl-3b\",\"provider\":\"Google\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"prompt_tune_inferable\"},{\"id\":\"prompt_tune_trainable\"},{\"id\":\"text_generation\"}],\"short_description\":\"A pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format.\",\"long_description\":\"flan-t5-xl (3B) is a 3 billion parameter model based on the Flan-T5 family. It is a pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format, and fine-tuned on the Fine-tuned Language Net (FLAN) with instructions for better zero-shot and few-shot performance.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"3b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\",\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"retrieval_augmented_generation\"},{\"id\":\"classification\",\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"generation\",\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":4096,\"training_data_max_records\":10000},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4095},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-12-07\"}],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"random\"},\"init_text\":{\"default\":\"text\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":20,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"Input: {{input}} Output:\"},\"batch_size\":{\"default\":16,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":256},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":128},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":16,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.3,\"min\":0.00001,\"max\":0.5}}},{\"model_id\":\"google/flan-t5-xxl\",\"label\":\"flan-t5-xxl-11b\",\"provider\":\"Google\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"flan-t5-xxl is an 11 billion parameter model based on the Flan-T5 family.\",\"long_description\":\"flan-t5-xxl (11B) is an 11 billion parameter model based on the Flan-T5 family. It is a pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format, and fine-tuned on the Fine-tuned Language Net (FLAN) with instructions for better zero-shot and few-shot performance.\",\"input_tier\":\"class_2\",\"output_tier\":\"class_2\",\"number_params\":\"11b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":3}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":4096},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":700},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-07-07\"}]},{\"model_id\":\"google/flan-ul2\",\"label\":\"flan-ul2-20b\",\"provider\":\"Google\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"flan-ul2 is an encoder decoder model based on the T5 architecture and instruction-tuned using the Fine-tuned Language Net.\",\"long_description\":\"flan-ul2 (20B) is an encoder decoder model based on the T5 architecture and instruction-tuned using the Fine-tuned Language Net (FLAN). Compared to the original UL2 model, flan-ul2 (20B) is more usable for few-shot in-context learning because it was trained with a three times larger receptive field. flan-ul2 (20B) outperforms flan-t5 (11B) by an overall relative improvement of +3.2%.\",\"input_tier\":\"class_3\",\"output_tier\":\"class_3\",\"number_params\":\"20b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":4}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":4096},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":700},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-07-07\"}]},{\"model_id\":\"ibm-mistralai/merlinite-7b\",\"label\":\"merlinite-7b\",\"provider\":\"Mistral AI\",\"tuned_by\":\"IBM\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Merlinite-7b is a Mistral-7b-derivative model trained with the LAB methodology, using Mixtral-8x7b-Instruct as a teacher model.\",\"long_description\":\"This model is made with AutoGPTQ, which mainly leverages the quantization technique to 'compress' the model weights from FP16 to 4-bit INT and performs 'decompression' on-the-fly before computation (in FP16). As a result, the GPU memory, and the data transferring between GPU memory and GPU compute engine, compared to the original FP16 model, is greatly reduced. The major quantization parameters used in the process are listed below.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"7b\",\"min_shot_size\":1,\"task_ids\":[\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":3}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":32768},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":8192},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8192},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8192}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-18\"}]},{\"model_id\":\"ibm-mistralai/mixtral-8x7b-instruct-v01-q\",\"label\":\"mixtral-8x7b-instruct-v01-q\",\"provider\":\"Mistral AI\",\"tuned_by\":\"IBM\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Mixtral-8-7b-instruct-v01-gptq model is made with AutoGPTQ, which mainly leverages the quantization technique to 'compress' the model weights from FP16 to 4-bit INT and performs 'decompression' on-the-fly before computation (in FP16)\",\"long_description\":\"This model is made with AutoGPTQ, which mainly leverages the quantization technique to 'compress' the model weights from FP16 to 4-bit INT and performs 'decompression' on-the-fly before computation (in FP16). As a result, the GPU memory, and the data transferring between GPU memory and GPU compute engine, compared to the original FP16 model, is greatly reduced. The major quantization parameters used in the process are listed below.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"46.7b\",\"min_shot_size\":1,\"task_ids\":[\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":3}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":32768},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-02-15\"},{\"id\":\"constricted\",\"label\":\"deprecated and constricted\",\"start_date\":\"2024-04-19\",\"alternative_model_ids\":[\"ibm-mistralai/mixtral-8x7b-instruct-v01\"]},{\"id\":\"withdrawn\",\"start_date\":\"2024-06-20\",\"alternative_model_ids\":[\"ibm-mistralai/mixtral-8x7b-instruct-v01\"]}]},{\"model_id\":\"ibm/granite-13b-chat-v2\",\"label\":\"granite-13b-chat-v2\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"13b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":3}},{\"id\":\"summarization\",\"ratings\":{\"quality\":2}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":2}},{\"id\":\"classification\",\"ratings\":{\"quality\":3}},{\"id\":\"generation\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":2}}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":8191},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8191},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8191}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-12-01\"}],\"versions\":[{\"version\":\"2.1.0\",\"available_date\":\"2024-02-15\"},{\"version\":\"2.0.0\",\"available_date\":\"2023-12-01\"}]},{\"model_id\":\"ibm/granite-13b-instruct-v2\",\"label\":\"granite-13b-instruct-v2\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"prompt_tune_inferable\"},{\"id\":\"prompt_tune_trainable\"},{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"13b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":3}},{\"id\":\"summarization\",\"ratings\":{\"quality\":2},\"tags\":[\"function_prompt_tune_trainable\"],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"text\"},\"init_text\":{\"default\":\"Please write a summary highlighting the main points of the following text:\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":40,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"Please write a summary highlighting the main points of the following text: {{input}}\"},\"batch_size\":{\"default\":8,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":1024},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":512},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":1,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.0002,\"min\":0.00001,\"max\":0.5}}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":2}},{\"id\":\"classification\",\"ratings\":{\"quality\":3},\"tags\":[\"function_prompt_tune_trainable\"],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"text\"},\"init_text\":{\"default\":\"Classify the text:\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":20,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"Input: {{input}} Output:\"},\"batch_size\":{\"default\":8,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":1024},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":512},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":32,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.0006,\"min\":0.00001,\"max\":0.5}}},{\"id\":\"generation\",\"tags\":[\"function_prompt_tune_trainable\"],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"text\"},\"init_text\":{\"default\":\"text\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":20,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"{{input}}\"},\"batch_size\":{\"default\":16,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":1024},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":512},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":16,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.0002,\"min\":0.00001,\"max\":0.5}}},{\"id\":\"extraction\",\"ratings\":{\"quality\":2}}],\"model_limits\":{\"max_sequence_length\":8192,\"training_data_max_records\":10000},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":8191},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8191},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8191}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-12-01\"}],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"random\"},\"init_text\":{\"default\":\"text\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":20,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"{{input}}\"},\"batch_size\":{\"default\":16,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":1024},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":512},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":16,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.0002,\"min\":0.00001,\"max\":0.5}}},{\"model_id\":\"ibm/granite-20b-code-instruct\",\"label\":\"granite-20b-code-instruct\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"20b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\"},{\"id\":\"classification\"},{\"id\":\"generation\"},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-06\"}]},{\"model_id\":\"ibm/granite-20b-multilingual\",\"label\":\"granite-20b-multilingual\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"20b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":3}},{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":3}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-03-14\"}],\"versions\":[{\"version\":\"1.1.0\",\"available_date\":\"2024-04-18\"},{\"version\":\"1.0.0\",\"available_date\":\"2024-03-14\"}]},{\"model_id\":\"ibm/granite-34b-code-instruct\",\"label\":\"granite-34b-code-instruct\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"34b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\"},{\"id\":\"classification\"},{\"id\":\"generation\"},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":16384},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":8192},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8192},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8192}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-06\"}]},{\"model_id\":\"ibm/granite-3b-code-instruct\",\"label\":\"granite-3b-code-instruct\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"3b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\"},{\"id\":\"classification\"},{\"id\":\"generation\"},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-09\"}]},{\"model_id\":\"ibm/granite-7b-lab\",\"label\":\"granite-7b-lab\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"7b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\"},{\"id\":\"retrieval_augmented_generation\"},{\"id\":\"classification\"},{\"id\":\"generation\"},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":4096},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4095},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-18\"}]},{\"model_id\":\"ibm/granite-8b-code-instruct\",\"label\":\"granite-8b-code-instruct\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"8b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\"},{\"id\":\"classification\"},{\"id\":\"generation\"},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-09\"}]},{\"model_id\":\"ibm/slate-125m-english-rtrvr\",\"label\":\"slate-125m-english-rtrvr\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"embedding\"}],\"short_description\":\"An embedding model. It has 125 million parameters and an embedding dimension of 768.\",\"long_description\":\"This model follows the standard 'sentence transformers' approach, relying on bi-encoders. It generates embeddings for various inputs such as queries, passages, or documents. The training objective is to maximize cosine similarity between two text pieces: text A (query text) and text B (passage text). This process yields sentence embeddings q and p, allowing for comparison through cosine similarity.\",\"input_tier\":\"class_c1\",\"output_tier\":\"class_c1\",\"number_params\":\"125m\",\"limits\":{\"lite\":{\"call_time\":\"5m0s\"},\"v2-professional\":{\"call_time\":\"10m0s\"},\"v2-standard\":{\"call_time\":\"10m0s\"}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-18\"}]},{\"model_id\":\"ibm/slate-30m-english-rtrvr\",\"label\":\"slate-30m-english-rtrvr\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"embedding\"}],\"short_description\":\"An embedding model. It has 30 million parameters and an embedding dimension of 384.\",\"long_description\":\"This model follows the standard 'sentence transformers' approach, relying on bi-encoders. It generates embeddings for various inputs such as queries, passages, or documents. The training objective is to maximize cosine similarity between two text pieces: text A (query text) and text B (passage text). This process yields sentence embeddings q and p, allowing for comparison through cosine similarity.\",\"input_tier\":\"class_c1\",\"output_tier\":\"class_c1\",\"number_params\":\"30m\",\"limits\":{\"lite\":{\"call_time\":\"5m0s\"},\"v2-professional\":{\"call_time\":\"10m0s\"},\"v2-standard\":{\"call_time\":\"10m0s\"}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-18\"}]},{\"model_id\":\"intfloat/multilingual-e5-large\",\"label\":\"multilingual-e5-large\",\"provider\":\"intfloat\",\"source\":\"intfloat\",\"functions\":[{\"id\":\"embedding\"}],\"short_description\":\"An embedding model. It has 560 million parameters, has 24 layers and the embedding size is 1024.\",\"long_description\":\"This model gets continually trained on a mixture of multilingual datasets. It supports 100 languages from xlm-roberta.\",\"input_tier\":\"class_c1\",\"output_tier\":\"class_c1\",\"number_params\":\"560m\",\"limits\":{\"lite\":{\"call_time\":\"5m0s\"},\"v2-professional\":{\"call_time\":\"10m0s\"},\"v2-standard\":{\"call_time\":\"10m0s\"}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-16\"}]},{\"model_id\":\"meta-llama/llama-2-13b-chat\",\"label\":\"llama-2-13b-chat\",\"provider\":\"Meta\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"prompt_tune_inferable\"},{\"id\":\"prompt_tune_trainable\"},{\"id\":\"text_generation\"}],\"short_description\":\"Llama-2-13b-chat is an auto-regressive language model that uses an optimized transformer architecture.\",\"long_description\":\"Llama-2-13b-chat is a pretrained and fine-tuned generative text model with 13 billion parameters, optimized for dialogue use cases.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"13b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":3},\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":4}},{\"id\":\"classification\",\"ratings\":{\"quality\":4},\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"generation\",\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":4096,\"training_data_max_records\":10000},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":2048},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-11-09\"}],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"random\"},\"init_text\":{\"default\":\"text\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":20,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"{{input}}\"},\"batch_size\":{\"default\":8,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":1024},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":512},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":16,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.002,\"min\":0.00001,\"max\":0.5}}},{\"model_id\":\"meta-llama/llama-2-70b-chat\",\"label\":\"llama-2-70b-chat\",\"provider\":\"Meta\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Llama-2-70b-chat is an auto-regressive language model that uses an optimized transformer architecture.\",\"long_description\":\"Llama-2-70b-chat is a pretrained and fine-tuned generative text model with 70 billion parameters, optimized for dialogue use cases.\",\"input_tier\":\"class_2\",\"output_tier\":\"class_2\",\"number_params\":\"70b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":3}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":4}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":4096},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":900},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-09-07\"}]},{\"model_id\":\"meta-llama/llama-3-70b-instruct\",\"label\":\"llama-3-70b-instruct\",\"provider\":\"Meta\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Llama-3-70b-instruct is an auto-regressive language model that uses an optimized transformer architecture.\",\"long_description\":\"Llama-3-70b-instruct is a pretrained and fine-tuned generative text model with 70 billion parameters, optimized for dialogue use cases.\",\"input_tier\":\"class_2\",\"output_tier\":\"class_2\",\"number_params\":\"70b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":3}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":4}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-18\"}]},{\"model_id\":\"meta-llama/llama-3-8b-instruct\",\"label\":\"llama-3-8b-instruct\",\"provider\":\"Meta\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Llama-3-8b-instruct is an auto-regressive language model that uses an optimized transformer architecture.\",\"long_description\":\"Llama-3-8b-instruct is a pretrained and fine-tuned generative text model with 8 billion parameters, optimized for dialogue use cases.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"8b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":3}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":4}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-18\"}]},{\"model_id\":\"mistralai/mixtral-8x7b-instruct-v01\",\"label\":\"mixtral-8x7b-instruct-v01\",\"provider\":\"Mistral AI\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts.\",\"long_description\":\"This model is made with AutoGPTQ, which mainly leverages the quantization technique to 'compress' the model weights from FP16 to 4-bit INT and performs 'decompression' on-the-fly before computation (in FP16). As a result, the GPU memory, and the data transferring between GPU memory and GPU compute engine, compared to the original FP16 model, is greatly reduced. The major quantization parameters used in the process are listed below.\",\"input_tier\":\"class_1\",\"output_tier\":\"class_1\",\"number_params\":\"46.7b\",\"min_shot_size\":1,\"task_ids\":[\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":3}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":32768},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":16384},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":16384},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":16384}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-17\"}]},{\"model_id\":\"sentence-transformers/all-minilm-l12-v2\",\"label\":\"all-minilm-l12-v2\",\"provider\":\"sentence-transformers\",\"source\":\"sentence-transformers\",\"functions\":[{\"id\":\"embedding\"}],\"short_description\":\"An embedding model with 128 token limit. It has 33.4 million parameters and an embedding dimension of 384.\",\"long_description\":\"This model follows sentence transformers approach, it maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\",\"input_tier\":\"class_c1\",\"output_tier\":\"class_c1\",\"number_params\":\"33.4m\",\"limits\":{\"lite\":{\"call_time\":\"5m0s\"},\"v2-professional\":{\"call_time\":\"10m0s\"},\"v2-standard\":{\"call_time\":\"10m0s\"}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-16\"}]}]}\n"
     ]
    }
   ],
   "source": [
    "TRACE_NAME = \"elementary-my-dear-watson\"\n",
    "\n",
    "tracer: LastMileTracer = get_lastmile_tracer(\n",
    "    TRACE_NAME, os.getenv(\"LASTMILE_API_TOKEN\")\n",
    ")\n",
    "watson_model: Model = init_watson_model()\n",
    "wrapper = wrap_watson(watson_model, tracer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_generation(mode: Mode, prompt: str) -> None:\n",
    "    logger.info(\"IBM WatsonX Generation Script Starting...\")\n",
    "\n",
    "    # n.b. required for multiple api keys, make sure `.env` is in your local path\n",
    "    dotenv.load_dotenv()\n",
    "    \n",
    "    response: str = \"\"\n",
    "    if Mode.GENERATE == mode:\n",
    "        logger.info(f\"running with mode: {mode}\")\n",
    "        response = wrapper.generate(prompt)\n",
    "    elif Mode.GENERATE_TEXT == mode:\n",
    "        logger.info(f\"running with mode: {mode}\")\n",
    "        response = wrapper.generate_text(prompt)\n",
    "    else:\n",
    "        logger.error(f\"unsupported mode: {mode}\")\n",
    "    logger.info(f\"response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2024-06-17 01:14:20,000 1619447352.py:2: IBM WatsonX Generation Script Starting...\n",
      "[INFO] 2024-06-17 01:14:20,001 1619447352.py:9: running with mode: Mode.GENERATE\n",
      "[INFO] 2024-06-17 01:14:21,687 wml_resource.py:93: Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2024-05-10'\n",
      "[DEBUG] 2024-06-17 01:14:21,687 wml_resource.py:98: Response(POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2024-05-10): {\"model_id\":\"ibm/granite-13b-chat-v2\",\"model_version\":\"2.1.0\",\"created_at\":\"2024-06-17T05:14:21.568Z\",\"results\":[{\"generated_text\":\"\\n\\nCertainly! One of Edison's most famous inventions was the phonograph, the world's first practical device for recording and\",\"generated_token_count\":25,\"input_token_count\":13,\"stop_reason\":\"max_tokens\"}]}\n",
      "[INFO] 2024-06-17 01:14:21,964 1619447352.py:16: response: {'model_id': 'ibm/granite-13b-chat-v2', 'model_version': '2.1.0', 'created_at': '2024-06-17T05:14:21.568Z', 'results': [{'generated_text': \"\\n\\nCertainly! One of Edison's most famous inventions was the phonograph, the world's first practical device for recording and\", 'generated_token_count': 25, 'input_token_count': 13, 'stop_reason': 'max_tokens'}]}\n"
     ]
    }
   ],
   "source": [
    "run_generation(\n",
    "    mode=Mode.GENERATE,\n",
    "    prompt=\"Can you please tell me an amusing anecdote about Thomas Edison?\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
