{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n",
                "# Use watsonx Granite Model Series, Chroma, and LangChain to answer questions (RAG)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "#### Disclaimers\n",
                "\n",
                "- Use only Projects and Spaces that are available in watsonx context.\n",
                "\n",
                "## Notebook content\n",
                "This notebook contains the steps and code to demonstrate support of Retrieval Augumented Generation in watsonx.ai. It introduces commands for data retrieval, knowledge base building & querying, and model testing.\n",
                "\n",
                "Some familiarity with Python is helpful. This notebook uses Python 3.10.\n",
                "\n",
                "### About Retrieval Augmented Generation\n",
                "Retrieval Augmented Generation (RAG) is a versatile pattern that can unlock a number of use cases requiring factual recall of information, such as querying a knowledge base in natural language.\n",
                "\n",
                "In its simplest form, RAG requires 3 steps:\n",
                "\n",
                "- Index knowledge base passages (once)\n",
                "- Retrieve relevant passage(s) from knowledge base (for every user query)\n",
                "- Generate a response by feeding retrieved passage into a large language model (for every user query)\n",
                "\n",
                "## Contents\n",
                "\n",
                "This notebook contains the following parts:\n",
                "\n",
                "- [Setup](#setup)\n",
                "- [Document data loading](#data)\n",
                "- [Build up knowledge base](#build_base)\n",
                "- [Foundation Models on watsonx](#models)\n",
                "- [Generate a retrieval-augmented response to a question](#predict)\n",
                "- [Summary and next steps](#summary)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "<a id=\"setup\"></a>\n",
                "##  Set up the environment\n",
                "\n",
                "Before you use the sample code in this notebook, you must perform the following setup tasks:\n",
                "\n",
                "-  Create a <a href=\"https://cloud.ibm.com/catalog/services/watson-machine-learning\" target=\"_blank\" rel=\"noopener no referrer\">Watson Machine Learning (WML) Service</a> instance (a free plan is offered and information about how to create the instance can be found <a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/wml-plans.html?context=wx&audience=wdp\" target=\"_blank\" rel=\"noopener no referrer\">here</a>).\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "### Install and import the dependecies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "# !pip install \"langchain==0.1.10\" | tail -n 1\n",
                "# !pip install \"ibm-watsonx-ai>=0.2.6\" | tail -n 1\n",
                "# !pip install -U langchain_ibm | tail -n 1\n",
                "# !pip install wget | tail -n 1\n",
                "# !pip install sentence-transformers | tail -n 1\n",
                "# !pip install \"chromadb\" --upgrade | tail -n 1\n",
                "# !pip install \"sqlalchemy==2.0.1\" | tail -n 1\n",
                "# !pip install \"lastmile-eval>=0.0.45\"\n",
                "# !pip install \"lastmile-eval[ui]\"\n",
                "# !pip install \"tracing-auto-instrumentation[langchain]\" --upgrade"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "### watsonx API connection\n",
                "This cell defines the credentials required to work with watsonx API for Foundation\n",
                "Model inferencing.\n",
                "\n",
                "**Action:** Provide the IBM Cloud user API key. For details, see <a href=\"https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui\" target=\"_blank\" rel=\"noopener no referrer\">documentation</a>.\n",
                "\n",
                "For Jupyter notebook, save these in an `.env` file within this project directory. For Google Colab, use Secret Manager to set these (key icon on the left)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "try:\n",
                "    # First try this in case we're running on Google Collab\n",
                "    from google.colab import userdata\n",
                "    os.environ['WATSONX_API_KEY'] =  userdata.get('WATSONX_API_KEY')\n",
                "    os.environ['PROJECT_ID'] =  userdata.get('PROJECT_ID')\n",
                "    os.environ['SPACE_ID'] =  userdata.get('SPACE_ID')\n",
                "except ModuleNotFoundError:\n",
                "    import dotenv\n",
                "    dotenv.load_dotenv()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "credentials = {\n",
                "    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
                "    \"apikey\": os.getenv(\"WATSONX_API_KEY\")\n",
                "}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "### Defining the project id\n",
                "The API requires project id that provides the context for the call. We will obtain the id from the project in which this notebook runs. Otherwise, please provide the project id.\n",
                "\n",
                "**Hint**: You can find the `project_id` as follows. Open the prompt lab in watsonx.ai. At the very top of the UI, there will be `Projects / <project name> /`. Click on the `<project name>` link. Then get the `project_id` from Project's Manage tab (Project -> Manage -> General -> Details).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "try:\n",
                "    project_id = os.environ[\"PROJECT_ID\"]\n",
                "except KeyError:\n",
                "    project_id = input(\"Please enter your project_id (hit enter): \")\n",
                "\n",
                "try:\n",
                "    space_id = os.environ[\"SPACE_ID\"]\n",
                "except KeyError:\n",
                "    space_id = input(\"Please enter your space id if you have one (hit enter): \") or None"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "<a id=\"data\"></a>\n",
                "## Document data loading\n",
                "\n",
                "Download the file with State of the Union."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "import wget\n",
                "\n",
                "!mkdir -p 'data'\n",
                "filename = 'data/state_of_the_union.txt'\n",
                "url = 'https://raw.github.com/IBM/watson-machine-learning-samples/master/cloud/data/foundation_models/state_of_the_union.txt'\n",
                "\n",
                "if not os.path.isfile(filename):\n",
                "    wget.download(url, out=filename)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LastMile Auto Instrumentation Setup\n",
                "\n",
                "An instance of `LangChainInstrumentor` is created with a project name. The `instrument()` method is called to instrument the code for tracing and monitoring."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tracing_auto_instrumentation.langchain import LangChainInstrumentor\n",
                "\n",
                "# Create an instance of LangChainInstrumentor and instrument the code\n",
                "PROJECT_NAME = \"ibm x lastmile\"\n",
                "instrumentor = LangChainInstrumentor(project_name=PROJECT_NAME)\n",
                "instrumentor.instrument()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "<a id=\"build_base\"></a>\n",
                "## Build up knowledge base\n",
                "\n",
                "The most common approach in RAG is to create dense vector representations of the knowledge base in order to calculate the semantic similarity to a given user query.\n",
                "\n",
                "In this basic example, we take the State of the Union speech content (filename), split it into chunks, embed it using an open-source embedding model, load it into <a href=\"https://www.trychroma.com/\" target=\"_blank\" rel=\"noopener no referrer\">Chroma</a>, and then query it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "from langchain.document_loaders import TextLoader\n",
                "from langchain.text_splitter import CharacterTextSplitter\n",
                "from langchain.vectorstores import Chroma\n",
                "\n",
                "loader = TextLoader(filename)\n",
                "documents = loader.load()\n",
                "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
                "texts = text_splitter.split_documents(documents)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "The dataset we are using is already split into self-contained passages that can be ingested by Chroma."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "### Create an embedding function\n",
                "\n",
                "Note that you can feed a custom embedding function to be used by chromadb. The performance of Chroma db may differ depending on the embedding model used. In following example we use watsonx.ai Embedding service. We can check available embedding models using `get_embedding_model_specs`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from ibm_watsonx_ai.foundation_models.utils import get_embedding_model_specs\n",
                "\n",
                "get_embedding_model_specs(credentials.get('url'))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_ibm import WatsonxEmbeddings\n",
                "from ibm_watsonx_ai.foundation_models.utils.enums import EmbeddingTypes\n",
                "\n",
                "embeddings = WatsonxEmbeddings(\n",
                "    model_id=EmbeddingTypes.IBM_SLATE_30M_ENG.value,\n",
                "    url=credentials[\"url\"],\n",
                "    apikey=credentials[\"apikey\"],\n",
                "    project_id=project_id,\n",
                "    space_id=space_id\n",
                ")\n",
                "\n",
                "docsearch = Chroma.from_documents(texts, embeddings)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Compatibility watsonx.ai Embeddings with LangChain\n",
                "\n",
                " LangChain retrievals use `embed_documents` and `embed_query` under the hood to generate embedding vectors for uploaded documents and user query respectively."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "help(WatsonxEmbeddings)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "<a id=\"models\"></a>\n",
                "## Foundation Models on `watsonx.ai`"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "IBM watsonx foundation models are among the <a href=\"https://python.langchain.com/docs/integrations/llms/watsonxllm\" target=\"_blank\" rel=\"noopener no referrer\">list of LLM models supported by Langchain</a>. This example shows how to communicate with <a href=\"https://newsroom.ibm.com/2023-09-28-IBM-Announces-Availability-of-watsonx-Granite-Model-Series,-Client-Protections-for-IBM-watsonx-Models\" target=\"_blank\" rel=\"noopener no referrer\">Granite Model Series</a> using <a href=\"https://python.langchain.com/docs/get_started/introduction\" target=\"_blank\" rel=\"noopener no referrer\">Langchain</a>."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "### Defining model\n",
                "You need to specify `model_id` that will be used for inferencing:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
                "\n",
                "model_id = ModelTypes.GRANITE_13B_CHAT_V2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "### Defining the model parameters\n",
                "We need to provide a set of model parameters that will influence the result:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
                "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
                "\n",
                "parameters = {\n",
                "    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n",
                "    GenParams.MIN_NEW_TOKENS: 1,\n",
                "    GenParams.MAX_NEW_TOKENS: 100,\n",
                "    GenParams.STOP_SEQUENCES: [\"<|endoftext|>\"]\n",
                "}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### LangChain CustomLLM wrapper for watsonx model\n",
                "Initialize the `WatsonxLLM` class from Langchain with defined parameters and `ibm/granite-13b-chat-v2`. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "from langchain_ibm import WatsonxLLM\n",
                "\n",
                "watsonx_granite = WatsonxLLM(\n",
                "    model_id=model_id.value,\n",
                "    url=credentials.get(\"url\"),\n",
                "    apikey=credentials.get(\"apikey\"),\n",
                "    project_id=project_id,\n",
                "    params=parameters\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "<a id=\"predict\"></a>\n",
                "## Generate a retrieval-augmented response to a question"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Build the `RetrievalQA` (question answering chain) to automate the RAG task."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.chains import RetrievalQA\n",
                "\n",
                "qa = RetrievalQA.from_chain_type(llm=watsonx_granite, chain_type=\"stuff\", retriever=docsearch.as_retriever())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "### Sample QA chain\n",
                "\n",
                "Get questions from the previously loaded test dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%%\n"
                }
            },
            "outputs": [],
            "source": [
                "query = \"What is the goal of the Cancer Moonshot initiative mentioned in the speech?\"\n",
                "qa.invoke(query)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Evaluations\n",
                "\n",
                "This section demonstrates how to evaluate the outputs of the question-answering model using the `lastmile_eval` package.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from lastmile_eval.rag.debugger.api.evaluation import (\n",
                "    create_input_set,\n",
                "    run_and_evaluate,\n",
                "    SaveOptions,\n",
                ")\n",
                "\n",
                "# Define inputs and ground truth for the evaluation\n",
                "inputs = [\"What is the goal of the Cancer Moonshot initiative mentioned in the speech?\", # 1\n",
                "          \"What did President Zelenskyy say in his speech to the European Parliament?\", # 2\n",
                "          'What statement did the President make regarding the sanctions on Russia?',\n",
                "          \"What did the President say about the future exploration of Mars?\"]  # 4 (not in data)\n",
                "\n",
                "ground_truths = [\"The Cancer Moonshot initiative aims to cut the cancer death rate by at least 50% over the next 25 years, turn more cancers into treatable diseases, and provide more support for patients and families.\", # 1\n",
                "                 'In his speech to the European Parliament, President Zelenskyy said, \"Light will win over darkness.\"', # 2\n",
                "                 'We are cutting off Russia’s largest banks from the international financial system.  Preventing Russia’s central bank from defending the Russian Ruble making Putin’s $630 Billion “war fund” worthless.', #3\n",
                "                 \"The President did not address the future exploration of Mars in the speech.\"] # 4\n",
                "\n",
                "evaluator_names = {\"qa\", \"similarity\", \"relevance\"}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run and evaluate outputs using the created input set\n",
                "\n",
                "# Wrap the qa.invoke function to return the final value as the output\n",
                "def run_qa_and_return_final_value(query) -> str:\n",
                "    return qa.invoke(query)[\"result\"]\n",
                "\n",
                "save_options = SaveOptions(\n",
                "    example_set_name=\"State of the Union Example Cases\", \n",
                "    evaluation_result_name=\"State of the Union Evaluation Results\"\n",
                ")\n",
                "\n",
                "create_input_set_separately = False\n",
                "\n",
                "if create_input_set_separately:\n",
                "    # This will create an input set, then evaluation set in two separate steps\n",
                "    # The main difference is that when you create the input (test) set, you aren't \n",
                "    # running the `run_query_fn` function, so you won't have the trace data associated\n",
                "    # with your inputs\n",
                "    input_set_id = create_input_set(\n",
                "        project_name=PROJECT_NAME,\n",
                "        queries=inputs, \n",
                "        input_set_name=\"State of the Union Test Cases\",\n",
                "        ground_truths=ground_truths,\n",
                "    ).id\n",
                "    run_and_evaluate(\n",
                "        project_name=PROJECT_NAME,\n",
                "        run_query_fn=run_qa_and_return_final_value, # type: ignore\n",
                "        input_set_id=input_set_id, \n",
                "        evaluators=evaluator_names,\n",
                "        save_options=save_options,\n",
                "    )\n",
                "else:\n",
                "    # An input set hasn't been created yet, so the trace data will be\n",
                "    # created alongside with it\n",
                "    run_and_evaluate(\n",
                "        project_name=PROJECT_NAME,\n",
                "        run_query_fn=run_qa_and_return_final_value, # type: ignore\n",
                "        inputs=inputs,\n",
                "        ground_truths=ground_truths, \n",
                "        evaluators=evaluator_names,\n",
                "        save_options=save_options,\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Launch The Rag Debugger UI\n",
                "\n",
                "<img width=\"1915\" alt=\"Screenshot 2024-05-21 at 11 30 01 PM\" src=\"https://github.com/lastmile-ai/aiconfig/assets/141073967/2f2a72c9-fb07-402b-bc6f-fdc46f529edd\"> \n",
                "\n",
                "<br><br>\n",
                "\n",
                "<img width=\"1917\" alt=\"Screenshot 2024-05-21 at 11 30 14 PM\" src=\"https://github.com/lastmile-ai/aiconfig/assets/141073967/4a98b9b3-b96e-430e-9156-79560cbea1be\">"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Launch the rag-debugger\n",
                "# !rag-debug launch"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "collapsed": false,
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "<a id=\"summary\"></a>\n",
                "## Summary and next steps\n",
                "\n",
                " You successfully completed this notebook!.\n",
                " \n",
                " You learned how to answer question using RAG using watsonx and LangChain.\n",
                " \n",
                "Check out our _<a href=\"https://ibm.github.io/watsonx-ai-python-sdk/samples.html\" target=\"_blank\" rel=\"noopener no referrer\">Online Documentation</a>_ for more samples, tutorials, documentation, how-tos, and blog posts. "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "pycharm": {
                    "name": "#%% md\n"
                }
            },
            "source": [
                "Copyright © 2023, 2024 IBM. This notebook and its source code are released under the terms of the MIT License."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "BIG-bench",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
