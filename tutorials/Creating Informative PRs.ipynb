{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Leveraging AI for Accurate PRs: A Guide to Using OpenAI for Title and Description Generation, and Evaluating with LastMile Eval\n",
                "\n",
                "In this notebook, we will explore how to leverage OpenAI's language model to generate accurate and descriptive titles and descriptions for pull requests (PRs). We will then use the LastMile Eval library to evaluate the quality of the generated content.\n",
                "\n",
                "## Prerequisites\n",
                "\n",
                "Before we begin, make sure you have the following libraries installed:\n",
                "\n",
                "- `requests`: Used for making HTTP requests to the GitHub API.\n",
                "- `openai`: The OpenAI library for interacting with the OpenAI API.\n",
                "- `lastmile-eval`: The LastMile Eval library for evaluating the generated content.\n",
                "\n",
                "You can install these libraries using the following commands:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# !pip install requests\n",
                "# !pip install openai\n",
                "# !pip install lastmile-eval"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Fetching Pull Request Diffs\n",
                "\n",
                "We start by defining a function `get_pull_request_diff` that takes a pull request link and fetches the diff of the pull request using the GitHub API."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "\n",
                "# List of merged pull request URLs\n",
                "merged_prs = [\n",
                "    \"https://github.com/keras-team/keras/pull/19720\",\n",
                "    \"https://github.com/keras-team/keras/pull/19728\",\n",
                "    \"https://github.com/keras-team/keras/pull/19729\"\n",
                "]\n",
                "\n",
                "def get_pull_request_diff(pr_link: str) -> str:\n",
                "    \"\"\"Fetches the diff of a pull request using the GitHub API.\"\"\"\n",
                "    diff_suffix = \".diff\"\n",
                "    diff_url = f'{pr_link}{diff_suffix}'\n",
                "\n",
                "    response = requests.get(diff_url)\n",
                "    return response.text"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's take a look at the diff of the first PR:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "diff --git a/keras/src/export/export_lib.py b/keras/src/export/export_lib.py\n",
                        "index 1157630da0e..e3749c2b33c 100644\n",
                        "--- a/keras/src/export/export_lib.py\n",
                        "+++ b/keras/src/export/export_lib.py\n",
                        "@@ -621,18 +621,17 @@ def export_model(model, filepath):\n",
                        "             input_signature = [input_signature]\n",
                        "         export_archive.add_endpoint(\"serve\", model.__call__, input_signature)\n",
                        "     else:\n",
                        "-        save_spec = _get_save_spec(model)\n",
                        "-        if not save_spec or not model._called:\n",
                        "+        input_signature = _get_input_signature(model)\n",
                        "+        if not input_signature or not model._called:\n",
                        "             raise ValueError(\n",
                        "                 \"The model provided has never called. \"\n",
                        "                 \"It must be called at least once before export.\"\n",
                        "             )\n",
                        "-        input_signature = [save_spec]\n",
                        "         export_archive.add_endpoint(\"serve\", model.__call__, input_signature)\n",
                        "     export_archive.write_out(filepath)\n",
                        " \n",
                        " \n",
                        "-def _get_save_spec(model):\n",
                        "+def _get_input_signature(model):\n",
                        "     shapes_dict = getattr(model, \"_build_shapes_dict\", None)\n",
                        "     if not shapes_dict:\n",
                        "         return None\n",
                        "@@ -654,16 +653,7 @@ def make_tensor_spec(structure):\n",
                        "                 f\"Unsupported type {type(structure)} for {structure}\"\n",
                        "             )\n",
                        " \n",
                        "-    if len(shapes_dict) == 1:\n",
                        "-        value = list(shapes_dict.values())[0]\n",
                        "-        return make_tensor_spec(value)\n",
                        "-\n",
                        "-    specs = {}\n",
                        "-    for key, value in shapes_dict.items():\n",
                        "-        key = key.rstrip(\"_shape\")\n",
                        "-        specs[key] = make_tensor_spec(value)\n",
                        "-\n",
                        "-    return specs\n",
                        "+    return [make_tensor_spec(value) for value in shapes_dict.values()]\n",
                        " \n",
                        " \n",
                        " @keras_export(\"keras.layers.TFSMLayer\")\n",
                        "diff --git a/keras/src/export/export_lib_test.py b/keras/src/export/export_lib_test.py\n",
                        "index f754525f383..210eb99a9d2 100644\n",
                        "--- a/keras/src/export/export_lib_test.py\n",
                        "+++ b/keras/src/export/export_lib_test.py\n",
                        "@@ -145,6 +145,31 @@ def call(self, inputs):\n",
                        "         )\n",
                        "         revived_model.serve(bigger_input)\n",
                        " \n",
                        "+    def test_model_with_multiple_inputs(self):\n",
                        "+\n",
                        "+        class TwoInputsModel(models.Model):\n",
                        "+            def call(self, x, y):\n",
                        "+                return x + y\n",
                        "+\n",
                        "+            def build(self, y_shape, x_shape):\n",
                        "+                self.built = True\n",
                        "+\n",
                        "+        temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n",
                        "+        model = TwoInputsModel()\n",
                        "+        ref_input_x = tf.random.normal((3, 10))\n",
                        "+        ref_input_y = tf.random.normal((3, 10))\n",
                        "+        ref_output = model(ref_input_x, ref_input_y)\n",
                        "+\n",
                        "+        export_lib.export_model(model, temp_filepath)\n",
                        "+        revived_model = tf.saved_model.load(temp_filepath)\n",
                        "+        self.assertAllClose(\n",
                        "+            ref_output, revived_model.serve(ref_input_x, ref_input_y)\n",
                        "+        )\n",
                        "+        # Test with a different batch size\n",
                        "+        revived_model.serve(\n",
                        "+            tf.random.normal((6, 10)), tf.random.normal((6, 10))\n",
                        "+        )\n",
                        "+\n",
                        "     @parameterized.named_parameters(\n",
                        "         named_product(model_type=[\"sequential\", \"functional\", \"subclass\"])\n",
                        "     )\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "pr_diff = get_pull_request_diff(merged_prs[0])\n",
                "print(pr_diff)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Generating Pull Request Description\n",
                "\n",
                "Next, we use OpenAI's language model to generate a description for the pull request based on the diff. Make sure to set your OpenAI API key in the environment variable `OPENAI_API_KEY`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "- Updated `export_lib.py` to use `_get_input_signature` instead of `_get_save_spec`.\n",
                        "- Introduced `_get_input_signature` to handle models with multiple inputs by returning a list of input signatures.\n",
                        "- Added a new test in `export_lib_test.py` to validate the model with multiple inputs.\n",
                        "- Successfully tested the model with different batch sizes to ensure correctness.\n"
                    ]
                }
            ],
            "source": [
                "import openai\n",
                "\n",
                "client = openai.OpenAI()\n",
                "\n",
                "# System prompt template for generating PR description\n",
                "system_prompt_template = (\n",
                "    \"You are a developer who writes amazing code. \"\n",
                "    \"You are working on a project and you need to generate a pull request description for a pull request diff. \"\n",
                "    \"When given a diff, generate a description for the pull request. Say only the description with formatting\"\n",
                ")\n",
                "\n",
                "response = client.chat.completions.create(\n",
                "    model=\"gpt-3.5-turbo\",\n",
                "    messages=[\n",
                "        {\"role\": \"system\", \"content\": system_prompt_template},\n",
                "        {\"role\": \"user\", \"content\": pr_diff}\n",
                "    ]\n",
                ")\n",
                "\n",
                "generated_pr_description = response.choices[0].message.content \n",
                "print(generated_pr_description)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Generating Pull Request Title\n",
                "\n",
                "We can also generate a concise and descriptive title for the pull request based on the generated description."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Refactor export_lib to use _get_input_signature for multiple inputs\n"
                    ]
                }
            ],
            "source": [
                "# System prompt template for generating PR title\n",
                "system_prompt_template = (\n",
                "    \"You are a developer who writes amazing code. \"\n",
                "    \"You are working on a project and you need to generate a pull request title from a pull request description. \"\n",
                "    \"When given a diff, generate a title for the pull request. Say only the title\"\n",
                ")\n",
                "\n",
                "response = client.chat.completions.create(\n",
                "    model=\"gpt-3.5-turbo\",\n",
                "    messages=[\n",
                "        {\"role\": \"system\", \"content\": system_prompt_template},\n",
                "        {\"role\": \"user\", \"content\": generated_pr_description}\n",
                "    ]\n",
                ")\n",
                "\n",
                "generated_pr_title = response.choices[0].message.content \n",
                "print(generated_pr_title)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Evaluating Generated Content\n",
                "\n",
                "Finally, we use the LastMile Eval library to evaluate the quality of the generated description and title. The `calculate_summarization_score` function asks GPT-3.5 to generate a list of float scores indicating the summary quality of each input-reference pair, where 1.0 denotes 'good' and 0.0 denotes otherwise."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "🐌!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "fb3c3fe755a543a5a94804ea123ac061",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "llm_classify |          | 0/1 (0.0%) | ⏳ 00:00<? | ?it/s"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "🐌!! If running llm_classify inside a notebook, patching the event loop with nest_asyncio will allow asynchronous eval submission, and is significantly faster. To patch the event loop, run `nest_asyncio.apply()`.\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "4b9393ef090c488c8680da50304b3c21",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "llm_classify |          | 0/1 (0.0%) | ⏳ 00:00<? | ?it/s"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Description score: [1.0]\n",
                        "Title score: [1.0]\n"
                    ]
                }
            ],
            "source": [
                "from lastmile_eval.text import calculate_summarization_score\n",
                "\n",
                "description_score = calculate_summarization_score(\n",
                "    [generated_pr_description],\n",
                "    [pr_diff],\n",
                "    model_name=\"gpt-3.5-turbo\"\n",
                ")\n",
                "\n",
                "title_score = (calculate_summarization_score\n",
                "    [generated_pr_title],\n",
                "    [generated_pr_description],\n",
                "    model_name=\"gpt-3.5-turbo\"\n",
                ")\n",
                "\n",
                "print(f\"Description score: {description_score}\")\n",
                "print(f\"Title score: {title_score}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The scores indicate that both the generated description and title are of good quality and accurately summarize the pull request diff.\n",
                "\n",
                "That's it! You now have a notebook that demonstrates how to generate accurate and descriptive pull request titles and descriptions using OpenAI, and evaluate their quality using LastMile Eval."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Creating Test Sets and Evaluation runs with lastmile-eval"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "from lastmile_eval.rag.debugger.api.evaluation import create_input_set\n",
                "from lastmile_eval.rag.debugger.api.evaluation import run_and_store_evaluations\n",
                "from lastmile_eval.text import calculate_summarization_score\n",
                "\n",
                "\n",
                "test_set_id = create_input_set( [generated_pr_description, generated_pr_title], \"pr_generator\", [pr_diff, generated_pr_description]).ids[0]\n",
                "\n",
                "def wrap_summarize(df: pd.DataFrame) -> list[float]:\n",
                "    def helper(row) -> float:\n",
                "        return calculate_summarization_score(\n",
                "            [row[\"query\"]],\n",
                "            [row[\"groundTruth\"]],\n",
                "            model_name=\"gpt-3.5-turbo\"\n",
                "        )[0]\n",
                "    \n",
                "    return df.apply(helper, axis=1)\n",
                "\n",
                "run_and_store_evaluations(test_set_id, project_id = None, trace_level_evaluators={\"summarize\": wrap_summarize}, dataset_level_evaluators = {}, evaluation_set_name=\"summarization\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Open The Debugger UI to see the generated evaluation scores.\n",
                "\n",
                "<img width=\"959\" alt=\"Screenshot 2024-05-20 at 5 34 01 PM\" src=\"https://github-production-user-asset-6210df.s3.amazonaws.com/141073967/332206801-b5fdadc0-88e5-4602-b84f-e1997283d81f.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240520%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240520T213720Z&X-Amz-Expires=300&X-Amz-Signature=bd2acd3267b56463c279d159c2784a27b176e2386760c1ae86476b4e5a16e842&X-Amz-SignedHeaders=host&actor_id=141073967&key_id=0&repo_id=768880246\">\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!rag-debug launch"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "empty2",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
