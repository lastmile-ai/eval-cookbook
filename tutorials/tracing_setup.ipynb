{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZLoBuonVDCS"
      },
      "source": [
        "## Step 1: Import modules and packages, download reference docs\n",
        "\n",
        "This goes through entire LastMile AI Eval flow from\n",
        "\n",
        "1. create ingestion trace\n",
        "2. generate a query + ground truth context pair per each node context in a document\n",
        "  - taking those queries and running rag query traces to get actual retrieved context\n",
        "3. listing query traces I want to include in a test set (defaults to last N queries for now)\n",
        "4. create test Set with given query_traces, as well as storing the ground truth for the associated context for each query\n",
        "5. create evaluation metrics based on ones provided by Llama Index\n",
        "   - note: this is mainly from Llama Index, so the evaluation metrics are only focused on retrieval, nothing on outputs (though I store those as output events too)\n",
        "6. create evaluation set by feeding these metrics with test set we just created\n",
        "\n",
        "Some notes:\n",
        "- no manual id grepping needed --> all taken care of by helper functions\n",
        "probably needs to be better designed in future, just was focused on getting unblocked\n",
        "- need to refactor ingestion_trace_id to map to trace-level, not marking rag query event level (right now it doesn't work, I'll add that later)\n",
        "- some other small API convenience functions need to be added to the API, such as a helper function for `list_evaluation_sets()`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeuZV8CZVDCT",
        "outputId": "f8c5de16-89a1-4ddd-f21d-32c3a1240207"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "# IMPORTANT: After running this cell, you MUST\n",
        "# restart kernel for these changes to take effect\n",
        "\n",
        "# !pip list | grep lastmile\n",
        "\n",
        "# !pip3 install lastmile-eval #--upgrade --force-reinstall\n",
        "\n",
        "!pwd\n",
        "\n",
        "# Hacky way to locally install the lastmile-eval package lol\n",
        "!pip3 install -e ../../../../..\n",
        "\n",
        "!pip3 install llama-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecMX3MSpeceh",
        "outputId": "cef93c71-037e-4532-f471-d315ed320d4f"
      },
      "outputs": [],
      "source": [
        "!pip list | grep lastmile-eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WqZUBdb4VDCU"
      },
      "outputs": [],
      "source": [
        "# The nest_asyncio module enables the nesting of asynchronous functions within an already running async loop.\n",
        "# This is necessary because Jupyter notebooks inherently operate in an asynchronous loop.\n",
        "# By applying nest_asyncio, we can run additional async functions within this existing loop without conflicts.\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "from llama_index.core.evaluation import generate_question_context_pairs\n",
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    SimpleDirectoryReader,\n",
        "    ServiceContext,\n",
        ")\n",
        "from llama_index.core.node_parser import SimpleNodeParser\n",
        "from llama_index.core.evaluation import generate_question_context_pairs\n",
        "from llama_index.core.evaluation import RetrieverEvaluator\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from lastmile_eval.rag.debugger.tracing import get_lastmile_tracer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "apw5FNNMVDCV",
        "outputId": "c859dbba-db3f-4e31-9110-7a46327f5a0f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import dotenv\n",
        "# You can get your OPENAI_API_KEY from https://platform.openai.com/api-keys\n",
        "\n",
        "dotenv.load_dotenv()\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "LASTMILE_API_TOKEN = os.getenv(\"LASTMILE_API_TOKEN\")\n",
        "\n",
        "os.environ[\"LASTMILE_API_TOKEN\"] = LASTMILE_API_TOKEN\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQtulqxOVDCV",
        "outputId": "b6c5f83a-95d9-42a3-faf7-2915e1f3455b"
      },
      "outputs": [],
      "source": [
        "!mkdir -p 'data/paul_graham/'\n",
        "!curl 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -o 'data/paul_graham/paul_graham_essay.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY5tB8_mVDCV"
      },
      "source": [
        "## Step 2: Run and Trace Ingestion Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpaKBoOAVDCV"
      },
      "outputs": [],
      "source": [
        "# Instantiate a tracer object\n",
        "tracer = get_lastmile_tracer(\"my_cool_tracer\")\n",
        "\n",
        "\n",
        "# You can use the tracer either as a decorator around a function (like below)\n",
        "# or with the \"with ... as span_variable_name:\" syntax\n",
        "@tracer.start_as_current_span(\"ingestion-root-span\")\n",
        "def run_ingestion_flow():\n",
        "    documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n",
        "\n",
        "    # Register the doc file paths as a parameter\n",
        "    doc_file_paths = [\n",
        "        doc.metadata.get(\"file_path\")\n",
        "        for doc in documents\n",
        "        if doc.metadata.get(\"file_path\") is not None\n",
        "    ]\n",
        "    tracer.register_param(\"doc_file_paths\", str(doc_file_paths))\n",
        "\n",
        "    with tracer.start_as_current_span(\n",
        "        \"create-document-nodes\"\n",
        "    ) as _node_parser_span:\n",
        "      # Register chunk_size as a parameter in this\n",
        "      # trace's parameter set\n",
        "      chunk_size = 512\n",
        "      tracer.register_param(\"chunk_size\", chunk_size)\n",
        "\n",
        "      node_parser = SimpleNodeParser.from_defaults(chunk_size=chunk_size)\n",
        "      nodes = node_parser.get_nodes_from_documents(documents)\n",
        "\n",
        "    # Mark a RAG Ingestion trace event\n",
        "    #   --> For now this only accepts strings and list of strings\n",
        "    #   --> We can add more specific events (like what you'll see with\n",
        "    #      the `mark_rag_query_trace_event` method) in the future\n",
        "      tracer.mark_rag_ingestion_trace_event(\"Created document nodes!\")\n",
        "    with tracer.start_as_current_span(\n",
        "        \"embed-document-nodes\"\n",
        "    ) as _create_node_span:\n",
        "        vector_index = VectorStoreIndex(nodes)\n",
        "        query_engine = vector_index.as_query_engine()\n",
        "        tracer.mark_rag_ingestion_trace_event(\"Created embeddings!\")\n",
        "\n",
        "    # We use these variables later in the notebook so need to return them\n",
        "    # in this function\n",
        "    return nodes, vector_index, query_engine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1qj7TcrVDCV"
      },
      "outputs": [],
      "source": [
        "# Run the ingestion flow and save the trace data\n",
        "# This saves it to two tables:\n",
        "# 1) The raw trace data that gets saved to Jaeger\n",
        "# 2) The structured trace data that includes the paramSets, events,\n",
        "#   etc that gets saved to our Postgres tables\n",
        "\n",
        "# Run this cell once to generate an ingestion trace\n",
        "nodes, vector_index, query_engine = run_ingestion_flow()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IIzk6G8VDCV",
        "outputId": "c0671c6a-cddb-4bf3-995e-d0a3d8403376"
      },
      "outputs": [],
      "source": [
        "# Let's print the trace data from Jaeger to\n",
        "# show you what it looks like (search for \"operationName\" in the data)\n",
        "\n",
        "from lastmile_eval.rag.debugger.tracing import (\n",
        "    get_latest_ingestion_trace_id,\n",
        "    get_trace_data,\n",
        ")\n",
        "\n",
        "ingestion_trace_id = get_latest_ingestion_trace_id()\n",
        "get_trace_data(ingestion_trace_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "Znk3omg3VDCW",
        "outputId": "9509c207-cd9e-4a0a-dda4-3730fd6eabc0"
      },
      "outputs": [],
      "source": [
        "# Now let's fetch the trace event data from our postgres table\n",
        "# Notice that the `traceId` column matches with the raw trace data\n",
        "\n",
        "from lastmile_eval.rag.debugger.tracing import list_ingestion_trace_events\n",
        "\n",
        "ingestion_trace_events = list_ingestion_trace_events(take=1)\n",
        "pd.DataFrame.from_records(ingestion_trace_events[\"ingestionTraces\"]).rename(  # type: ignore[fixme]\n",
        "    columns={\"id\": \"ragIngestionTraceEventId\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ4hNS0pVDCW"
      },
      "source": [
        "## Part 3: Run and Trace Query Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "H2Ita6wMVDCW"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from lastmile_eval.rag.debugger.api import (\n",
        "    QueryReceived,\n",
        "    ContextRetrieved,\n",
        "    PromptResolved,\n",
        "    LLMOutputReceived,\n",
        ")\n",
        "\n",
        "LLM_NAME = \"gpt-4\"\n",
        "\n",
        "# Note, normally you can just call `query_engine.query(user_query)`\n",
        "# but this abstracts away a lot of the steps so we will be doing\n",
        "# each step manually to showcase how to use the tracer\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "Context information is below.\n",
        "---------------------\n",
        "{context_str}\n",
        "---------------------\n",
        "Given the context information and not prior knowledge, answer the query.\n",
        "Query: {query_str}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "@tracer.start_as_current_span(\"query-root-span\")\n",
        "def run_query_flow(user_query: str, ingestion_trace_id: str):\n",
        "    tracer.mark_rag_query_trace_event(\n",
        "        QueryReceived(query=user_query), ingestion_trace_id\n",
        "    )\n",
        "\n",
        "    with tracer.start_as_current_span(\n",
        "        \"retrieve-context\"\n",
        "    ) as _retrieve_context_span:\n",
        "        similarity_top_k = 5\n",
        "        tracer.register_param(\"similarity_top_k\", similarity_top_k)\n",
        "\n",
        "        retriever = vector_index.as_retriever(\n",
        "            similarity_top_k=similarity_top_k\n",
        "        )\n",
        "        retrieved_nodes = retriever.retrieve(user_query)\n",
        "        retrieved_contexts = [node.get_text() for node in retrieved_nodes]\n",
        "\n",
        "        retrieved_node_ids = [node.id_ for node in retrieved_nodes]\n",
        "        tracer.register_param(\"retrieved_node_ids\", retrieved_node_ids)\n",
        "\n",
        "        tracer.mark_rag_query_trace_event(\n",
        "            ContextRetrieved(context=retrieved_contexts), ingestion_trace_id\n",
        "        )\n",
        "\n",
        "    with tracer.start_as_current_span(\"resolve-prompt\") as _resolve_prompt_span:\n",
        "        resolved_prompt = PROMPT_TEMPLATE.replace(\n",
        "            \"{context_str}\", \"\\n\\n\\n\".join(retrieved_contexts)\n",
        "        ).replace(\"{query_str}\", user_query)\n",
        "        tracer.mark_rag_query_trace_event(\n",
        "            PromptResolved(fully_resolved_prompt=resolved_prompt),\n",
        "            ingestion_trace_id,\n",
        "        )\n",
        "\n",
        "    with tracer.start_as_current_span(\"call-llm\") as _llm_span:\n",
        "        openai_client = openai.Client(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "        response = openai_client.chat.completions.create(\n",
        "            model=LLM_NAME,\n",
        "            messages=[{\"role\": \"user\", \"content\": resolved_prompt}],\n",
        "        )\n",
        "        output: str = response.choices[0].message.content\n",
        "        tracer.mark_rag_query_trace_event(\n",
        "            LLMOutputReceived(llm_output=output), ingestion_trace_id\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMQ5nHcYVDCW"
      },
      "outputs": [],
      "source": [
        "# TODO: Right now the ingestion_trace_id within mark_rag_query_trace_event is\n",
        "# no-op due to changes in assumptions, I'll fix later\n",
        "run_query_flow(\"What did the author do growing up?\", ingestion_trace_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "GTXnkr4XVDCW",
        "outputId": "9883c082-42bd-4e30-fa24-a9a15be560c3"
      },
      "outputs": [],
      "source": [
        "# Just like what we did with the ingestion trace,\n",
        "# let's print out what this looks like in the PostGres data, as well as the\n",
        "# pure trace data again\n",
        "from lastmile_eval.rag.debugger.tracing import (\n",
        "    list_query_trace_events,\n",
        ")\n",
        "\n",
        "query_trace_events = list_query_trace_events(take=1)\n",
        "query_trace_events_df = pd.DataFrame.from_records(query_trace_events[\"queryTraces\"]).rename(  # type: ignore[fixme]\n",
        "    columns={\"id\": \"ragQueryTraceEventsId\"}\n",
        ")\n",
        "query_trace_events_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PepT9DXcm4B1",
        "outputId": "66e1f665-5006-432b-ca5a-1201f01416f2"
      },
      "outputs": [],
      "source": [
        "# This is what the trace data looks like\n",
        "from lastmile_eval.rag.debugger.tracing import (\n",
        "    get_trace_data,\n",
        ")\n",
        "\n",
        "query_trace_id = query_trace_events_df.iloc[0][\"traceId\"]\n",
        "get_trace_data(query_trace_id)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPMbbFE4VDCX"
      },
      "source": [
        "## Part 4: Create Test Sets and Run Evaluators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCpbwqGqVDCX",
        "outputId": "1a43a2a8-4bda-4960-8850-0f402e7b3772"
      },
      "outputs": [],
      "source": [
        "# NOTE: Running this cell on all the nodes will take a while (probably 5-10mins), so please be patient\n",
        "\n",
        "# Change this to a lower value if you want to run faster\n",
        "# If we use None, we will not use this value and use total_queries_per_batch\n",
        "# instead\n",
        "total_queries_to_run_override = (\n",
        "    5  # None\n",
        ")\n",
        "\n",
        "\n",
        "# Ok we're now going to artifically generate a bunch of query + context\n",
        "# (ground truth) pairs. We will then run the `run_query_flow()` method on these\n",
        "# generated queries later\n",
        "\n",
        "# Define an LLM\n",
        "llm = OpenAI(model=LLM_NAME)\n",
        "\n",
        "\n",
        "# This method `generate_question_context_pairs()` essentially\n",
        "# calls an LLM to generate questions for us. See this URL for more details:\n",
        "# https://github.com/run-llama/llama_index/blob/8b373239396134a92c9277b36aa7023c633c018a/llama-index-finetuning/llama_index/finetuning/embeddings/common.py#L49-L64\n",
        "num_questions_per_chunk = 1\n",
        "qa_dataset = generate_question_context_pairs(\n",
        "    nodes[0:total_queries_to_run_override or len(nodes)],\n",
        "    llm=llm,\n",
        "    num_questions_per_chunk=num_questions_per_chunk\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a6rPAZTVDCX",
        "outputId": "c48cfeac-98ef-46a5-e1aa-3f3c07293101"
      },
      "outputs": [],
      "source": [
        "# Run these queries through the `run_query_flow()` method\n",
        "\n",
        "total_queries_per_batch = len(qa_dataset.queries)\n",
        "total_queries_to_run = min(total_queries_to_run_override or total_queries_per_batch, total_queries_per_batch)\n",
        "\n",
        "expected_node_ids: list[str] = []\n",
        "for i, (query_id, query) in enumerate(qa_dataset.queries.items()):\n",
        "    run_query_flow(query, ingestion_trace_id)\n",
        "    associated_node_id_for_query = qa_dataset.relevant_docs[query_id]\n",
        "    expected_node_ids.append(associated_node_id_for_query[0])\n",
        "\n",
        "    print(f\"Finished running {i+1}/{total_queries_to_run} queries...\")\n",
        "    if i + 1 == total_queries_to_run:\n",
        "        break\n",
        "\n",
        "# Have to reverse because the get_rag_query_trace_events() method\n",
        "# returns the most recent trace events first\n",
        "expected_node_ids.reverse()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "6zjuOAs2VDCX",
        "outputId": "804da333-a754-4972-c3d4-789c5f1e94f3"
      },
      "outputs": [],
      "source": [
        "from lastmile_eval.rag.debugger.tracing import list_query_trace_events\n",
        "\n",
        "query_trace_events = list_query_trace_events(take=total_queries_to_run)\n",
        "query_trace_events_df = pd.DataFrame.from_records(query_trace_events[\"queryTraces\"]).rename(  # type: ignore[fixme]\n",
        "    columns={\"id\": \"ragQueryTraceId\"}\n",
        ")\n",
        "query_trace_events_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5 - Run Evaluators from Query Trace Events data\n",
        "\n",
        "This directly creates evaluation sets using the method `evaluate_rag_outputs()` without the need to create intermediate test cases and test sets. All you need is to define your query trace event rows in a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from lastmile_eval.rag.debugger.api.evaluation import evaluate_rag_outputs\n",
        "from lastmile_eval.rag.debugger.tracing import (\n",
        "    get_query_trace_event,\n",
        ")\n",
        "\n",
        "from lastmile_eval.text.metrics import calculate_rouge1_score\n",
        "from llama_index.core.evaluation import HitRate, MRR\n",
        "\n",
        "\n",
        "def rouge1(df: pd.DataFrame):\n",
        "    return [\n",
        "        # some weird error where it doesn't work for 0 values\n",
        "        0.01 + x for x in calculate_rouge1_score(df[\"output\"].tolist(), df[\"groundTruth\"].tolist())\n",
        "    ]\n",
        "\n",
        "def extract_data_to_evaluate(\n",
        "    row: pd.Series,\n",
        ") -> tuple[list[str], list[str]]:\n",
        "    trace_query_id: str = row[\"ragQueryTraceId\"]\n",
        "    trace_query_data = get_query_trace_event(trace_query_id)\n",
        "    retrieved_node_ids = trace_query_data[\"paramSet\"][\"retrieved_node_ids\"]\n",
        "    expected_node_ids: list[str] = [row[\"groundTruth\"]]\n",
        "    return (retrieved_node_ids, expected_node_ids)\n",
        "\n",
        "\n",
        "def compute_eval_score(\n",
        "    retrieved_and_expected_node_ids_tuple: tuple[list[str], list[str]],\n",
        "    evaluator: HitRate | MRR,\n",
        ") -> float:\n",
        "    retrieved_node_ids, expected_node_ids = (\n",
        "        retrieved_and_expected_node_ids_tuple\n",
        "    )\n",
        "    return evaluator.compute(\n",
        "        retrieved_ids=retrieved_node_ids, expected_ids=expected_node_ids\n",
        "    ).score\n",
        "\n",
        "# Example using a row-level function on the dataframe\n",
        "def compute_mrr(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    We are demonstrating methods that are applied across a row instead of\n",
        "    entire dataframe, such as the MRR and Hit Rate metrics from the \n",
        "    llama_index.core.evaluation package. In order to do this, we define a\n",
        "    method at the row level where we:\n",
        "    \n",
        "    1. Extract the data to evaluate from the row\n",
        "    2. Run the evaluators on this extracted data\n",
        "    \n",
        "    After that's done, we pass this row-level method to df.apply()\n",
        "    \"\"\"\n",
        "    def evaluate_using_row_method(row: pd.Series) -> float:\n",
        "        node_id_tuple = extract_data_to_evaluate(row)\n",
        "        return compute_eval_score(node_id_tuple, MRR())\n",
        "    \n",
        "    return df.apply(evaluate_using_row_method, axis=1)\n",
        "\n",
        "def compute_hit_rate(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Another row-function example with hit_rate\n",
        "    \"\"\"\n",
        "    def evaluate_using_row_method(row: pd.Series) -> float:\n",
        "        node_id_tuple = extract_data_to_evaluate(row)\n",
        "        return compute_eval_score(node_id_tuple, HitRate())\n",
        "    \n",
        "    return df.apply(evaluate_using_row_method, axis=1)\n",
        "    \n",
        "trace_level_evaluators = {\n",
        "    \"rouge1\": rouge1,\n",
        "    \"mrr\": compute_mrr,\n",
        "    \"hit_rate\": compute_hit_rate,\n",
        "}\n",
        "\n",
        "# We must add groundTruth to the dataframe\n",
        "query_trace_events_df[\"groundTruth\"] = expected_node_ids\n",
        "\n",
        "eval_result = evaluate_rag_outputs(\n",
        "    project_id=\"can be anything for now\",\n",
        "    trace_level_evaluators=trace_level_evaluators,\n",
        "    dataset_level_evaluators={},\n",
        "    df=query_trace_events_df,\n",
        "    lastmile_api_token=LASTMILE_API_TOKEN,\n",
        "    evaluation_set_name=\"Cool new evaluation set name\"\n",
        ")\n",
        "\n",
        "#print out result\n",
        "eval_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 6 - Run Evaluators by creating intermediate test cases, test sets first\n",
        "\n",
        "This is showing how to manually create test set "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6EVmRWzVDCX",
        "outputId": "86ee4dfb-610a-4272-fb0c-9b1491fedae9"
      },
      "outputs": [],
      "source": [
        "from lastmile_eval.rag.debugger.api import (\n",
        "    create_test_set_from_rag_query_traces,\n",
        ")\n",
        "\n",
        "create_test_set_from_rag_query_traces(\n",
        "    query_trace_events_df,\n",
        "    test_set_name=\"Retrieval Eval Test Set\",\n",
        "    lastmile_api_token=LASTMILE_API_TOKEN,\n",
        "    ground_truth=expected_node_ids,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brdPi_JcVDCX"
      },
      "outputs": [],
      "source": [
        "from lastmile_eval.rag.debugger.api import (\n",
        "    get_latest_test_set_id,\n",
        ")\n",
        "\n",
        "test_set_id = get_latest_test_set_id()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "id": "teZEUJ8UVDCY",
        "outputId": "703a5d62-f64c-4f22-d664-c62e78f4cf2a"
      },
      "outputs": [],
      "source": [
        "from lastmile_eval.rag.debugger.api import download_test_set\n",
        "test_set_df = download_test_set(test_set_id)\n",
        "test_set_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "-KxkWqLXVDCY"
      },
      "outputs": [],
      "source": [
        "from lastmile_eval.rag.debugger.tracing import (\n",
        "    get_query_trace_event,\n",
        ")\n",
        "\n",
        "# Define some out of the box retrieval evaluators\n",
        "# TODO: Set up some evaluators that also measure outputs too\n",
        "\"\"\"\n",
        "Hit Rate:\n",
        "Hit rate calculates the fraction of queries where the correct answer is found\n",
        "within the top-k retrieved documents. In simpler terms, it’s about how often\n",
        "our system gets it right within the top few guesses.\n",
        "\n",
        "Mean Reciprocal Rank (MRR):\n",
        "For each query, MRR evaluates the system’s accuracy by looking at the rank of\n",
        "the highest-placed relevant document. Specifically, it’s the average of the\n",
        "reciprocals of these ranks across all the queries. So, if the first relevant\n",
        "document is the top result, the reciprocal rank is 1; if it’s second, the\n",
        "reciprocal rank is 1/2, and so on.\n",
        "\"\"\"\n",
        "from llama_index.core.evaluation import HitRate, MRR\n",
        "hit_rate_evaluator = HitRate()\n",
        "mrr_evaluator = MRR()\n",
        "metric_evaluators = [hit_rate_evaluator, mrr_evaluator]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "7pzSiaB6VDCY",
        "outputId": "2daaaa9c-7237-48aa-e6e6-319849d68f7e"
      },
      "outputs": [],
      "source": [
        "# Manually doing it\n",
        "data = []\n",
        "\n",
        "def retrieved_correct_context_node(test_set_df):\n",
        "    for _index, row in test_set_df.iterrows():\n",
        "        trace_query_id = row[\"ragQueryTraceId\"]\n",
        "        trace_query_data = get_query_trace_event(trace_query_id)\n",
        "        print(f\"{trace_query_data=}\")\n",
        "        retrieved_node_ids = trace_query_data[\"paramSet\"][\"retrieved_node_ids\"]\n",
        "        expected_node_ids: list[str] = [row[\"groundTruth\"]]\n",
        "\n",
        "        evaluator_results = [\n",
        "            evaluator.compute(\n",
        "                retrieved_ids=retrieved_node_ids,\n",
        "                expected_ids=expected_node_ids,\n",
        "            ).score\n",
        "            for evaluator in metric_evaluators\n",
        "        ]\n",
        "        data.append([trace_query_id, *evaluator_results])\n",
        "    # trace_query_id = list_query_trace_events(take=1)[\"queryTraces\"][0][\"id\"]\n",
        "\n",
        "\n",
        "retrieved_correct_context_node(test_set_df)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "columns = [\"Trace Query Event Id\", \"Hit Rate\", \"MRR\"]\n",
        "eval_pd = pd.DataFrame(data, columns=columns)\n",
        "eval_pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVa94Mh7VDCZ",
        "outputId": "146e7635-8f59-418f-b1ba-4060c7fe40dd"
      },
      "outputs": [],
      "source": [
        "# Using the run_and_store_evaluations method\n",
        "from lastmile_eval.rag.debugger.api import run_and_store_evaluations\n",
        "\n",
        "result = run_and_store_evaluations(\n",
        "    test_set_id,\n",
        "    \"Fake project name\",\n",
        "    {\n",
        "        \"Hit Rate\": compute_hit_rate,\n",
        "        \"MRR\": compute_mrr,\n",
        "    },\n",
        "    {},\n",
        "    LASTMILE_API_TOKEN,\n",
        "    f\"Evaluation Results for Test Set {test_set_id}\",\n",
        ")\n",
        "\n",
        "# TODO: Print out the evaluation test table with final evaluation metrics\n",
        "result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "uDSGDED0tFzw",
        "outputId": "b381ffd6-c0df-4e51-c4d2-0a7fb2950abf"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from requests import Response\n",
        "from typing import Any, Optional\n",
        "\n",
        "# TODO: Save this as it's own helper SDK from the lastmile-eval package\n",
        "def list_evaluation_sets(\n",
        "    take: int = 10,\n",
        "    # TODO: Create macro for default timeout value\n",
        "    timeout: int = 60,\n",
        ") -> dict[str, Any]:  # TODO: Define eplicit typing for JSON response return\n",
        "    \"\"\"\n",
        "    Get a list of evaluation sets from the LastMile API.\n",
        "\n",
        "    Args:\n",
        "        take: The number of evaluation sets to return. The default is 10.\n",
        "        lastmile_api_token: The API token for the LastMile API. If not provided,\n",
        "            will try to get the token from the LASTMILE_API_TOKEN\n",
        "            environment variable.\n",
        "            You can create a token from the \"API Tokens\" section from this website:\n",
        "            https://lastmileai.dev/settings?page=tokens\n",
        "        timeout: The maximum time in seconds to wait for the request to complete.\n",
        "            The default is 60.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the evaluation sets.\n",
        "    \"\"\"\n",
        "    lastmile_endpoint = f\"https://lastmileai.dev/api/evaluation_sets/list?pageSize={str(take)}\"\n",
        "\n",
        "    response: Response = requests.get(\n",
        "        lastmile_endpoint,\n",
        "        headers={\"Authorization\": f\"Bearer {LASTMILE_API_TOKEN}\"},\n",
        "        timeout=timeout,\n",
        "    )\n",
        "    # TODO: Handle response errors\n",
        "    return response.json()\n",
        "\n",
        "evaluation_sets = list_evaluation_sets(take=1)\n",
        "evaluation_sets_df = pd.DataFrame.from_records(evaluation_sets[\"evaluationSets\"]).rename(  # type: ignore[fixme]\n",
        "    columns={\"id\": \"evaluationSetId\"}\n",
        ")\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "evaluation_sets_df\n",
        "\n",
        "# TODO: evaluationSetMetrics looks a bit weird, should probalby have helper\n",
        "# method to display it better, but it's ok for now"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
