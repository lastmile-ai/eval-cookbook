{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "3e67f200",
            "metadata": {},
            "source": [
                "# How to call functions with chat models - Traced With LastMile\n",
                "\n",
                "\n",
                "This Notebook demonstrates how to use the LastMile OpenAI wrapper, a powerful tool that captures, logs, and traces your interactions with the OpenAI API, making it easier to debug and analyze your code.\n",
                "It also covers how to use the Chat Completions API in combination with external functions to extend the capabilities of GPT models.\n",
                "\n",
                "The LastMile OpenAI wrapper is designed to seamlessly integrate with your existing OpenAI code, allowing you to trace and monitor API calls without requiring significant changes to your implementation. By using this wrapper, you can gain valuable insights into how your application interacts with the OpenAI API, helping you identify potential issues and optimize your code.\n",
                "\n",
                "`tools` is an optional parameter in the Chat Completion API which can be used to provide function specifications. The purpose of this is to enable models to generate function arguments which adhere to the provided specifications. Note that the API will not actually execute any function calls. It is up to developers to execute function calls using model outputs.\n",
                "\n",
                "Within the `tools` parameter, if the `functions` parameter is provided then by default the model will decide when it is appropriate to use one of the functions. The API can be forced to use a specific function by setting the `tool_choice` parameter to `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}`. The API can also be forced to not use any function by setting the `tool_choice` parameter to `\"none\"`. If a function is used, the output will contain `\"finish_reason\": \"tool_calls\"` in the response, as well as a `tool_calls` object that has the name of the function and the generated function arguments.\n",
                "\n",
                "### Overview\n",
                "\n",
                "This notebook contains the following 2 sections:\n",
                "\n",
                "- **How to generate function arguments:** Specify a set of functions and use the API to generate function arguments.\n",
                "- **How to call functions with model generated arguments:** Close the loop by actually executing functions with model generated arguments."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "64c85e26",
            "metadata": {},
            "source": [
                "## How to generate function arguments"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "80e71f33",
            "metadata": {
                "pycharm": {
                    "is_executing": true
                }
            },
            "outputs": [],
            "source": [
                "# !pip install scipy --quiet\n",
                "# !pip install tenacity --quiet\n",
                "# !pip install tiktoken --quiet\n",
                "# !pip install termcolor --quiet\n",
                "# !pip install openai --quiet\n",
                "# !pip install \"tracing-auto-instrumentation[openai]\""
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3108b91b",
            "metadata": {},
            "source": [
                "Setting up the OpenAI Wrapper is as easy as the following 5 lines."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "dab872c5",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:45:23.563149Z",
                    "start_time": "2024-05-15T17:45:22.925978Z"
                }
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[DEBUG] 2024-06-19 18:49:50,041 _config.py:80: load_ssl_context verify=True cert=None trust_env=True http2=False\n",
                        "[DEBUG] 2024-06-19 18:49:50,041 _config.py:146: load_verify_locations cafile='/opt/homebrew/Caskroom/miniconda/base/envs/eval/lib/python3.12/site-packages/certifi/cacert.pem'\n"
                    ]
                }
            ],
            "source": [
                "import openai\n",
                "\n",
                "from lastmile_eval.rag.debugger.api.tracing import LastMileTracer\n",
                "\n",
                "from tracing_auto_instrumentation.openai import wrap_openai\n",
                "from lastmile_eval.rag.debugger.tracing.sdk import get_lastmile_tracer\n",
                "\n",
                "tracer: LastMileTracer = get_lastmile_tracer(\n",
                "    tracer_name=\"OpenAI Function Calling\",\n",
                ")\n",
                "client = wrap_openai(openai.OpenAI(), tracer)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "1251594d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# OpenAI Setup\n",
                "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
                "from termcolor import colored\n",
                "import json\n",
                "\n",
                "import openai\n",
                "\n",
                "GPT_MODEL = \"gpt-4o\""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "69ee6a93",
            "metadata": {},
            "source": [
                "### Utilities\n",
                "\n",
                "First let's define a few utilities for making calls to the Chat Completions API and for maintaining and keeping track of the conversation state."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "745ceec5",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:45:28.816345Z",
                    "start_time": "2024-05-15T17:45:28.814155Z"
                }
            },
            "outputs": [],
            "source": [
                "@retry(wait=wait_random_exponential(multiplier=1, max=40), stop=stop_after_attempt(3))\n",
                "def chat_completion_request(messages, tools=None, tool_choice=None, model=GPT_MODEL):\n",
                "    try:\n",
                "        response = client.chat.completions.create(\n",
                "            model=model,\n",
                "            messages=messages,\n",
                "            tools=tools,\n",
                "            tool_choice=tool_choice,\n",
                "        )\n",
                "        return response\n",
                "    except Exception as e:\n",
                "        print(\"Unable to generate ChatCompletion response\")\n",
                "        print(f\"Exception: {e}\")\n",
                "        return e\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "c4d1c99f",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:45:30.003910Z",
                    "start_time": "2024-05-15T17:45:30.001259Z"
                }
            },
            "outputs": [],
            "source": [
                "def pretty_print_conversation(messages):\n",
                "    role_to_color = {\n",
                "        \"system\": \"red\",\n",
                "        \"user\": \"green\",\n",
                "        \"assistant\": \"blue\",\n",
                "        \"function\": \"magenta\",\n",
                "    }\n",
                "    \n",
                "    for message in messages:\n",
                "        if message[\"role\"] == \"system\":\n",
                "            print(colored(f\"system: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
                "        elif message[\"role\"] == \"user\":\n",
                "            print(colored(f\"user: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
                "        elif message[\"role\"] == \"assistant\" and message.get(\"function_call\"):\n",
                "            print(colored(f\"assistant: {message['function_call']}\\n\", role_to_color[message[\"role\"]]))\n",
                "        elif message[\"role\"] == \"assistant\" and not message.get(\"function_call\"):\n",
                "            print(colored(f\"assistant: {message['content']}\\n\", role_to_color[message[\"role\"]]))\n",
                "        elif message[\"role\"] == \"function\":\n",
                "            print(colored(f\"function ({message['name']}): {message['content']}\\n\", role_to_color[message[\"role\"]]))\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "29d4e02b",
            "metadata": {},
            "source": [
                "### Basic concepts\n",
                "\n",
                "Let's create some function specifications to interface with a hypothetical weather API. We'll pass these function specification to the Chat Completions API in order to generate function arguments that adhere to the specification."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "d2e25069",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:45:31.794879Z",
                    "start_time": "2024-05-15T17:45:31.792617Z"
                }
            },
            "outputs": [],
            "source": [
                "tools = [\n",
                "    {\n",
                "        \"type\": \"function\",\n",
                "        \"function\": {\n",
                "            \"name\": \"get_current_weather\",\n",
                "            \"description\": \"Get the current weather\",\n",
                "            \"parameters\": {\n",
                "                \"type\": \"object\",\n",
                "                \"properties\": {\n",
                "                    \"location\": {\n",
                "                        \"type\": \"string\",\n",
                "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
                "                    },\n",
                "                    \"format\": {\n",
                "                        \"type\": \"string\",\n",
                "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
                "                        \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
                "                    },\n",
                "                },\n",
                "                \"required\": [\"location\", \"format\"],\n",
                "            },\n",
                "        }\n",
                "    },\n",
                "    {\n",
                "        \"type\": \"function\",\n",
                "        \"function\": {\n",
                "            \"name\": \"get_n_day_weather_forecast\",\n",
                "            \"description\": \"Get an N-day weather forecast\",\n",
                "            \"parameters\": {\n",
                "                \"type\": \"object\",\n",
                "                \"properties\": {\n",
                "                    \"location\": {\n",
                "                        \"type\": \"string\",\n",
                "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
                "                    },\n",
                "                    \"format\": {\n",
                "                        \"type\": \"string\",\n",
                "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
                "                        \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
                "                    },\n",
                "                    \"num_days\": {\n",
                "                        \"type\": \"integer\",\n",
                "                        \"description\": \"The number of days to forecast\",\n",
                "                    }\n",
                "                },\n",
                "                \"required\": [\"location\", \"format\", \"num_days\"]\n",
                "            },\n",
                "        }\n",
                "    },\n",
                "]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "bfc39899",
            "metadata": {},
            "source": [
                "If we prompt the model about the current weather, it will respond with some clarifying questions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "518d6827",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:45:35.282310Z",
                    "start_time": "2024-05-15T17:45:33.861496Z"
                }
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[DEBUG] 2024-06-19 18:49:50,078 _base_client.py:446: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"}, {'role': 'user', 'content': \"What's the weather like today\"}], 'model': 'gpt-4o', 'tool_choice': None, 'tools': [{'type': 'function', 'function': {'name': 'get_current_weather', 'description': 'Get the current weather', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'format': {'type': 'string', 'enum': ['celsius', 'fahrenheit'], 'description': 'The temperature unit to use. Infer this from the users location.'}}, 'required': ['location', 'format']}}}, {'type': 'function', 'function': {'name': 'get_n_day_weather_forecast', 'description': 'Get an N-day weather forecast', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'format': {'type': 'string', 'enum': ['celsius', 'fahrenheit'], 'description': 'The temperature unit to use. Infer this from the users location.'}, 'num_days': {'type': 'integer', 'description': 'The number of days to forecast'}}, 'required': ['location', 'format', 'num_days']}}}]}}\n",
                        "[DEBUG] 2024-06-19 18:49:50,093 _base_client.py:949: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
                        "[DEBUG] 2024-06-19 18:49:50,094 _trace.py:45: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
                        "[DEBUG] 2024-06-19 18:49:50,112 _trace.py:45: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x12bef67b0>\n",
                        "[DEBUG] 2024-06-19 18:49:50,112 _trace.py:45: start_tls.started ssl_context=<ssl.SSLContext object at 0x14a1aa850> server_hostname='api.openai.com' timeout=5.0\n",
                        "[DEBUG] 2024-06-19 18:49:50,126 _trace.py:45: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x10a3e1ca0>\n",
                        "[DEBUG] 2024-06-19 18:49:50,126 _trace.py:45: send_request_headers.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:50,127 _trace.py:45: send_request_headers.complete\n",
                        "[DEBUG] 2024-06-19 18:49:50,127 _trace.py:45: send_request_body.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:50,127 _trace.py:45: send_request_body.complete\n",
                        "[DEBUG] 2024-06-19 18:49:50,127 _trace.py:45: receive_response_headers.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:50,960 _trace.py:45: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 19 Jun 2024 22:49:50 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'lastmile-ai'), (b'openai-processing-ms', b'546'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'12000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'11999945'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_85b2e90480bbf0d75919f2eebb80f103'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=pzwtjDi9aiE86GhuA33cICMhraNH7IVRKv0vcIkQHVA-1718837390-1.0.1.1-UlATUG_aKUhYRaUrnjRiCL4r1ifgwd2lIIQc5wqMC2maVV.x8ikfo7crdde173_cHboPRiqyBTVQREsrqg0McA; path=/; expires=Wed, 19-Jun-24 23:19:50 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=htmw3yUFQnDcRL8UqzN0uBL2YVP29sf2kKMGnr9uY0M-1718837390935-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'896713188fa1238e-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
                        "[INFO] 2024-06-19 18:49:50,962 _client.py:1026: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "[DEBUG] 2024-06-19 18:49:50,962 _trace.py:45: receive_response_body.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:50,963 _trace.py:45: receive_response_body.complete\n",
                        "[DEBUG] 2024-06-19 18:49:50,963 _trace.py:45: response_closed.started\n",
                        "[DEBUG] 2024-06-19 18:49:50,964 _trace.py:45: response_closed.complete\n",
                        "[DEBUG] 2024-06-19 18:49:50,964 _base_client.py:988: HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Wed, 19 Jun 2024 22:49:50 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('openai-organization', 'lastmile-ai'), ('openai-processing-ms', '546'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=15724800; includeSubDomains'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '12000000'), ('x-ratelimit-remaining-requests', '9999'), ('x-ratelimit-remaining-tokens', '11999945'), ('x-ratelimit-reset-requests', '6ms'), ('x-ratelimit-reset-tokens', '0s'), ('x-request-id', 'req_85b2e90480bbf0d75919f2eebb80f103'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=pzwtjDi9aiE86GhuA33cICMhraNH7IVRKv0vcIkQHVA-1718837390-1.0.1.1-UlATUG_aKUhYRaUrnjRiCL4r1ifgwd2lIIQc5wqMC2maVV.x8ikfo7crdde173_cHboPRiqyBTVQREsrqg0McA; path=/; expires=Wed, 19-Jun-24 23:19:50 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('set-cookie', '_cfuvid=htmw3yUFQnDcRL8UqzN0uBL2YVP29sf2kKMGnr9uY0M-1718837390935-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '896713188fa1238e-EWR'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
                        "[DEBUG] 2024-06-19 18:49:50,965 _base_client.py:996: request_id: req_85b2e90480bbf0d75919f2eebb80f103\n",
                        "[DEBUG] 2024-06-19 18:49:50,980 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
                        "[DEBUG] 2024-06-19 18:49:51,059 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/trace/create HTTP/11\" 200 10\n",
                        "[DEBUG] 2024-06-19 18:49:51,061 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
                        "[DEBUG] 2024-06-19 18:49:51,137 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/rag_query_traces/create HTTP/11\" 200 None\n",
                        "[DEBUG] 2024-06-19 18:49:51,140 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
                        "[DEBUG] 2024-06-19 18:49:51,213 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/rag_events/create HTTP/11\" 200 None\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "ChatCompletionMessage(content='Sure, I can help with that. Could you please provide the name of the city and state or country you are interested in?', role='assistant', function_call=None, tool_calls=None)"
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "messages = []\n",
                "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
                "messages.append({\"role\": \"user\", \"content\": \"What's the weather like today\"})\n",
                "chat_response = chat_completion_request(\n",
                "    messages, tools=tools\n",
                ")\n",
                "assistant_message = chat_response.choices[0].message\n",
                "messages.append(assistant_message)\n",
                "assistant_message\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "4c999375",
            "metadata": {},
            "source": [
                "Once we provide the missing information, it will generate the appropriate function arguments for us."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "23c42a6e",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:45:43.553403Z",
                    "start_time": "2024-05-15T17:45:42.205590Z"
                }
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[DEBUG] 2024-06-19 18:49:51,229 _base_client.py:446: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"}, {'role': 'user', 'content': \"What's the weather like today\"}, {'content': 'Sure, I can help with that. Could you please provide the name of the city and state or country you are interested in?', 'role': 'assistant'}, {'role': 'user', 'content': \"I'm in Glasgow, Scotland.\"}], 'model': 'gpt-4o', 'tool_choice': None, 'tools': [{'type': 'function', 'function': {'name': 'get_current_weather', 'description': 'Get the current weather', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'format': {'type': 'string', 'enum': ['celsius', 'fahrenheit'], 'description': 'The temperature unit to use. Infer this from the users location.'}}, 'required': ['location', 'format']}}}, {'type': 'function', 'function': {'name': 'get_n_day_weather_forecast', 'description': 'Get an N-day weather forecast', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'format': {'type': 'string', 'enum': ['celsius', 'fahrenheit'], 'description': 'The temperature unit to use. Infer this from the users location.'}, 'num_days': {'type': 'integer', 'description': 'The number of days to forecast'}}, 'required': ['location', 'format', 'num_days']}}}]}}\n",
                        "[DEBUG] 2024-06-19 18:49:51,230 _base_client.py:949: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
                        "[DEBUG] 2024-06-19 18:49:51,231 _trace.py:45: send_request_headers.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:51,232 _trace.py:45: send_request_headers.complete\n",
                        "[DEBUG] 2024-06-19 18:49:51,232 _trace.py:45: send_request_body.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:51,233 _trace.py:45: send_request_body.complete\n",
                        "[DEBUG] 2024-06-19 18:49:51,233 _trace.py:45: receive_response_headers.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:51,904 _trace.py:45: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 19 Jun 2024 22:49:51 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'lastmile-ai'), (b'openai-processing-ms', b'432'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'12000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'11999906'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_46590559b78c4c4f651978e6c0e21d4b'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8967131f7d4f238e-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
                        "[INFO] 2024-06-19 18:49:51,905 _client.py:1026: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "[DEBUG] 2024-06-19 18:49:51,906 _trace.py:45: receive_response_body.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:51,907 _trace.py:45: receive_response_body.complete\n",
                        "[DEBUG] 2024-06-19 18:49:51,907 _trace.py:45: response_closed.started\n",
                        "[DEBUG] 2024-06-19 18:49:51,907 _trace.py:45: response_closed.complete\n",
                        "[DEBUG] 2024-06-19 18:49:51,908 _base_client.py:988: HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Wed, 19 Jun 2024 22:49:51 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'lastmile-ai', 'openai-processing-ms': '432', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '12000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '11999906', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '0s', 'x-request-id': 'req_46590559b78c4c4f651978e6c0e21d4b', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '8967131f7d4f238e-EWR', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
                        "[DEBUG] 2024-06-19 18:49:51,908 _base_client.py:996: request_id: req_46590559b78c4c4f651978e6c0e21d4b\n",
                        "[DEBUG] 2024-06-19 18:49:51,944 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/trace/create HTTP/11\" 200 10\n",
                        "[DEBUG] 2024-06-19 18:49:51,947 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
                        "[DEBUG] 2024-06-19 18:49:52,018 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/rag_query_traces/create HTTP/11\" 200 None\n",
                        "[DEBUG] 2024-06-19 18:49:52,020 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
                        "[DEBUG] 2024-06-19 18:49:52,094 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/rag_events/create HTTP/11\" 200 None\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_ucjREdbEpsyL3fFVzNJGGN1W', function=Function(arguments='{\"location\":\"Glasgow, Scotland\",\"format\":\"celsius\"}', name='get_current_weather'), type='function')])"
                        ]
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "messages.append({\"role\": \"user\", \"content\": \"I'm in Glasgow, Scotland.\"})\n",
                "chat_response = chat_completion_request(\n",
                "    messages, tools=tools\n",
                ")\n",
                "assistant_message = chat_response.choices[0].message\n",
                "messages.append(assistant_message)\n",
                "assistant_message\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "c14d4762",
            "metadata": {},
            "source": [
                "By prompting it differently, we can get it to target the other function we've told it about."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "fa232e54",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:45:47.090638Z",
                    "start_time": "2024-05-15T17:45:46.302475Z"
                }
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[DEBUG] 2024-06-19 18:49:52,104 _base_client.py:446: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"}, {'role': 'user', 'content': 'what is the weather going to be like in Glasgow, Scotland over the next x days'}], 'model': 'gpt-4o', 'tool_choice': None, 'tools': [{'type': 'function', 'function': {'name': 'get_current_weather', 'description': 'Get the current weather', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'format': {'type': 'string', 'enum': ['celsius', 'fahrenheit'], 'description': 'The temperature unit to use. Infer this from the users location.'}}, 'required': ['location', 'format']}}}, {'type': 'function', 'function': {'name': 'get_n_day_weather_forecast', 'description': 'Get an N-day weather forecast', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'format': {'type': 'string', 'enum': ['celsius', 'fahrenheit'], 'description': 'The temperature unit to use. Infer this from the users location.'}, 'num_days': {'type': 'integer', 'description': 'The number of days to forecast'}}, 'required': ['location', 'format', 'num_days']}}}]}}\n",
                        "[DEBUG] 2024-06-19 18:49:52,105 _base_client.py:949: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
                        "[DEBUG] 2024-06-19 18:49:52,106 _trace.py:45: send_request_headers.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:52,107 _trace.py:45: send_request_headers.complete\n",
                        "[DEBUG] 2024-06-19 18:49:52,107 _trace.py:45: send_request_body.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:52,108 _trace.py:45: send_request_body.complete\n",
                        "[DEBUG] 2024-06-19 18:49:52,108 _trace.py:45: receive_response_headers.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:52,699 _trace.py:45: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 19 Jun 2024 22:49:52 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'lastmile-ai'), (b'openai-processing-ms', b'377'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'12000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'11999933'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_3f8af90a400d3b34f92d7a770ee3faa5'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89671324e919238e-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
                        "[INFO] 2024-06-19 18:49:52,699 _client.py:1026: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "[DEBUG] 2024-06-19 18:49:52,700 _trace.py:45: receive_response_body.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:52,700 _trace.py:45: receive_response_body.complete\n",
                        "[DEBUG] 2024-06-19 18:49:52,700 _trace.py:45: response_closed.started\n",
                        "[DEBUG] 2024-06-19 18:49:52,700 _trace.py:45: response_closed.complete\n",
                        "[DEBUG] 2024-06-19 18:49:52,701 _base_client.py:988: HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Wed, 19 Jun 2024 22:49:52 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'lastmile-ai', 'openai-processing-ms': '377', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '12000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '11999933', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '0s', 'x-request-id': 'req_3f8af90a400d3b34f92d7a770ee3faa5', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89671324e919238e-EWR', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
                        "[DEBUG] 2024-06-19 18:49:52,701 _base_client.py:996: request_id: req_3f8af90a400d3b34f92d7a770ee3faa5\n",
                        "[DEBUG] 2024-06-19 18:49:52,732 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/trace/create HTTP/11\" 200 10\n",
                        "[DEBUG] 2024-06-19 18:49:52,734 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
                        "[DEBUG] 2024-06-19 18:49:52,808 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/rag_query_traces/create HTTP/11\" 200 None\n",
                        "[DEBUG] 2024-06-19 18:49:52,810 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
                        "[DEBUG] 2024-06-19 18:49:52,879 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/rag_events/create HTTP/11\" 200 None\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "ChatCompletionMessage(content='Could you please specify the number of days (\"x\") you would like the weather forecast for Glasgow, Scotland?', role='assistant', function_call=None, tool_calls=None)"
                        ]
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "messages = []\n",
                "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
                "messages.append({\"role\": \"user\", \"content\": \"what is the weather going to be like in Glasgow, Scotland over the next x days\"})\n",
                "chat_response = chat_completion_request(\n",
                "    messages, tools=tools\n",
                ")\n",
                "assistant_message = chat_response.choices[0].message\n",
                "messages.append(assistant_message)\n",
                "assistant_message\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "6172ddac",
            "metadata": {},
            "source": [
                "Once again, the model is asking us for clarification because it doesn't have enough information yet. In this case it already knows the location for the forecast, but it needs to know how many days are required in the forecast."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "c7d8a543",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:45:49.790820Z",
                    "start_time": "2024-05-15T17:45:48.847752Z"
                }
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[DEBUG] 2024-06-19 18:49:52,890 _base_client.py:446: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"}, {'role': 'user', 'content': 'what is the weather going to be like in Glasgow, Scotland over the next x days'}, {'content': 'Could you please specify the number of days (\"x\") you would like the weather forecast for Glasgow, Scotland?', 'role': 'assistant'}, {'role': 'user', 'content': '5 days'}], 'model': 'gpt-4o', 'tool_choice': None, 'tools': [{'type': 'function', 'function': {'name': 'get_current_weather', 'description': 'Get the current weather', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'format': {'type': 'string', 'enum': ['celsius', 'fahrenheit'], 'description': 'The temperature unit to use. Infer this from the users location.'}}, 'required': ['location', 'format']}}}, {'type': 'function', 'function': {'name': 'get_n_day_weather_forecast', 'description': 'Get an N-day weather forecast', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'format': {'type': 'string', 'enum': ['celsius', 'fahrenheit'], 'description': 'The temperature unit to use. Infer this from the users location.'}, 'num_days': {'type': 'integer', 'description': 'The number of days to forecast'}}, 'required': ['location', 'format', 'num_days']}}}]}}\n",
                        "[DEBUG] 2024-06-19 18:49:52,891 _base_client.py:949: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
                        "[DEBUG] 2024-06-19 18:49:52,892 _trace.py:45: send_request_headers.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:52,892 _trace.py:45: send_request_headers.complete\n",
                        "[DEBUG] 2024-06-19 18:49:52,893 _trace.py:45: send_request_body.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:52,893 _trace.py:45: send_request_body.complete\n",
                        "[DEBUG] 2024-06-19 18:49:52,894 _trace.py:45: receive_response_headers.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:53,526 _trace.py:45: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 19 Jun 2024 22:49:53 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'lastmile-ai'), (b'openai-processing-ms', b'490'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'12000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'11999902'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_83ad554a4032050200e5be9acb788865'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89671329dd85238e-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
                        "[INFO] 2024-06-19 18:49:53,527 _client.py:1026: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "[DEBUG] 2024-06-19 18:49:53,527 _trace.py:45: receive_response_body.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:53,528 _trace.py:45: receive_response_body.complete\n",
                        "[DEBUG] 2024-06-19 18:49:53,528 _trace.py:45: response_closed.started\n",
                        "[DEBUG] 2024-06-19 18:49:53,529 _trace.py:45: response_closed.complete\n",
                        "[DEBUG] 2024-06-19 18:49:53,529 _base_client.py:988: HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Wed, 19 Jun 2024 22:49:53 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'lastmile-ai', 'openai-processing-ms': '490', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '12000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '11999902', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '0s', 'x-request-id': 'req_83ad554a4032050200e5be9acb788865', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89671329dd85238e-EWR', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
                        "[DEBUG] 2024-06-19 18:49:53,530 _base_client.py:996: request_id: req_83ad554a4032050200e5be9acb788865\n",
                        "[DEBUG] 2024-06-19 18:49:53,561 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/trace/create HTTP/11\" 200 10\n",
                        "[DEBUG] 2024-06-19 18:49:53,566 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
                        "[DEBUG] 2024-06-19 18:49:53,652 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/rag_query_traces/create HTTP/11\" 200 None\n",
                        "[DEBUG] 2024-06-19 18:49:53,655 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
                        "[DEBUG] 2024-06-19 18:49:53,726 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/rag_events/create HTTP/11\" 200 None\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_Cyw1IcLEeg0kk6I4g1c94dhA', function=Function(arguments='{\"location\":\"Glasgow, Scotland\",\"format\":\"celsius\",\"num_days\":5}', name='get_n_day_weather_forecast'), type='function')]))"
                        ]
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "messages.append({\"role\": \"user\", \"content\": \"5 days\"})\n",
                "chat_response = chat_completion_request(\n",
                "    messages, tools=tools\n",
                ")\n",
                "chat_response.choices[0]\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "4b758a0a",
            "metadata": {},
            "source": [
                "#### Forcing the use of specific functions or no function"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "412f79ba",
            "metadata": {},
            "source": [
                "We can force the model to use a specific function, for example get_n_day_weather_forecast by using the function_call argument. By doing so, we force the model to make assumptions about how to use it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "559371b7",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:45:54.194255Z",
                    "start_time": "2024-05-15T17:45:52.975746Z"
                }
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[DEBUG] 2024-06-19 18:49:53,737 _base_client.py:446: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"}, {'role': 'user', 'content': 'Give me a weather report for Toronto, Canada.'}], 'model': 'gpt-4o', 'tool_choice': {'type': 'function', 'function': {'name': 'get_n_day_weather_forecast'}}, 'tools': [{'type': 'function', 'function': {'name': 'get_current_weather', 'description': 'Get the current weather', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'format': {'type': 'string', 'enum': ['celsius', 'fahrenheit'], 'description': 'The temperature unit to use. Infer this from the users location.'}}, 'required': ['location', 'format']}}}, {'type': 'function', 'function': {'name': 'get_n_day_weather_forecast', 'description': 'Get an N-day weather forecast', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'format': {'type': 'string', 'enum': ['celsius', 'fahrenheit'], 'description': 'The temperature unit to use. Infer this from the users location.'}, 'num_days': {'type': 'integer', 'description': 'The number of days to forecast'}}, 'required': ['location', 'format', 'num_days']}}}]}}\n",
                        "[DEBUG] 2024-06-19 18:49:53,739 _base_client.py:949: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
                        "[DEBUG] 2024-06-19 18:49:53,739 _trace.py:45: send_request_headers.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:53,740 _trace.py:45: send_request_headers.complete\n",
                        "[DEBUG] 2024-06-19 18:49:53,740 _trace.py:45: send_request_body.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:53,741 _trace.py:45: send_request_body.complete\n",
                        "[DEBUG] 2024-06-19 18:49:53,741 _trace.py:45: receive_response_headers.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:54,206 _trace.py:45: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 19 Jun 2024 22:49:54 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'lastmile-ai'), (b'openai-processing-ms', b'255'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'12000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'11999941'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_1553c6eb2ad91cad7abd407a6547d634'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8967132f19fa238e-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
                        "[INFO] 2024-06-19 18:49:54,207 _client.py:1026: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "[DEBUG] 2024-06-19 18:49:54,208 _trace.py:45: receive_response_body.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:54,209 _trace.py:45: receive_response_body.complete\n",
                        "[DEBUG] 2024-06-19 18:49:54,209 _trace.py:45: response_closed.started\n",
                        "[DEBUG] 2024-06-19 18:49:54,209 _trace.py:45: response_closed.complete\n",
                        "[DEBUG] 2024-06-19 18:49:54,210 _base_client.py:988: HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Wed, 19 Jun 2024 22:49:54 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'lastmile-ai', 'openai-processing-ms': '255', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '12000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '11999941', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '0s', 'x-request-id': 'req_1553c6eb2ad91cad7abd407a6547d634', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '8967132f19fa238e-EWR', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
                        "[DEBUG] 2024-06-19 18:49:54,210 _base_client.py:996: request_id: req_1553c6eb2ad91cad7abd407a6547d634\n",
                        "[DEBUG] 2024-06-19 18:49:54,244 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/trace/create HTTP/11\" 200 10\n",
                        "[DEBUG] 2024-06-19 18:49:54,246 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
                        "[DEBUG] 2024-06-19 18:49:54,316 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/rag_query_traces/create HTTP/11\" 200 None\n",
                        "[DEBUG] 2024-06-19 18:49:54,319 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
                        "[DEBUG] 2024-06-19 18:49:54,394 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/rag_events/create HTTP/11\" 200 None\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_Ls5lHOFgA1wRFOkM1cYiV2El', function=Function(arguments='{\"location\":\"Toronto, Canada\",\"format\":\"celsius\",\"num_days\":1}', name='get_n_day_weather_forecast'), type='function')])"
                        ]
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# in this cell we force the model to use get_n_day_weather_forecast\n",
                "messages = []\n",
                "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
                "messages.append({\"role\": \"user\", \"content\": \"Give me a weather report for Toronto, Canada.\"})\n",
                "chat_response = chat_completion_request(\n",
                "    messages, tools=tools, tool_choice={\"type\": \"function\", \"function\": {\"name\": \"get_n_day_weather_forecast\"}}\n",
                ")\n",
                "chat_response.choices[0].message"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "a7ab0f58",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:45:56.841233Z",
                    "start_time": "2024-05-15T17:45:55.433397Z"
                }
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[DEBUG] 2024-06-19 18:49:54,405 _base_client.py:446: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"}, {'role': 'user', 'content': 'Give me a weather report for Toronto, Canada.'}], 'model': 'gpt-4o', 'tool_choice': None, 'tools': [{'type': 'function', 'function': {'name': 'get_current_weather', 'description': 'Get the current weather', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'format': {'type': 'string', 'enum': ['celsius', 'fahrenheit'], 'description': 'The temperature unit to use. Infer this from the users location.'}}, 'required': ['location', 'format']}}}, {'type': 'function', 'function': {'name': 'get_n_day_weather_forecast', 'description': 'Get an N-day weather forecast', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'format': {'type': 'string', 'enum': ['celsius', 'fahrenheit'], 'description': 'The temperature unit to use. Infer this from the users location.'}, 'num_days': {'type': 'integer', 'description': 'The number of days to forecast'}}, 'required': ['location', 'format', 'num_days']}}}]}}\n",
                        "[DEBUG] 2024-06-19 18:49:54,407 _base_client.py:949: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
                        "[DEBUG] 2024-06-19 18:49:54,408 _trace.py:45: send_request_headers.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:54,408 _trace.py:45: send_request_headers.complete\n",
                        "[DEBUG] 2024-06-19 18:49:54,409 _trace.py:45: send_request_body.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:54,409 _trace.py:45: send_request_body.complete\n",
                        "[DEBUG] 2024-06-19 18:49:54,409 _trace.py:45: receive_response_headers.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:56,697 _trace.py:45: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 19 Jun 2024 22:49:56 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'lastmile-ai'), (b'openai-processing-ms', b'2168'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'12000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'11999940'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_5b74839dc865b3aa5e6300de3937a1f2'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'896713334cec238e-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
                        "[INFO] 2024-06-19 18:49:56,698 _client.py:1026: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "[DEBUG] 2024-06-19 18:49:56,698 _trace.py:45: receive_response_body.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:56,699 _trace.py:45: receive_response_body.complete\n",
                        "[DEBUG] 2024-06-19 18:49:56,700 _trace.py:45: response_closed.started\n",
                        "[DEBUG] 2024-06-19 18:49:56,700 _trace.py:45: response_closed.complete\n",
                        "[DEBUG] 2024-06-19 18:49:56,701 _base_client.py:988: HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Wed, 19 Jun 2024 22:49:56 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'lastmile-ai', 'openai-processing-ms': '2168', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '12000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '11999940', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '0s', 'x-request-id': 'req_5b74839dc865b3aa5e6300de3937a1f2', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '896713334cec238e-EWR', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
                        "[DEBUG] 2024-06-19 18:49:56,701 _base_client.py:996: request_id: req_5b74839dc865b3aa5e6300de3937a1f2\n",
                        "[DEBUG] 2024-06-19 18:49:56,730 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/trace/create HTTP/11\" 200 10\n",
                        "[DEBUG] 2024-06-19 18:49:56,732 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
                        "[DEBUG] 2024-06-19 18:49:56,807 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/rag_query_traces/create HTTP/11\" 200 None\n",
                        "[DEBUG] 2024-06-19 18:49:56,810 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
                        "[DEBUG] 2024-06-19 18:49:56,898 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/rag_events/create HTTP/11\" 200 None\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_izI2oQ34kEYETjNmE4zB4buy', function=Function(arguments='{\"location\": \"Toronto, Canada\", \"format\": \"celsius\"}', name='get_current_weather'), type='function'), ChatCompletionMessageToolCall(id='call_SifNXr4diPtSbkwxbkDidgwh', function=Function(arguments='{\"location\": \"Toronto, Canada\", \"format\": \"celsius\", \"num_days\": 3}', name='get_n_day_weather_forecast'), type='function')])"
                        ]
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# if we don't force the model to use get_n_day_weather_forecast it may not\n",
                "messages = []\n",
                "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
                "messages.append({\"role\": \"user\", \"content\": \"Give me a weather report for Toronto, Canada.\"})\n",
                "chat_response = chat_completion_request(\n",
                "    messages, tools=tools\n",
                ")\n",
                "chat_response.choices[0].message"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "3bd70e48",
            "metadata": {},
            "source": [
                "We can also force the model to not use a function at all. By doing so we prevent it from producing a proper function call."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "acfe54e6",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:45:59.800346Z",
                    "start_time": "2024-05-15T17:45:59.289603Z"
                }
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[DEBUG] 2024-06-19 18:49:56,906 _base_client.py:446: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"}, {'role': 'user', 'content': 'Give me the current weather (use Celcius) for Toronto, Canada.'}], 'model': 'gpt-4o', 'tool_choice': 'none', 'tools': [{'type': 'function', 'function': {'name': 'get_current_weather', 'description': 'Get the current weather', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'format': {'type': 'string', 'enum': ['celsius', 'fahrenheit'], 'description': 'The temperature unit to use. Infer this from the users location.'}}, 'required': ['location', 'format']}}}, {'type': 'function', 'function': {'name': 'get_n_day_weather_forecast', 'description': 'Get an N-day weather forecast', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'format': {'type': 'string', 'enum': ['celsius', 'fahrenheit'], 'description': 'The temperature unit to use. Infer this from the users location.'}, 'num_days': {'type': 'integer', 'description': 'The number of days to forecast'}}, 'required': ['location', 'format', 'num_days']}}}]}}\n",
                        "[DEBUG] 2024-06-19 18:49:56,907 _base_client.py:949: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
                        "[DEBUG] 2024-06-19 18:49:56,908 _trace.py:45: send_request_headers.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:56,908 _trace.py:45: send_request_headers.complete\n",
                        "[DEBUG] 2024-06-19 18:49:56,909 _trace.py:45: send_request_body.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:56,909 _trace.py:45: send_request_body.complete\n",
                        "[DEBUG] 2024-06-19 18:49:56,909 _trace.py:45: receive_response_headers.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:57,281 _trace.py:45: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 19 Jun 2024 22:49:57 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'lastmile-ai'), (b'openai-processing-ms', b'254'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'12000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'11999937'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_29577723cdc5ea5dc5263a58f1ae4b70'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'89671342ea19238e-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
                        "[INFO] 2024-06-19 18:49:57,282 _client.py:1026: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "[DEBUG] 2024-06-19 18:49:57,282 _trace.py:45: receive_response_body.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:57,283 _trace.py:45: receive_response_body.complete\n",
                        "[DEBUG] 2024-06-19 18:49:57,284 _trace.py:45: response_closed.started\n",
                        "[DEBUG] 2024-06-19 18:49:57,284 _trace.py:45: response_closed.complete\n",
                        "[DEBUG] 2024-06-19 18:49:57,284 _base_client.py:988: HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Wed, 19 Jun 2024 22:49:57 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'lastmile-ai', 'openai-processing-ms': '254', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '12000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '11999937', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '0s', 'x-request-id': 'req_29577723cdc5ea5dc5263a58f1ae4b70', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '89671342ea19238e-EWR', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
                        "[DEBUG] 2024-06-19 18:49:57,285 _base_client.py:996: request_id: req_29577723cdc5ea5dc5263a58f1ae4b70\n",
                        "[DEBUG] 2024-06-19 18:49:57,313 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/trace/create HTTP/11\" 200 10\n",
                        "[DEBUG] 2024-06-19 18:49:57,315 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
                        "[DEBUG] 2024-06-19 18:49:57,385 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/rag_query_traces/create HTTP/11\" 200 None\n",
                        "[DEBUG] 2024-06-19 18:49:57,388 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
                        "[DEBUG] 2024-06-19 18:49:57,459 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/rag_events/create HTTP/11\" 200 None\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "ChatCompletionMessage(content=\"I'll get the current weather for Toronto, Canada in Celsius for you.\", role='assistant', function_call=None, tool_calls=None)"
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "messages = []\n",
                "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
                "messages.append({\"role\": \"user\", \"content\": \"Give me the current weather (use Celcius) for Toronto, Canada.\"})\n",
                "chat_response = chat_completion_request(\n",
                "    messages, tools=tools, tool_choice=\"none\"\n",
                ")\n",
                "chat_response.choices[0].message\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b616353b",
            "metadata": {},
            "source": [
                "### Parallel Function Calling\n",
                "\n",
                "Newer models such as gpt-4o or gpt-3.5-turbo can call multiple functions in one turn."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "380eeb68",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:46:04.048553Z",
                    "start_time": "2024-05-15T17:46:01.273501Z"
                }
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[DEBUG] 2024-06-19 18:49:57,471 _base_client.py:446: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"}, {'role': 'user', 'content': 'what is the weather going to be like in San Francisco and Glasgow over the next 4 days'}], 'model': 'gpt-4o', 'tool_choice': None, 'tools': [{'type': 'function', 'function': {'name': 'get_current_weather', 'description': 'Get the current weather', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'format': {'type': 'string', 'enum': ['celsius', 'fahrenheit'], 'description': 'The temperature unit to use. Infer this from the users location.'}}, 'required': ['location', 'format']}}}, {'type': 'function', 'function': {'name': 'get_n_day_weather_forecast', 'description': 'Get an N-day weather forecast', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'format': {'type': 'string', 'enum': ['celsius', 'fahrenheit'], 'description': 'The temperature unit to use. Infer this from the users location.'}, 'num_days': {'type': 'integer', 'description': 'The number of days to forecast'}}, 'required': ['location', 'format', 'num_days']}}}]}}\n",
                        "[DEBUG] 2024-06-19 18:49:57,472 _base_client.py:949: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
                        "[DEBUG] 2024-06-19 18:49:57,473 _trace.py:45: send_request_headers.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:57,474 _trace.py:45: send_request_headers.complete\n",
                        "[DEBUG] 2024-06-19 18:49:57,474 _trace.py:45: send_request_body.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:57,475 _trace.py:45: send_request_body.complete\n",
                        "[DEBUG] 2024-06-19 18:49:57,475 _trace.py:45: receive_response_headers.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:58,741 _trace.py:45: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 19 Jun 2024 22:49:58 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'lastmile-ai'), (b'openai-processing-ms', b'1006'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'12000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'11999931'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_960d8c1926a7ffab492042287862e8df'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'896713467d1b238e-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
                        "[INFO] 2024-06-19 18:49:58,742 _client.py:1026: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "[DEBUG] 2024-06-19 18:49:58,742 _trace.py:45: receive_response_body.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:58,743 _trace.py:45: receive_response_body.complete\n",
                        "[DEBUG] 2024-06-19 18:49:58,744 _trace.py:45: response_closed.started\n",
                        "[DEBUG] 2024-06-19 18:49:58,744 _trace.py:45: response_closed.complete\n",
                        "[DEBUG] 2024-06-19 18:49:58,745 _base_client.py:988: HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Wed, 19 Jun 2024 22:49:58 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'lastmile-ai', 'openai-processing-ms': '1006', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '12000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '11999931', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '0s', 'x-request-id': 'req_960d8c1926a7ffab492042287862e8df', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '896713467d1b238e-EWR', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
                        "[DEBUG] 2024-06-19 18:49:58,745 _base_client.py:996: request_id: req_960d8c1926a7ffab492042287862e8df\n",
                        "[DEBUG] 2024-06-19 18:49:58,777 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/trace/create HTTP/11\" 200 10\n",
                        "[DEBUG] 2024-06-19 18:49:58,779 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
                        "[DEBUG] 2024-06-19 18:49:58,849 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/rag_query_traces/create HTTP/11\" 200 None\n",
                        "[DEBUG] 2024-06-19 18:49:58,852 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
                        "[DEBUG] 2024-06-19 18:49:58,929 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/rag_events/create HTTP/11\" 200 None\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "[ChatCompletionMessageToolCall(id='call_SMJza50PKl8m8m5ndk8ATxeE', function=Function(arguments='{\"location\": \"San Francisco, CA\", \"format\": \"fahrenheit\", \"num_days\": 4}', name='get_n_day_weather_forecast'), type='function'),\n",
                            " ChatCompletionMessageToolCall(id='call_X3mjNqD7IbfrpZf5jrnhVL1T', function=Function(arguments='{\"location\": \"Glasgow, UK\", \"format\": \"celsius\", \"num_days\": 4}', name='get_n_day_weather_forecast'), type='function')]"
                        ]
                    },
                    "execution_count": 14,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "messages = []\n",
                "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
                "messages.append({\"role\": \"user\", \"content\": \"what is the weather going to be like in San Francisco and Glasgow over the next 4 days\"})\n",
                "chat_response = chat_completion_request(\n",
                "    messages, tools=tools, model=GPT_MODEL\n",
                ")\n",
                "\n",
                "assistant_message = chat_response.choices[0].message.tool_calls\n",
                "assistant_message"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "b4482aee",
            "metadata": {},
            "source": [
                "## How to call functions with model generated arguments\n",
                "\n",
                "In our next example, we'll demonstrate how to execute functions whose inputs are model-generated, and use this to implement an agent that can answer questions for us about a database. For simplicity we'll use the [Chinook sample database](https://www.sqlitetutorial.net/sqlite-sample-database/).\n",
                "\n",
                "*Note:* SQL generation can be high-risk in a production environment since models are not perfectly reliable at generating correct SQL."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "f7654fef",
            "metadata": {},
            "source": [
                "### Specifying a function to execute SQL queries\n",
                "\n",
                "First let's define some helpful utility functions to extract data from a SQLite database."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "3230b819",
            "metadata": {},
            "outputs": [],
            "source": [
                "!mkdir data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "30f6b60e",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:46:07.270851Z",
                    "start_time": "2024-05-15T17:46:07.265545Z"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Opened database successfully\n"
                    ]
                }
            ],
            "source": [
                "import sqlite3\n",
                "\n",
                "conn = sqlite3.connect(\"data/Chinook.db\")\n",
                "print(\"Opened database successfully\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "abec0214",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:46:09.345308Z",
                    "start_time": "2024-05-15T17:46:09.342998Z"
                }
            },
            "outputs": [],
            "source": [
                "def get_table_names(conn):\n",
                "    \"\"\"Return a list of table names.\"\"\"\n",
                "    table_names = []\n",
                "    tables = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
                "    for table in tables.fetchall():\n",
                "        table_names.append(table[0])\n",
                "    return table_names\n",
                "\n",
                "\n",
                "def get_column_names(conn, table_name):\n",
                "    \"\"\"Return a list of column names.\"\"\"\n",
                "    column_names = []\n",
                "    columns = conn.execute(f\"PRAGMA table_info('{table_name}');\").fetchall()\n",
                "    for col in columns:\n",
                "        column_names.append(col[1])\n",
                "    return column_names\n",
                "\n",
                "\n",
                "def get_database_info(conn):\n",
                "    \"\"\"Return a list of dicts containing the table name and columns for each table in the database.\"\"\"\n",
                "    table_dicts = []\n",
                "    for table_name in get_table_names(conn):\n",
                "        columns_names = get_column_names(conn, table_name)\n",
                "        table_dicts.append({\"table_name\": table_name, \"column_names\": columns_names})\n",
                "    return table_dicts\n"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "77e6e5ea",
            "metadata": {},
            "source": [
                "Now can use these utility functions to extract a representation of the database schema."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "0c0104cd",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:46:11.303746Z",
                    "start_time": "2024-05-15T17:46:11.301210Z"
                }
            },
            "outputs": [],
            "source": [
                "database_schema_dict = get_database_info(conn)\n",
                "database_schema_string = \"\\n\".join(\n",
                "    [\n",
                "        f\"Table: {table['table_name']}\\nColumns: {', '.join(table['column_names'])}\"\n",
                "        for table in database_schema_dict\n",
                "    ]\n",
                ")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "ae73c9ee",
            "metadata": {},
            "source": [
                "As before, we'll define a function specification for the function we'd like the API to generate arguments for. Notice that we are inserting the database schema into the function specification. This will be important for the model to know about."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "0258813a",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:46:16.569530Z",
                    "start_time": "2024-05-15T17:46:16.567801Z"
                }
            },
            "outputs": [],
            "source": [
                "tools = [\n",
                "    {\n",
                "        \"type\": \"function\",\n",
                "        \"function\": {\n",
                "            \"name\": \"ask_database\",\n",
                "            \"description\": \"Use this function to answer user questions about music. Input should be a fully formed SQL query.\",\n",
                "            \"parameters\": {\n",
                "                \"type\": \"object\",\n",
                "                \"properties\": {\n",
                "                    \"query\": {\n",
                "                        \"type\": \"string\",\n",
                "                        \"description\": f\"\"\"\n",
                "                                SQL query extracting info to answer the user's question.\n",
                "                                SQL should be written using this database schema:\n",
                "                                {database_schema_string}\n",
                "                                The query should be returned in plain text, not in JSON.\n",
                "                                \"\"\",\n",
                "                    }\n",
                "                },\n",
                "                \"required\": [\"query\"],\n",
                "            },\n",
                "        }\n",
                "    }\n",
                "]"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "da08c121",
            "metadata": {},
            "source": [
                "### Executing SQL queries\n",
                "\n",
                "Now let's implement the function that will actually excute queries against the database."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "id": "65585e74",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:46:19.198723Z",
                    "start_time": "2024-05-15T17:46:19.197043Z"
                }
            },
            "outputs": [],
            "source": [
                "def ask_database(conn, query):\n",
                "    \"\"\"Function to query SQLite database with a provided SQL query.\"\"\"\n",
                "    try:\n",
                "        results = str(conn.execute(query).fetchall())\n",
                "    except Exception as e:\n",
                "        results = f\"query failed with error: {e}\"\n",
                "    return results"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8f6885e9f0af5c40",
            "metadata": {},
            "source": [
                "##### Steps to invoke a function call using Chat Completions API: \n",
                "\n",
                "**Step 1**: Prompt the model with content that may result in model selecting a tool to use. The description of the tools such as a function names and signature is defined in the 'Tools' list and passed to the model in API call. If selected, the function name and parameters are included in the response.<br>\n",
                "  \n",
                "**Step 2**: Check programmatically if model wanted to call a function. If true, proceed to step 3. <br>  \n",
                "**Step 3**: Extract the function name and parameters from response, call the function with parameters. Append the result to messages. <br>    \n",
                "**Step 4**: Invoke the chat completions API with the message list to get the response. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "id": "e8b7cb9cdc7a7616",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:46:25.725379Z",
                    "start_time": "2024-05-15T17:46:24.255505Z"
                }
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[DEBUG] 2024-06-19 18:49:59,234 _base_client.py:446: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'What is the name of the album with the most tracks?'}], 'model': 'gpt-4o', 'tool_choice': 'auto', 'tools': [{'type': 'function', 'function': {'name': 'ask_database', 'description': 'Use this function to answer user questions about music. Input should be a fully formed SQL query.', 'parameters': {'type': 'object', 'properties': {'query': {'type': 'string', 'description': \"\\n                                SQL query extracting info to answer the user's question.\\n                                SQL should be written using this database schema:\\n                                \\n                                The query should be returned in plain text, not in JSON.\\n                                \"}}, 'required': ['query']}}}]}}\n",
                        "[DEBUG] 2024-06-19 18:49:59,235 _base_client.py:949: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
                        "[DEBUG] 2024-06-19 18:49:59,236 _trace.py:45: send_request_headers.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:59,237 _trace.py:45: send_request_headers.complete\n",
                        "[DEBUG] 2024-06-19 18:49:59,237 _trace.py:45: send_request_body.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:49:59,237 _trace.py:45: send_request_body.complete\n",
                        "[DEBUG] 2024-06-19 18:49:59,237 _trace.py:45: receive_response_headers.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:50:00,144 _trace.py:45: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 19 Jun 2024 22:50:00 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'lastmile-ai'), (b'openai-processing-ms', b'789'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'12000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'11999969'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_c4504bdcb6dee2631af46aeb83938d42'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'896713517dd0238e-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
                        "[INFO] 2024-06-19 18:50:00,145 _client.py:1026: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "[DEBUG] 2024-06-19 18:50:00,145 _trace.py:45: receive_response_body.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:50:00,146 _trace.py:45: receive_response_body.complete\n",
                        "[DEBUG] 2024-06-19 18:50:00,146 _trace.py:45: response_closed.started\n",
                        "[DEBUG] 2024-06-19 18:50:00,147 _trace.py:45: response_closed.complete\n",
                        "[DEBUG] 2024-06-19 18:50:00,147 _base_client.py:988: HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Wed, 19 Jun 2024 22:50:00 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'lastmile-ai', 'openai-processing-ms': '789', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '12000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '11999969', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '0s', 'x-request-id': 'req_c4504bdcb6dee2631af46aeb83938d42', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '896713517dd0238e-EWR', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
                        "[DEBUG] 2024-06-19 18:50:00,148 _base_client.py:996: request_id: req_c4504bdcb6dee2631af46aeb83938d42\n",
                        "[DEBUG] 2024-06-19 18:50:00,178 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/trace/create HTTP/11\" 200 10\n",
                        "[DEBUG] 2024-06-19 18:50:00,182 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
                        "[DEBUG] 2024-06-19 18:50:00,250 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/rag_query_traces/create HTTP/11\" 200 None\n",
                        "[DEBUG] 2024-06-19 18:50:00,253 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
                        "[DEBUG] 2024-06-19 18:50:00,350 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/rag_events/create HTTP/11\" 200 None\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_izI2oQ34kEYETjNmE4zB4buy', function=Function(arguments='{\"query\":\"SELECT album_name, COUNT(track_id) as track_count FROM tracks GROUP BY album_name ORDER BY track_count DESC LIMIT 1;\"}', name='ask_database'), type='function')])\n"
                    ]
                }
            ],
            "source": [
                "# Step #1: Prompt with content that may result in function call. In this case the model can identify the information requested by the user is potentially available in the database schema passed to the model in Tools description. \n",
                "messages = [{\n",
                "    \"role\":\"user\", \n",
                "    \"content\": \"What is the name of the album with the most tracks?\"\n",
                "}]\n",
                "\n",
                "response = client.chat.completions.create(\n",
                "    model='gpt-4o', \n",
                "    messages=messages, \n",
                "    tools= tools, \n",
                "    tool_choice=\"auto\"\n",
                ")\n",
                "\n",
                "# Append the message to messages list\n",
                "response_message = response.choices[0].message \n",
                "messages.append(response_message)\n",
                "\n",
                "print(response_message)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "id": "351c39def3417776",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2024-05-15T17:46:30.346444Z",
                    "start_time": "2024-05-15T17:46:29.699046Z"
                }
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[DEBUG] 2024-06-19 18:50:00,360 _base_client.py:446: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': 'What is the name of the album with the most tracks?'}, {'content': None, 'role': 'assistant', 'tool_calls': [{'id': 'call_izI2oQ34kEYETjNmE4zB4buy', 'function': {'arguments': '{\"query\":\"SELECT album_name, COUNT(track_id) as track_count FROM tracks GROUP BY album_name ORDER BY track_count DESC LIMIT 1;\"}', 'name': 'ask_database'}, 'type': 'function'}]}, {'role': 'tool', 'tool_call_id': 'call_izI2oQ34kEYETjNmE4zB4buy', 'name': 'ask_database', 'content': 'query failed with error: no such table: tracks'}], 'model': 'gpt-4o'}}\n",
                        "[DEBUG] 2024-06-19 18:50:00,361 _base_client.py:949: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
                        "[DEBUG] 2024-06-19 18:50:00,362 _trace.py:45: send_request_headers.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:50:00,362 _trace.py:45: send_request_headers.complete\n",
                        "[DEBUG] 2024-06-19 18:50:00,363 _trace.py:45: send_request_body.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:50:00,363 _trace.py:45: send_request_body.complete\n",
                        "[DEBUG] 2024-06-19 18:50:00,363 _trace.py:45: receive_response_headers.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:50:02,530 _trace.py:45: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Wed, 19 Jun 2024 22:50:02 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'lastmile-ai'), (b'openai-processing-ms', b'1902'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'12000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'11999956'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_06e4e9fb840257aedf9fa284418ae759'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'896713588b84238e-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
                        "[INFO] 2024-06-19 18:50:02,531 _client.py:1026: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "[DEBUG] 2024-06-19 18:50:02,532 _trace.py:45: receive_response_body.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-19 18:50:02,532 _trace.py:45: receive_response_body.complete\n",
                        "[DEBUG] 2024-06-19 18:50:02,533 _trace.py:45: response_closed.started\n",
                        "[DEBUG] 2024-06-19 18:50:02,533 _trace.py:45: response_closed.complete\n",
                        "[DEBUG] 2024-06-19 18:50:02,533 _base_client.py:988: HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers({'date': 'Wed, 19 Jun 2024 22:50:02 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'openai-organization': 'lastmile-ai', 'openai-processing-ms': '1902', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '12000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '11999956', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '0s', 'x-request-id': 'req_06e4e9fb840257aedf9fa284418ae759', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '896713588b84238e-EWR', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
                        "[DEBUG] 2024-06-19 18:50:02,534 _base_client.py:996: request_id: req_06e4e9fb840257aedf9fa284418ae759\n",
                        "[DEBUG] 2024-06-19 18:50:02,569 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/trace/create HTTP/11\" 200 10\n",
                        "[DEBUG] 2024-06-19 18:50:02,571 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
                        "[DEBUG] 2024-06-19 18:50:02,649 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/rag_query_traces/create HTTP/11\" 200 None\n",
                        "[DEBUG] 2024-06-19 18:50:02,651 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
                        "[DEBUG] 2024-06-19 18:50:02,725 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/rag_events/create HTTP/11\" 200 None\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "When identifying the album with the most tracks, it's important to narrow down whether we're talking about studio albums, compilations, box sets, or special editions. For example:\n",
                        "\n",
                        "1. **The Magnetic Fields – \"69 Love Songs\" (1999)**: This triple album by the indie pop band has 69 tracks.\n",
                        "2. **Prince – \"Crystal Ball\" (1998)**: A 3-CD set with 50 tracks, including some previously unreleased material.\n",
                        "\n",
                        "But compiling a definitive list can be complex due to variations in releases across different formats and regions. Keep in mind that in popular music, compilations and special editions may often have more tracks compared to standard studio albums.\n",
                        "\n",
                        "If you have a specific category in mind (such as studio albums or live recordings), I can provide more tailored information!\n"
                    ]
                }
            ],
            "source": [
                "# Step 2: determine if the response from the model includes a tool call.   \n",
                "tool_calls = response_message.tool_calls\n",
                "if tool_calls:\n",
                "    # If true the model will return the name of the tool / function to call and the argument(s)  \n",
                "    tool_call_id = tool_calls[0].id\n",
                "    tool_function_name = tool_calls[0].function.name\n",
                "    tool_query_string = eval(tool_calls[0].function.arguments)['query']\n",
                "    \n",
                "    # Step 3: Call the function and retrieve results. Append the results to the messages list.      \n",
                "    if tool_function_name == 'ask_database':\n",
                "        results = ask_database(conn, tool_query_string)\n",
                "        \n",
                "        messages.append({\n",
                "            \"role\":\"tool\", \n",
                "            \"tool_call_id\":tool_call_id, \n",
                "            \"name\": tool_function_name, \n",
                "            \"content\":results\n",
                "        })\n",
                "        \n",
                "        # Step 4: Invoke the chat completions API with the function response appended to the messages list\n",
                "        # Note that messages with role 'tool' must be a response to a preceding message with 'tool_calls'\n",
                "        model_response_with_function_call = client.chat.completions.create(\n",
                "            model=\"gpt-4o\",\n",
                "            messages=messages,\n",
                "        )  # get a new response from the model where it can see the function response\n",
                "        print(model_response_with_function_call.choices[0].message.content)\n",
                "    else: \n",
                "        print(f\"Error: function {tool_function_name} does not exist\")\n",
                "else: \n",
                "    # Model did not identify a function to call, result can be returned to the user \n",
                "    print(response_message.content) "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "2d89073c",
            "metadata": {},
            "source": [
                "## Next Steps\n",
                "\n",
                "See our other [notebook](How_to_call_functions_for_knowledge_retrieval.ipynb) that demonstrates how to use the Chat Completions API and functions for knowledge retrieval to interact conversationally with a knowledge base."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
