{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "3e67f200",
            "metadata": {},
            "source": [
                "# OpenAI Streaming - Traced With LastMile\n",
                "\n",
                "Four use cases:\n",
                "\n",
                "1. Text - Non-streaming\n",
                "2. [this one] Text - Streaming\n",
                "3. Tool Calls - Non-streaming\n",
                "4. Tool Calls - Streaming"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "80e71f33",
            "metadata": {
                "pycharm": {
                    "is_executing": true
                }
            },
            "outputs": [],
            "source": [
                "# !pip install scipy --quiet\n",
                "# !pip install tenacity --quiet\n",
                "# !pip install tiktoken --quiet\n",
                "# !pip install termcolor --quiet\n",
                "# !pip install openai --quiet\n",
                "# !pip install \"tracing-auto-instrumentation[openai]\"\n",
                "\n",
                "# Create ~/.env file with this line: OPENAI_API_KEY=<your key here>\n",
                "# You can get your key from https://platform.openai.com/api-keys \n",
                "import openai\n",
                "import dotenv\n",
                "import os\n",
                "dotenv.load_dotenv()\n",
                "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "4abf2967",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[DEBUG] 2024-06-20 01:14:34,012 _config.py:80: load_ssl_context verify=True cert=None trust_env=True http2=False\n",
                        "[DEBUG] 2024-06-20 01:14:34,013 _config.py:146: load_verify_locations cafile='/opt/homebrew/Caskroom/miniconda/base/envs/eval/lib/python3.12/site-packages/certifi/cacert.pem'\n"
                    ]
                }
            ],
            "source": [
                "import openai\n",
                "\n",
                "from lastmile_eval.rag.debugger.api.tracing import LastMileTracer\n",
                "\n",
                "from tracing_auto_instrumentation.openai import wrap_openai\n",
                "from lastmile_eval.rag.debugger.tracing.sdk import get_lastmile_tracer\n",
                "\n",
                "tracer: LastMileTracer = get_lastmile_tracer(\n",
                "    tracer_name=\"OpenAI Text Calling w. Streaming\",\n",
                ")\n",
                "client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
                "client = wrap_openai(client, tracer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "4276d4b0",
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_my_existing_openai_app(user_message: str, stream: bool = True):\n",
                "    completion_params = {\n",
                "        \"model\": \"gpt-3.5-turbo\",\n",
                "        \"top_p\": 1,\n",
                "        \"max_tokens\": 10,\n",
                "        \"temperature\": 1,\n",
                "        \"stream\": stream,\n",
                "        \"messages\": [\n",
                "            {\n",
                "                \"content\": user_message,\n",
                "                \"role\": \"user\",\n",
                "            }\n",
                "        ],\n",
                "    }\n",
                "\n",
                "    response = client.chat.completions.create(**completion_params)\n",
                "    print(\"Chat Completion Response: \")\n",
                "    if stream:\n",
                "        for chunk in response:\n",
                "            print(f\"{chunk=}\")\n",
                "    else:\n",
                "        print(f\"{response=}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "9aa6f3f6",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[DEBUG] 2024-06-20 01:14:34,027 _base_client.py:446: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Tell me a joke about apples', 'role': 'user'}], 'model': 'gpt-3.5-turbo', 'max_tokens': 10, 'stream': False, 'temperature': 1, 'top_p': 1}}\n",
                        "[DEBUG] 2024-06-20 01:14:34,034 _base_client.py:949: Sending HTTP Request: POST https://api.openai.com/v1/chat/completions\n",
                        "[DEBUG] 2024-06-20 01:14:34,035 _trace.py:45: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
                        "[DEBUG] 2024-06-20 01:14:34,051 _trace.py:45: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x1224c40b0>\n",
                        "[DEBUG] 2024-06-20 01:14:34,051 _trace.py:45: start_tls.started ssl_context=<ssl.SSLContext object at 0x16c4f2850> server_hostname='api.openai.com' timeout=5.0\n",
                        "[DEBUG] 2024-06-20 01:14:34,067 _trace.py:45: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x16c43e360>\n",
                        "[DEBUG] 2024-06-20 01:14:34,068 _trace.py:45: send_request_headers.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-20 01:14:34,068 _trace.py:45: send_request_headers.complete\n",
                        "[DEBUG] 2024-06-20 01:14:34,069 _trace.py:45: send_request_body.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-20 01:14:34,069 _trace.py:45: send_request_body.complete\n",
                        "[DEBUG] 2024-06-20 01:14:34,069 _trace.py:45: receive_response_headers.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-20 01:14:34,749 _trace.py:45: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 20 Jun 2024 05:14:34 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'lastmile-ai'), (b'openai-processing-ms', b'545'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'2000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'1999981'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'5b646f56e56fa02f6bad64fb984a49ad'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=6ZT9FA7h.SK1qSCnInkkRLWf93_UgojHv8F9KT_nxeY-1718860474-1.0.1.1-Q7nt9BkcIC2yKec9mltoTqYks016AfErp62ne_R8f2FDcXQJp58OzIv5f7Di.XJWdJuWPGNNLhBFhSCnUsqffQ; path=/; expires=Thu, 20-Jun-24 05:44:34 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=zrmg.NWc60DUBcsndJ0VsMDxfjZbJyaY2EqEAMDPJIs-1718860474753-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'896946ab0c875e6b-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
                        "[INFO] 2024-06-20 01:14:34,752 _client.py:1026: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "[DEBUG] 2024-06-20 01:14:34,753 _trace.py:45: receive_response_body.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-20 01:14:34,755 _trace.py:45: receive_response_body.complete\n",
                        "[DEBUG] 2024-06-20 01:14:34,756 _trace.py:45: response_closed.started\n",
                        "[DEBUG] 2024-06-20 01:14:34,756 _trace.py:45: response_closed.complete\n",
                        "[DEBUG] 2024-06-20 01:14:34,757 _base_client.py:988: HTTP Response: POST https://api.openai.com/v1/chat/completions \"200 OK\" Headers([('date', 'Thu, 20 Jun 2024 05:14:34 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('openai-organization', 'lastmile-ai'), ('openai-processing-ms', '545'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '2000000'), ('x-ratelimit-remaining-requests', '9999'), ('x-ratelimit-remaining-tokens', '1999981'), ('x-ratelimit-reset-requests', '6ms'), ('x-ratelimit-reset-tokens', '0s'), ('x-request-id', '5b646f56e56fa02f6bad64fb984a49ad'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=6ZT9FA7h.SK1qSCnInkkRLWf93_UgojHv8F9KT_nxeY-1718860474-1.0.1.1-Q7nt9BkcIC2yKec9mltoTqYks016AfErp62ne_R8f2FDcXQJp58OzIv5f7Di.XJWdJuWPGNNLhBFhSCnUsqffQ; path=/; expires=Thu, 20-Jun-24 05:44:34 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('set-cookie', '_cfuvid=zrmg.NWc60DUBcsndJ0VsMDxfjZbJyaY2EqEAMDPJIs-1718860474753-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '896946ab0c875e6b-EWR'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
                        "[DEBUG] 2024-06-20 01:14:34,758 _base_client.py:996: request_id: 5b646f56e56fa02f6bad64fb984a49ad\n",
                        "[DEBUG] 2024-06-20 01:14:34,773 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
                        "[DEBUG] 2024-06-20 01:14:34,857 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/trace/create HTTP/11\" 200 10\n",
                        "[DEBUG] 2024-06-20 01:14:34,860 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
                        "[DEBUG] 2024-06-20 01:14:34,940 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/rag_query_traces/create HTTP/11\" 200 763\n",
                        "[DEBUG] 2024-06-20 01:14:34,943 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
                        "[DEBUG] 2024-06-20 01:14:35,023 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/rag_events/create HTTP/11\" 200 770\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Chat Completion Response: \n",
                        "response=ChatCompletion(id='chatcmpl-9c4PyJzf8o0mVkIclRTJ1jS9cPTzR', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='Why did the apple go to the doctor? Because', role='assistant', function_call=None, tool_calls=None))], created=1718860474, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=10, prompt_tokens=13, total_tokens=23))\n"
                    ]
                }
            ],
            "source": [
                "# # Run your code as usual\n",
                "stream = False\n",
                "run_my_existing_openai_app(\"Tell me a joke about apples\", stream=stream)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "263685c1",
            "metadata": {},
            "source": [
                "## Time to test this with async calls"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "d2045d18",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[DEBUG] 2024-06-20 01:14:35,039 _config.py:80: load_ssl_context verify=True cert=None trust_env=True http2=False\n",
                        "[DEBUG] 2024-06-20 01:14:35,041 _config.py:146: load_verify_locations cafile='/opt/homebrew/Caskroom/miniconda/base/envs/eval/lib/python3.12/site-packages/certifi/cacert.pem'\n"
                    ]
                }
            ],
            "source": [
                "client = openai.AsyncOpenAI(api_key=OPENAI_API_KEY)\n",
                "client = wrap_openai(client, tracer)\n",
                "\n",
                "async def run_my_existing_openai_app_async(user_message: str, stream: bool = True):\n",
                "    completion_params = {\n",
                "        \"model\": \"gpt-3.5-turbo\",\n",
                "        \"top_p\": 1,\n",
                "        \"max_tokens\": 3000,\n",
                "        \"temperature\": 1,\n",
                "        \"stream\": stream,\n",
                "        \"messages\": [\n",
                "            {\n",
                "                \"content\": user_message,\n",
                "                \"role\": \"user\",\n",
                "            }\n",
                "        ],\n",
                "    }\n",
                "\n",
                "    response = await client.chat.completions.create(**completion_params)\n",
                "    print(\"Chat Completion Response: \")\n",
                "    if stream:\n",
                "        async for chunk in response:\n",
                "            print(f\"{chunk=}\")\n",
                "    else:\n",
                "        print(f\"{response=}\")\n",
                "    return response"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f9113e1f",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[DEBUG] 2024-06-20 01:14:35,059 _base_client.py:446: Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'json_data': {'messages': [{'content': 'Tell me a joke about apples', 'role': 'user'}], 'model': 'gpt-3.5-turbo', 'max_tokens': 3000, 'stream': False, 'temperature': 1, 'top_p': 1}}\n",
                        "[DEBUG] 2024-06-20 01:14:35,063 _trace.py:45: close.started\n",
                        "[DEBUG] 2024-06-20 01:14:35,064 _trace.py:45: close.complete\n",
                        "[DEBUG] 2024-06-20 01:14:35,067 _trace.py:85: connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
                        "[DEBUG] 2024-06-20 01:14:35,081 _trace.py:85: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x117fbe120>\n",
                        "[DEBUG] 2024-06-20 01:14:35,082 _trace.py:85: start_tls.started ssl_context=<ssl.SSLContext object at 0x16c3612d0> server_hostname='api.openai.com' timeout=5.0\n",
                        "[DEBUG] 2024-06-20 01:14:35,096 _trace.py:85: start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x117fbf8c0>\n",
                        "[DEBUG] 2024-06-20 01:14:35,096 _trace.py:85: send_request_headers.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-20 01:14:35,097 _trace.py:85: send_request_headers.complete\n",
                        "[DEBUG] 2024-06-20 01:14:35,097 _trace.py:85: send_request_body.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-20 01:14:35,098 _trace.py:85: send_request_body.complete\n",
                        "[DEBUG] 2024-06-20 01:14:35,098 _trace.py:85: receive_response_headers.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-20 01:14:35,717 _trace.py:85: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 20 Jun 2024 05:14:35 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'openai-organization', b'lastmile-ai'), (b'openai-processing-ms', b'307'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=15724800; includeSubDomains'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'2000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'1996992'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'90ms'), (b'x-request-id', b'req_fce0579f537641462328fdb1acb8138e'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=hX6BK0PpK.k767jKAqb_3mOmDjPcEAiqiWDhE_Cpzj8-1718860475-1.0.1.1-xD_weO6h81R3sJOvLNjrHwJ9M2n0KIj66IOgFdwXRhbk0DSTRhWdAYQRy_8CxfpjmvdpPuqXa0Ye1UMxrvebjg; path=/; expires=Thu, 20-Jun-24 05:44:35 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Set-Cookie', b'_cfuvid=ncLfXxrw4X5_U6OP5gzLAPr0mX5PIOK7w8ecOmmT9RY-1718860475643-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'896946b17f454205-EWR'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
                        "[INFO] 2024-06-20 01:14:35,719 _client.py:1773: HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
                        "[DEBUG] 2024-06-20 01:14:35,720 _trace.py:85: receive_response_body.started request=<Request [b'POST']>\n",
                        "[DEBUG] 2024-06-20 01:14:35,721 _trace.py:85: receive_response_body.complete\n",
                        "[DEBUG] 2024-06-20 01:14:35,722 _trace.py:85: response_closed.started\n",
                        "[DEBUG] 2024-06-20 01:14:35,723 _trace.py:85: response_closed.complete\n",
                        "[DEBUG] 2024-06-20 01:14:35,724 _base_client.py:1558: HTTP Request: POST https://api.openai.com/v1/chat/completions \"200 OK\"\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Chat Completion Response: \n",
                        "response=ChatCompletion(id='chatcmpl-9c4PzsHa3sAEE9TdIvrgHy44GTq6d', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Why did the apple go to school?\\n\\nBecause it wanted to be a \"smarty-pants!\"', role='assistant', function_call=None, tool_calls=None))], created=1718860475, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=19, prompt_tokens=13, total_tokens=32))\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[ERROR] 2024-06-20 01:14:35,729 __init__.py:156: Failed to detach context\n",
                        "Traceback (most recent call last):\n",
                        "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/eval/lib/python3.12/site-packages/opentelemetry/trace/__init__.py\", line 570, in use_span\n",
                        "    yield span\n",
                        "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/eval/lib/python3.12/site-packages/opentelemetry/sdk/trace/__init__.py\", line 1091, in start_as_current_span\n",
                        "    yield span\n",
                        "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/eval/lib/python3.12/site-packages/lastmile_eval/rag/debugger/tracing/lastmile_tracer.py\", line 229, in start_as_current_span\n",
                        "    yield span\n",
                        "  File \"/Users/rossdancraig/Projects/tracing_auto_instrumentation/src/tracing_auto_instrumentation/openai/async_openai_wrapper.py\", line 258, in create\n",
                        "    yield raw_response\n",
                        "GeneratorExit\n",
                        "\n",
                        "During handling of the above exception, another exception occurred:\n",
                        "\n",
                        "Traceback (most recent call last):\n",
                        "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/eval/lib/python3.12/site-packages/opentelemetry/context/__init__.py\", line 154, in detach\n",
                        "    _RUNTIME_CONTEXT.detach(token)\n",
                        "  File \"/opt/homebrew/Caskroom/miniconda/base/envs/eval/lib/python3.12/site-packages/opentelemetry/context/contextvars_context.py\", line 50, in detach\n",
                        "    self._current_context.reset(token)  # type: ignore\n",
                        "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                        "ValueError: <Token var=<ContextVar name='current_context' default={} at 0x114c5e8e0> at 0x16c52cc80> was created in a different Context\n",
                        "[DEBUG] 2024-06-20 01:14:35,773 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/trace/create HTTP/11\" 200 10\n",
                        "[DEBUG] 2024-06-20 01:14:35,774 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
                        "[DEBUG] 2024-06-20 01:14:35,863 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/rag_query_traces/create HTTP/11\" 200 800\n",
                        "[DEBUG] 2024-06-20 01:14:35,865 connectionpool.py:1051: Starting new HTTPS connection (1): lastmileai.dev:443\n",
                        "[DEBUG] 2024-06-20 01:14:35,944 connectionpool.py:546: https://lastmileai.dev:443 \"POST /api/rag_events/create HTTP/11\" 200 808\n"
                    ]
                }
            ],
            "source": [
                "stream = False\n",
                "response = await run_my_existing_openai_app_async(\"Tell me a joke about apples\", stream=stream)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
