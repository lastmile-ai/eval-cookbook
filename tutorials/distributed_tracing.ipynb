{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbh9LVaVvwUz"
      },
      "source": [
        "# Distributed Tracing with LastMile Tracing SDK\n",
        "\n",
        "In this example notebook, we showcase how to implement distributed tracing using the LastMile Tracing SDK.\n",
        "\n",
        "## Notebook Outline\n",
        "* [Introduction](#intro)\n",
        "* [Setup](#setup)\n",
        "* [Step 1: Instrument server code](#step1)\n",
        "* [Step 2: Instrument client code](#step2)\n",
        "* [Step 3: View Trace Data in UI](#step3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgUE0NodwnuB"
      },
      "source": [
        "<a name=\"intro\"></a>\n",
        "# Introduction\n",
        "**Distributed tracing** allows you to track and analyze the flow of requests across multiple services or components in a distributed system.\n",
        "\n",
        "This example involves the client sending a request to a server to generate a riddle with `gpt-3.5-turbo`. The server uses OpenAI's `gpt-3.5-turbo` to generate the riddle and returns it to the client. The LastMile Tracing SDK is used to to instrument tracing and capture trace information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZsHsTimxR_v"
      },
      "source": [
        "<a name=\"setup\"></a>\n",
        "\n",
        "# Setup\n",
        "\n",
        "To begin, we need to install lastmile-eval library and a couple other libraries for the example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AjBExwI2vwU0",
        "outputId": "d4f48c32-56d7-4241-ae4f-37f1fb287245"
      },
      "outputs": [],
      "source": [
        "!pip install lastmile-eval --upgrade\n",
        "# fork of multiprocessing, uses dill instead of pickle.\n",
        "!pip install multiprocess\n",
        "!pip install flask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO-RslYuxt4i"
      },
      "source": [
        "We also need the following API tokens/keys:\n",
        "\n",
        "* **LastMile AI API Token:** Go to the [LastMile Settings page](https://lastmileai.dev/settings?page=tokens). You will need to first create a LastMile AI account.\n",
        "* **OpenAI API Key:** Go to [OpenAI API Keys page](https://platform.openai.com/account/api-keys) to create and access your OpenAI API Key.\n",
        "\n",
        "Run the code cell below after setting the keys either in **Google Colab Secrets** or in `.env` in your directory. Avoid inputting keys directly in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1R2Orscwx4-T"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "    # If running on Google Colab, use userdata to securely input keys\n",
        "    from google.colab import userdata\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    LASTMILE_API_TOKEN = userdata.get('LASTMILE_API_TOKEN')\n",
        "except ModuleNotFoundError:\n",
        "    # If running locally, load keys from .env file\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "    LASTMILE_API_TOKEN = os.getenv('LASTMILE_API_TOKEN')\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
        "os.environ['LASTMILE_API_TOKEN'] = LASTMILE_API_TOKEN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wreDo5NjyRvR"
      },
      "source": [
        "<a name=\"step1\"></a>\n",
        "# Step 1: Instrument Server Code\n",
        "In this step, we are starting a Flask server that expsoses an endpoint for generating riddles. The server uses a `LastMile Tracer` for distributed tracing (across the client).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlLs3SGKvwU1",
        "outputId": "f53930d9-87ce-4391-face-976502a0d63d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "import multiprocess\n",
        "from flask import Flask, request\n",
        "from openai import OpenAI\n",
        "\n",
        "from lastmile_eval.rag.debugger.api import LastMileTracer\n",
        "from lastmile_eval.rag.debugger.tracing import get_lastmile_tracer\n",
        "\n",
        "\n",
        "def server():\n",
        "    # Instantiate LastMile Tracer\n",
        "    tracer: LastMileTracer = get_lastmile_tracer(\n",
        "        tracer_name=\"generate_riddle\",\n",
        "    )\n",
        "\n",
        "    app = Flask(__name__)\n",
        "\n",
        "    @app.route(\"/generate\")\n",
        "    def generate()-> str:\n",
        "        \"\"\"\n",
        "        Endpoint that generates a riddle using OpenAI's GPT-3.5-turbo model.\n",
        "        \"\"\"\n",
        "        span_context = request.headers.get(\"span\") # Expect a span to be passed to this endpoint\n",
        "\n",
        "        # Trace the span\n",
        "        with tracer.start_as_current_span(\"generate_endpoint\", context = span_context):\n",
        "            response = OpenAI().chat.completions.create(messages = [{\"role\": \"user\", \"content\":\"tell me a riddle\"}], model = \"gpt-3.5-turbo\")\n",
        "            riddle = response.choices[0].message.content\n",
        "            return json.dumps(riddle)\n",
        "\n",
        "    app.run(port=1234, debug=False)\n",
        "\n",
        "# Start Server in a Subprocess to avoid blocking execution of succeeding cells\n",
        "process = multiprocess.Process(target=server)\n",
        "process.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CX1ipalYzNtV"
      },
      "source": [
        "<a name=\"step2\"></a>\n",
        "# Step 2: Instrument Client Code\n",
        "In order distribute the trace context, we need to export the span context to a string and pass it as a header to the server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "Yvo8-EcTvwU1",
        "outputId": "9c053c18-dc96-4742-9a13-e4d0cb2c2301"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Message: Sending request to subprocess server, with span context.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\"I am taken from a mine, and shut up in a wooden case, from which I am never released, and yet I am used by almost every person. What am I?\"'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import requests\n",
        "from opentelemetry import trace\n",
        "\n",
        "from lastmile_eval.rag.debugger.api import LastMileTracer\n",
        "from lastmile_eval.rag.debugger.tracing import export_span, get_lastmile_tracer\n",
        "\n",
        "# Instantiate Tracer2\n",
        "tracer2: LastMileTracer = get_lastmile_tracer(\n",
        "    tracer_name=\"generate_riddle\", # project name\n",
        ")\n",
        "\n",
        "@tracer2.start_as_current_span(\"client\")\n",
        "def client_say_riddle():\n",
        "    try:\n",
        "        print(\"Message: Sending request to subprocess server, with span context.\")\n",
        "\n",
        "        # Export Span\n",
        "        response = requests.get('http://127.0.0.1:1234/generate', headers={\"span\": export_span(trace.get_current_span())})\n",
        "        return response.text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "client_say_riddle()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wDtiRtR0GoT"
      },
      "source": [
        "<a name=\"step3\"></a>\n",
        "\n",
        "# Step 3: View Trace Data in UI\n",
        "Now we can view our distributed trace in the RAG Debugger UI!\n",
        "#### From your terminal:\n",
        "\n",
        "Export your LASTMILE_API_TOKEN\n",
        "\n",
        "```bash\n",
        "export LASTMILE_API_TOKEN=\"<your-api-token>\"\n",
        "```\n",
        "\n",
        "Run this CLI command to access the UI\n",
        "\n",
        "```bash\n",
        "rag-debug launch\n",
        "```\n",
        "Navigate to the 'Traces' Page where you see all the Traces listed including the Distributed Trace we set up in this example.\n",
        "\n",
        "<img width=\"973\" alt=\"Screen Shot 2024-05-28 at 1 28 58 AM\" src=\"https://github.com/lastmile-ai/aiconfig/assets/81494782/35393b82-4d1e-4346-afc3-e4f6d99cd765/\">\n",
        "\n",
        "Let's click into the Trace.\n",
        "\n",
        "<img width=\"973\" alt=\"Screen Shot 2024-05-28 at 1 30 29 AM\" src=\"https://github.com/lastmile-ai/aiconfig/assets/81494782/d25b9b96-2e30-4dfe-8245-076ae860c50c\"/>\n",
        "\n",
        "Here we can see that the server span with `generate_endpoint` has been successfully propagated to the client span with the Tracer. This shows that we're able to trace across multiple services or components in a distributed system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSEK9Ony0nW0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "main",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
